\chapter*{Introduction} \label{chapter:introduction}
\addcontentsline{toc}{chapter}{Introduction}

The Language Modeling is the task of predicting what word comes next in the actual context. In this thesis we delve into a slightly more complicated task, into the Conditional Language Modeling, where the next generated word does not only depend on the context but also on some external factors.

Specifically, given tables of team and individual statistics (e.g. number of points scored by a team,  number of minutes played by a player etc.) we aim to generate a document-scale text that is coherent (e.g. without contradictions), written in good English, and that contains a lot of factual data based on the input tables. We train multiple Deep Neural Network models on the RotoWire dataset \citep{wiseman2017}. It consists of the statistics of National Basketball Association matches from 2014 - 2017 and the corresponding professionally written summaries from the fantasy sports news site \url{https://www.rotowire.com/basketball/}.

The statistics as well as the manual analysis of the dataset show numerous discrepancies (chapter \ref{chapter:preprocessing_and_statistics_of_the_dataset}). We elaborate on various preprocessing and cleaning methods that should make the learning path of our models easier.

We explore the Encoder-Decoder architecture \citep{sutskever2014sequence} along with many enhancements such as the attention (section \ref{section:attention_mechanism}), the copy attention (section \ref{section:copy_mechanism}), and the content selection and planning (section \ref{section:content_selection_and_planning}).

However this kind of architecture is aimed at processing sequential input while the structured data from the RotoWire dataset is in form of tables. Therefore following \citet{wiseman2017} we show the methods that keep most of the valuable positional information while transforming tabular data into a sequence of records. We also discuss the neural network architectures that learn what kind of information is stored in a particular record (section \ref{subsection:baseline_model_encoder}), and which records are complementary to each other (section \ref{subsection:content_selection}).

We hypothesize that the biggest performance bottleneck of our approach is the complexity of the input tables. Therefore we propose two methods which aim to reduce the complexity.

The quality of generated texts is measured using the BLEU score, and a set of manual metrics (Entity Recall measures how many of the entities mentioned in the gold summaries are also in the generated ones, while Factual Correctness measures how many of the generated numerical facts are grounded on the statistics from the input tabular data). We show that enhancements of the Encoder-Decoder architecture as well as the reduction of complexity of the input tables help the performance of our models.

All the models and preprocessing methods are developed in \emph{python 3.8} and \emph{tensorflow 2.4.1}. However the code is compatible with \emph{python 3.6} and \emph{tensorflow 2.3} (the versions used on Artifical Intelligence Cluster (AIC) where the training was executed).