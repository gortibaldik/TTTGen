\chapter*{Introduction} \label{chapter:introduction}
\addcontentsline{toc}{chapter}{Introduction}

Language Modeling is the task of predicting what word comes next in the actual context. In this thesis we delve into a slightly more complicated task, into the Conditional Language Modeling, where the next generated word does not only depend on the context but also on some external factors.

Specifically we examine ways of conditionally generating document-scale texts given structured input data. We train Deep Neural Network models on recently introduced RotoWire dataset \citep{wiseman2017} which contains statistical information about basketball matches paired with professionally written summaries collected from RotoWire fantasy sports news website.

In chapter \ref{chapter:data} we introduce the dataset, and reason about its characteristics. We show that the dataset is well suited for the task as the summaries are closely related to the input tabular data.

We follow up by collecting a set of statistics of the dataset. We propose several preprocessing methods which aim to simplify the learning path of the model, and show the resulting statistics of the preprocessed dataset (chapter \ref{chapter:preprocessing_and_statistics_of_the_dataset}).

Chapter \ref{chapter:neural_network_architectures} introduces several neural network architectures (Encoder-Decoder, Attention, Copy Mechanism) which serve as building blocks of the models prepared in chapter \ref{chapter:experimental_setup}. The models are built incrementally following approaches of \citet{wiseman2017} and \citet{puduppully2019datatotext}, starting with a baseline model and ending with Content Selection and Planning model.

Finally we present the experiments (chapter \ref{chapter:experiments}). Firstly, we discuss the evaluation of different aspects of the generated summaries (both automated and manual evaluation takes place). Next, we analyse the performance of the models prepared in chapter \ref{chapter:experimental_setup}. We manage to improve from BLEU score $10$ for the baseline model to $13.96$ for the Copy model with Content Selection encoder. We hypothesise that the performance bottleneck of our approach is the complexity of the input tables and propose two methods aiming to simplify the inputs. We show that our best model trained on the simplified inputs outperforms the Copy model with Content Selection encoder in all the evaluated metrics.

All the models and preprocessing methods are developed in \emph{python 3.8} and \emph{tensorflow 2.4.1}. The implementation is discussed in appendix \ref{chapter:appendix_a}.