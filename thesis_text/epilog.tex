\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

In this thesis, we explored document-scale text generation conditioned on the basketball match statistics. In chapter \ref{chapter:data} we presented the RotoWire dataset. We analysed the statistics of the dataset and came up with numerous preprocessing and cleaning methods (chapter \ref{chapter:preprocessing_and_statistics_of_the_dataset}). The rest of the thesis focused on creating building blocks, composing them into models (chapters \ref{chapter:neural_network_architectures}, \ref{chapter:experimental_setup}) and the evaluation and analysis of the experiments (chapter \ref{chapter:experiments}).

We followed the path set up by \citet{wiseman2017}. At first we trained a baseline Encoder-Decoder model with Attention that produced texts of reasonable quality although completely hallucinated facts. Next we added joint-copy mechanism to the Decoder part of the model, which caused massive improvement in terms of entity recall as well as factual correctness (the model is better at selecting important entities and produces correct facts about them). We choose to not implement the Conditional Copy mechanism proposed by Wiseman.

Inspired by \citet{puduppully2019datatotext} we experimented with Content Selection encoder + joint-copy decoder model (CopyCS), and with Content Selection and Planning model (CS\&P). While both of the models helped to improve the quality of the generated summaries in terms of all the measured metrics, we are not satisfied with the texts as they contain a lot of repetitions and many times they are not properly finished (the models do not learn to place the \emph{\textless EOS\textgreater} token). We think that it is caused by wrong hyperparameter settings, and a future work may be focused e.g. on applying learning rate decay, using layer normalization etc.

Analysing the content plans generated by CS\&P model hypothesized that the performance bottleneck may be the complexity of the input sequence of the records. Therefore at first we ordered the records and then we removed many unsignificant records from the inputs and trained CS\&P as well as the Copy models on the new data. The results prooved our hypothesis as we were able to obtain the biggest BLEU score (which improved from 10 for the baseline to 15 for the CS\&P model trained on shortened tables), entity recall and second biggest factual correctness out of all the trained models.

However the generated summaries do not match any of our expectations. While our best model learned to extract information about the teams, it still suffers from selecting wrong players to talk about (as the entity recall for our best model is only about 50\%), and copying wrong information (the factual correctness is on the same level as the entity recall).

We believe that future work should focus on improving the Content Planning mechansim, as we saw that CS\&P model generating from gold content plans hugely outperformed our best model on the BLEU score (22.8 vs 15.1), the Entity Recall (71.4\% vs 50.6\%) as well as on the factual correctness (54.69\% vs 49.87\%). We are also interested in the Transformer architecture for text generation \citep{vaswani2017attention}, specifically we would like to try the few-shot setting \citep{chen2020fewshot}.