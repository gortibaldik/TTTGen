\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

In this thesis, we explored document-scale text generation conditioned on the basketball match statistics. In chapter \ref{chapter:data} we presented the RotoWire dataset. We analysed the statistics of the dataset and came up with numerous preprocessing and cleaning methods (chapter \ref{chapter:preprocessing_and_statistics_of_the_dataset}). In the rest of the thesis we focused on creating building blocks, composing them into models (chapters \ref{chapter:neural_network_architectures}, \ref{chapter:experimental_setup}) and evaluating and analysing the experiments (chapter \ref{chapter:experiments}).

We followed the path set up by \citet{wiseman2017}. First we trained a baseline Encoder-Decoder model with Attention that produced texts of reasonable quality. However it suffered from severe \emph{fact hallucinations} (e.g. the baseline learned to generate \emph{some} number when describing how many points a player scored, instead of generating \emph{the exact value}). Next, we added joint-copy mechanism to the Decoder part of the model, which caused massive improvement in terms of entity recall as well as factual correctness (the model is better at selecting important entities and produces correct facts about them).

Inspired by \citet{puduppully2019datatotext}, we experimented with Content Selection encoder + joint-copy decoder model (CopyCS), and with Content Selection and Planning model (CS\&P). While both of the models helped to improve the quality of the generated summaries in terms of all measured metrics, we were not satisfied with the texts as they contained a lot of repetitions and in many cases they were not properly finished (the models did not learn to place correctly the \emph{\textless EOS\textgreater} token). Our hypothesis is that it is caused by suboptimal hyperparameter settings, and a future work may be focused e.g. on applying learning rate decay, using layer normalization~etc.

Analysing the content plans generated by CS\&P model we hypothesized that the performance bottleneck may be the complexity of the input sequence of the records. Therefore we ordered and removed many unsignificant records from the inputs and trained CS\&P as well as the Copy models on the new data. The results prooved our hypothesis as we were able to obtain the highest BLEU score (which improved from 10 for the baseline to 15 for the CS\&P model trained on shortened tables), entity recall and second highest factual correctness out of all trained models.

The generated summaries did not fully match of our expectations. While our best model learned to extract information about the teams, it still suffers from selecting wrong players to talk about (as the entity recall for our best model is only about 50\%), and copying wrong information (the factual correctness is on the same level as the entity recall).

We believe that future work should focus on improving the Content Planning mechansim, as we saw that CS\&P model generating from gold content plans hugely outperformed our best model on the BLEU score (22.8 vs 15.1), the Entity Recall (71.4\% vs 50.6\%) as well as on the factual correctness (54.69\% vs 49.87\%). It might be of interest to use the Transformer architecture for text generation part of the task \citep{vaswani2017attention}. Specifically we are considering the few-shot setting with pretrained transformer language models \citep{chen2020fewshot}.