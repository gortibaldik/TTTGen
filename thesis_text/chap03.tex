\chapter{Preprocessing and Statistics of the Datasets} \label{chapPreproc}

To generate text from structured data I choose the Deep Neural Networks and specifically the Encoder-Decoder (ED) neural architecture (section \ref{neural_nets_chapter}). The ED suited to process sequential one dimensional data, however we cope with two dimensional tables. In this chapter I describe how I handle the problem. In addition I further analyze the data and propose cleaning methods and reason about motivations for selected techniques.

\section{Transforming Tables to Records} \label{table_to_record_trans}

At first let's define a table. A table is a two dimensional data structure, where the information is stored not only in the actual values in cells, but also in the positional information. Values in the same column have the same type, whereas values in the same row belong to the same entity. An example of the table as we have defined it is in figure \ref{ex_struct}.

\begin{table}[h]
    \centering
    \begin{tabular}{llll}
        \toprule
        {} & type$_1$ & type$_2$ \dots \\
        \midrule
        entity$_1$ & value$_{1,1}$ &  value$_{1,2}$ \dots \\
        entity$_2$ & value$_{2,1}$ & value$_{2,2}$ \dots \\
        \dots &&
    \end{tabular}
    \caption{An example of structured data} \label{ex_struct}
\end{table}

I use the same notation as \citep{liang-etal-2009-learning}. Table $\mathcal{T}$ is transformed into a sequence of records $ \mathbf{s} = \{ r_i \}_{i=1}^{J} $, where $r_i$ denotes i-th record. To fulfill our goal of keeping the most of the positional information from the table, each record contains field $r.type$ denoting the type of the value, the actual value $r.value$ and the entity $r.entity$ to which the record belongs.

\tikzstyle{example_style} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, fill=yellow!10, align=left]

\section{RotoWire} \label{rotowire_preproc_section}

Firstly I present the statistics of the dataset \emph{before any preprocessing}. Next I elaborate about the particularities of the input tables. After working with the dataset for a while I found many of its deficiencies. I'm not the first one who spotted the flaws and there exist datasets based on RotoWire, which contain cleaner data and summaries corresponding better to input tables \citep{wang-2019-revisiting}, \citep{thomson-2020-sportsett}. However I choose to continue with the RotoWire dataset, as I am already accustomed to the format of the data.

\subsection{Dataset Statistics} \label{assumptions_ref}

I believe that the challenges posed by the RotoWire dataset can be summarized in a set of statistics. In this subsection I want to present the most important ones to help reader to understand the nature of the problem. 

Firstly, the target summaries as well as the sequences of input records are really long compared to other datasets modelling the same task (e.g. WikiBIO, WeatherGOV, RoboCup \emph{XXXXXX}).

Secondly, many words occur rarely and the generation system cannot learn a good representation of them. It should be noted that the problem can be resolved by the means of clever preprocessing (section \ref{bpeSection}) or with help of advanced neural architectures (section \ref{copy_mech_sec}).

Thirdly, there are many values which represent facts, e.g. values that cannot be deduced from the context (e.g. points a player scored in a match etc.) but must be selected and copied from the table.

The original dataset as prepared by \citep{wiseman2017}, is already divided to train (3398 samples), development (727 samples) and test (728 samples) sets. \emph{In the statistics presented below I state that there are only 3397 samples in the train set because one of the samples is the famous Lorem Ipsum template.}

\subsubsection{Length-wise Statistics}

The input tables contain huge amount of information. 2 teams and up to 30 players participate in a match of basketball. After transformation to a sequence, a player is represented by 24 and a team by 15 records. The type field $r.type$ is the only trait distinguishing the team and player records. Table \ref{stats_tables_orig_rw} summarizes the length statistics of the input sequences. 

The generation system should be able to understand the meaning of a record and shouldn't rely on specific organisation of the table. This is modelled by emplacing the team records at the end, so that the system will need to search for the team statistics. Since the size of the input sequence isn't uniform, the team records can start anywhere between 500th and 720th record. The organization of the table will be further explained in the chapter about the experiments \ref{experiments_chapter}.

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}    & \textbf{Max} & \textbf{Min} & \textbf{Avegage}& {} \\
        \textbf{Set} & \textbf{Number of} & \textbf{Number of} & \textbf{Number of} & \textbf{Size} \\
        {} & \textbf{Records} & \textbf{Records} & \textbf{Records} & {} \\
        \midrule
        train       & 750 & 558 & 644.65 & 3397  \\
        development & 702 & 582 & 644.66 & 727 \\
        test        & 702 & 558 & 645.03 & 728
    \end{tabular}
    \caption{Statistics of tables as used by \citep{wiseman2017}} \label{stats_tables_orig_rw}
\end{table}

There is much greater variance in the lengths of output summaries. While the longest input sequence is 134\% of the shortest one, from the data in table \ref{stats_sums_orig_rw}, we can see that the factor between the outputs is 511\%. The size of the inputs and the outputs places high memory and computation demands on the GPUs used for training, and needs a special treatment (as will be explained in section \ref{truncated_backprop_subsection}).

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}    & \textbf{Max} & \textbf{Min} & \textbf{Avegage}& {} \\
        \textbf{Set} & \textbf{Summary} & \textbf{Summary} & \textbf{Summary} & \textbf{Size} \\
        {} & \textbf{Length} & \textbf{Length} & \textbf{Length} & {} \\
        \midrule
        train      & 762 & 149 & 334.41 & 3397  \\
        validation & 813 & 154 & 339.97 & 727 \\
        test       & 782 & 149 & 346.83 & 728
    \end{tabular}
    \caption{Statistics of summaries as used by \citep{wiseman2017}} \label{stats_sums_orig_rw}
\end{table}


\subsubsection{Occurrences of Unique Tokens}

While the length of the inputs and the outputs increases computational demands, the low occurrency of certain token causes the \emph{rare word problem}. After discussions with my advisor I think it is reasonable to expect that the system should learn a good representation of a token if it appears at least 5 times in the train set.

There is about 11 300 unique tokens in the dataset (in the union of train, development and test set). In table \ref{stats_occur_rw} I present the statistics regarding the occurrences of the tokens. We can see that only 42 \% of all the tokens appear at least 5 times in the train part of the dataset.

However I expect that even if some anomaly in the real word happens (e.g. team scores 200 points, although at the time of writing no team in the history of NBA scored more than 186) the system should be able to simply \emph{copy} the value of \emph{TEAM-PTS} record without reasoning about the actual value. Consequently I present also the statistics of tokens that cannot be copied from the table. At last, since most of the named entities can be directly copied, there is no need to keep casing as it doesn't represent any valuable information.

In the end we see that under our assumptions about 60 \% of all the unique tokens cannot be learned by the generation system.

\begin{table}[h!]
    \centering
    \begin{tabular}{cccc}
        \toprule
        {}    & \textbf{Unique} & \textbf{$>= 5$} & \textbf{$>= 5$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} & \textbf{Absolute} & \textbf{Relative}\\
        \midrule
        train      & 9779 & 4158 & 42.52\% \\
        train\_wop$_1$ & 8604 & 3296 & 38.31\% \\
        train\_wopl$_2$ & 8031 & 3119 & 38.84\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize \textit{Note:} $_1$ train\_wop is training set with all the player names, city names, } \\
        \multicolumn{4}{l}{\footnotesize team names and numbers extracted $_2$ train\_wopl is train\_wop lowercased}
    \end{tabular}
    \caption{Occurrences of tokens in summaries from dataset RotoWire} \label{stats_occur_rw}
\end{table}

In table \ref{stats_overlap_rw} we can see how many of the unique tokens learned during training can be found in the respective development and test datasets. Under our assumptions we can expect the generated text to share less than 65 \% of the vocabulary with the gold references. 

\begin{table}[h!]
    \centering
    \begin{tabular}{cccc}
        \toprule
        {}    &  \textbf{Unique} &\textbf{Train} & \textbf{Train$_{>=5}$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} &\textbf{Overlap} & \textbf{Overlap} \\
        \midrule
        valid      & 5625 & 88.18\% & 66.63\% \\
        test       & 5741 & 87.46\% & 65.72\% \\
        \hline
        valid\_wop$_1$      & 4714 & 86.36\% & 61.92\% \\
        test\_wop$_2$       & 4803 & 86.03\% & 61.13\% \\
        \hline
        valid\_wopl$_3$      & 4442 & 86.74\% & 62.36\% \\
        test\_wopl$_4$       & 4531 & 86.32\% & 61.37\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize \textit{Note:} train$_{>=5}$ is a set of all the tokens with more than} \\
        \multicolumn{4}{l}{\footnotesize 5 occurrences in the train dataset summaries $_1$, $_2$, $_3$, $_4$} \\
        \multicolumn{4}{l}{\footnotesize have the same meaning as in table \ref{stats_occur_rw}}
    \end{tabular}
    \caption{Overlap of train dataset summaries and valid/test dataset summaries} \label{stats_overlap_rw}
\end{table}

\subsection{Transformations of Input Tables} \label{trans_in_tb_rw}

Firstly I want to talk about the preprocessing of actual values stored in the cells of the table. After that I present the format of a record which is fed to the generation system.

\subsubsection{Tabular Types}

There are 39 types (different headers of columns as discussed in section \ref{table_to_record_trans}). A type is associated to textual or integer value which describes either a team or an individual. There are only 7 types bound to textual values (\emph{TEAM-NAME}, \emph{TEAM-CITY}, \emph{FIRST\_NAME}$^*$, \emph{SE\-COND\-\linebreak[4]\_NAME}$^*$, \emph{PLAYER\_NAME}$^*$, \emph{START\_POSITION}$^*$, \emph{TEAM\_CITY}$^*$)\footnote{$^*$ asterisk labels types bound to an individual}

\subsubsection{Numerical values}

The other 33 types desribe absolute integer values (\emph{TEAM-PTS}, \emph{FTM}\footnote{Number of converted free throws by an individual, \emph{"free throws made"}} \dots) or relative integer values (\emph{TEAM-FT\_PCT}, \emph{FT\_PCT}\footnote{\emph{player free throw percentage}}, \dots). During preprocessing no changes are made to any tabular numerical value\footnote{As described in \citep{wiseman2017} all the relative values are converted to integers. I don't consider this as a preprocessing since only the converted values are available in the dataset.}.

\subsubsection{Textual values} \label{trans_p_nms}

Regarding the textual values, I consider each as a single token. Since the names of teams are already one word long (with one exception needing a transformation \emph{Trail Blazers $\rightarrow$ Trail\_Blazers}) the transformation is rather trivial. The similar observation applies to names of cities (with 6 exceptions: \emph{Oklahoma\_City}, \emph{San\_Antonio}, \emph{New\_Orleans}, \emph{Los\_Angeles}, \emph{Golden\_State}, \emph{New\_York}), and start positions.

All the player names consist of at least 2 words (with one exception: \emph{Nene}). As being discussed earlier, we want as many values from the table to be copiable as possible.

The authors of the dataset \citep{wiseman2017}, as well as the authors of one of the more successful approaches to the dataset \citep{puduppully2019datatotext} make use of three special types, \emph{PLAYER\_NAME}, \emph{FIRST\_NAME} and \emph{SECOND\_NAME} which allow to distinguish if \emph{James} refers to the first name of star player \emph{James Harden} or to a second name of the legend of \emph{LeBron James}.

My approach is based on the idea of making data as dense as possible to make the learning of the generation system easier. Therefore I transform the name of each player to a single token. To make the copying possible a preprocessing of the output summaries as described in \ref{player_nm_trans_summary} must take place. 

Consequently \emph{FIRST\_NAME} and \emph{SECOND\_NAME} records aren't needed anymore which results in another benefit, shorter inputs. (as there are 22 - 30 players involved in the match, the size of inputs becomes about 10 \% (44 - 60 records) shorter)

\subsubsection{Entities}

Keďže vstupná tabuľka obsahuje informácie o obidvoch hrajúcich tímoch a\linebreak[4]všetkých hráčoch na súpiskách, každý záznam musí obsahovať aj hodnotu~$r.e$ popisujúcu, ktorú entitu záznam charakterizuje. Vďaka transformácii mien\linebreak[4]hráčov a tímov, môžeme ako entitu použiť názov tímu, resp. meno hráča. Okrem toho však ku každému záznamu pridáme špeciálnu položku $r.ha$, ktorá symbolizuje, či sa záznam vzťahuje ku domácemu, alebo hosťujúcemu tímu. 

\subsubsection{Record Format}

Nakoniec teda používam záznamy, ktoré obsahujú položky $r.f$ - typ záznamu, $r.e$ - entita, $r.v$ - hodnota a $r.ha$ - domáci/hostia. (tu by mal byť pridaný príklad, ako to vyzerá)

Kvôli tomu vykonávame také transformácie, aby sa jednak vyskytovalo čo najviac tokenov čo najčastejšie, a aby v sumároch boli používané rovnaké tokeny ako v tabuľke.

\subsubsection{Number Transformations} \label{num_trans_rw}

Rovnako ako \citep{wiseman2017} a \citep{puduppully2019datatotext}, reprezentujeme čísla len pomocou číslic. Táto transformácia je motivovaná druhým predpokladom a teda tým, že dúfame, že väčšinu čísel bude neurónová sieť kopírovať a nie generovať. To znamená, že napríklad token $five$ transformujeme na $5$. Používame na to knižnicu text2num \footnote[1]{\url{https://github.com/allo-media/text2num}}. Niektoré čísla však majú v basketbalovej terminológii špeciálny význam. Špecificky teda netransformujeme číslo $three$, pretože sa často vyskytuje ako súčasť slovných spojení \emph{three pointer, three pt range \dots}

\subsubsection{Player name transformations} \label{player_nm_trans_summary}

Ako už vyplýva z \ref{trans_p_nms} a z predpokladov, ktoré sme si stanovili, chceme v texte reprezentovať jednoho hráča práve jedným tokenom. To platí aj pre extrémne prípady ako \emph{Luc Richard Mbah a Moute}, ktorý bol v datasete reprezentovaný šiestimi rôznymi kombináciami elíps v jeho mene. Najčastejšie však vychádza, že v texte sa najprv hráč spomenie a následne sa už oslovuje len priezviskom a až na 17 prípadov (čo činí 2.4\% zo 668 hráčov spomenutých aspoň v jednej tabuľke) je meno hráča vždy zložené z 2 tokenov. Na transformáciu mien hráčov používam jednoduchý algoritmus, ktorý napriek tomu, že nerieši všetky referencie úplne presne, dosahuje na oko slušnú presnosť.

\begin{figure}[!h]
\centering
\usetikzlibrary{shapes.multipart}
\begin{tikzpicture}
\node (original) [example_style, text width = 0.95*\columnwidth] {
    While King James struggled , James Harden was busy putting up a triple - double on the Detroit Pistons on Friday
};
\node (transformed) [example_style, text width = 0.95*\columnwidth, below=5mm of original]{
    while king LeBron\_James struggled , James\_Harden was busy putting up a triple - double on the Detroit Pistons on friday.
};
\draw [->] (original) edge (transformed);
\end{tikzpicture}
\caption{Example of transformation of player names leveraging the knowledge of players on the rosters} \label{cmp_original_vs_mine}
\end{figure}

Transformácia prebieha v troch krokoch. Teraz sa ich pokúsim popísať na príklade transformácie vety z figure \ref{cmp_original_vs_mine}:
\begin{itemize}
    \item \textbf{1. Extrakcia mien hráčov zo sumáru} \hfill \\
    Vytiahneme zo sumáru mená \emph{James} a \emph{James Harden}, ktoré sú súčasťou mena nejakého z hráčov, ktorí sú známi z korpusu
    \item \textbf{2. Vyriešenie referencií pre daný sumár a vytvorenie slovníka transformácií} \hfill \\
    Pokúsime sa zistiť, na čo odkazuje jednotokenové meno \emph{James}. Explicitne zakazujeme, aby sa vnímalo ako krstné meno a v boxscore daného zápasu nájdeme, že doň zasiahol niekto s menom \emph{LeBron James}. Do slovníka transformácií teda umiestnime transformáciu \emph{James $\rightarrow$ LeBron\_James}. \emph{James Harden} je viac tokenové meno, ktoré sme v prvom kroku rozoznali ako meno hráča, teda do slovníka pridáme len prepis \emph{James Harden $\rightarrow$ James\_Harden}
    \item 3. Aplikácia transformácií na sumár \hfill \\
    Prechádzame text a na najdlhšiu namatchovanú postupnosť aplikujeme transformáciu.
\end{itemize}

\subsection{Byte Pair Encoding} \label{bpeSection}

Pokiaľ by sme sa rozhodli, že ako slovník použijeme len slová, ktoré generačný systém pozná z tréningového datasetu, museli by sme nahradiť viac ako 10\% unikátnych slov z validačného, či testovacieho datasetu špeciálnymi \emph{UNK} tokenmi. Namiesto toho, sme sa rozhodli použiť prístup, ktorý predstavil \citep{sennrich2016}. Najprv popíšeme algoritmus, následne ukážeme, ako pomohol prekonať problém, o ktorom rozprávame.

\subsubsection{Algorithm}

Používame implementáciu algoritmu priamo od autorov\footnote{\url{https://github.com/rsennrich/subword-nmt}}, ktorá sa dá stiahnuť aj ako samostatný python package. Minimálny príklad možnej implementácie v pythone je ukázaný ako algoritmus \ref{bpe-algorithm}. 

Ako prvý krok sa každé slovo rozdelí na znaky a na koniec každého slova sa pridá špeciálny znak  \emph{\textless eow\textgreater}, ktorý umožní detokenizáciu na pôvodný text. Vstupný slovník sa inicializuje na množinu unikátnych znakov v texte (okrem bielych znakov, algoritmus sa týka len slov). Algoritmus mnohokrát prechádza text a pri každom prechode nájde najčastejšiu dvojicu za sebou idúcich znakov a spojí ju do jednoho nového znaku. Tento znak sa pridá do slovníka a všetky výskyty daných dvoch znakov za sebou sa nahradia novým znakom. 

Veľkosť výstupného slovníka je teda počet unikátnych znakov v pôvodnom texte ($+1$ kvôli \emph{\textless eow\textgreater} tokenu) $+$ počet prebehnutých iterácií.

Algoritmus má teda jeden hyperparameter a to počet iterácií, a teda veľkosť výstupného slovníka.

\subsubsection{Statistics of Transformed Dataset}

Vyskúšali sme tri možnosti nastavenia hyperparametru BPE\footnote{Možnosti nastavenia počtu iterácií BPE spolu so štatistikami s nimi spojenými sú prístupné na \url{https://github.com/gortibaldik/TTTGen/blob/master/rotowire/dataset_stats.md}}. Nakoniec sme zvolili 2000 iterácií algoritmu. Z procesu spájania tokenov sú vybrané čísla a mená hráčov, keďže chceme, aby tieto tokeny generačný systém priamo kopíroval zo vstupnej tabuľky. Tým pádom prienik tréningového a validačného (resp. testovacieho) datasetu nie je 100\%.\footnote{Konkrétne spolu 92 tokenov (31 unikátnych tokenov) z validačného a 87 tokenov (34 unikátnych tokenov) je nahradených UNK tokenom vo validačných dátach.}Výsledné štatistiky po všetkých transformáciách sú v tabuľkách \ref{table_train_final_rw} a \ref{table_vt_final_rw}.

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{cccc}
        \toprule
        {}    & \textbf{Unique} & \textbf{$>= 5$} & \textbf{$>= 5$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} & \textbf{Absolute} & \textbf{Relative}\\
        \midrule
        train      & 2839 & 2531 & 89.15\%
    \end{tabular}
    \caption{\small Occurrences of tokens in transformed summaries from dataset RotoWire} \label{table_train_final_rw}
\end{table}

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{cccc}
        \toprule
        {}    &  \textbf{Unique} &\textbf{Train} & \textbf{Train$_{>=5}$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} &\textbf{Overlap} & \textbf{Overlap} \\
        \midrule
        valid                & 2582 & 98.80\% & 95.70\% \\
        test                 & 5741 & 98.69\% & 95.45\%
    \end{tabular}
    \caption{\small Overlap of transformed train dataset summaries and valid/test dataset summaries} \label{table_vt_final_rw}
\end{table}

% nice formatting of python code
% https://tex.stackexchange.com/questions/105662/default-value-for-basicstyle-in-lstlisting/122916#122916
\lstdefinestyle{shared}
{
    numbers=left,
    numbersep=1em,
    numberstyle=\tiny\color{red},
    frame=single,
    framesep=\fboxsep,
    framerule=\fboxrule,
    rulecolor=\color{red!20},
    linewidth=13.7cm,
    breaklines=true,
    tabsize=2,
    columns=flexible,
}

\lstdefinestyle{python}
{
    style=shared,
    escapechar=\^,
    language={Python},
    basicstyle=\small\tt,
    keywordstyle=\color{blue},
    commentstyle=\color[rgb]{0.13,0.54,0.13},
    backgroundcolor=\color{cyan!5},
}

\lstnewenvironment{python}
{\lstset{style=python}}
{}

\begin{algorithm}[!h]
\begin{python}
import re, collections

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
    symbols = word.split()
    for i in range(len(symbols)-1):
        pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
    w_out = p.sub(''.join(pair), word)
    v_out[w_out] = v_in[word]
    return v_out

vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,
            'n e w e s t </w>':6, 'w i d e s t </w>':3}
num_merges = 10
for i in range(num_merges):
    pairs = get_stats(vocab)
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(best)

> r ^$\cdot$^ ^$\rightarrow$^ r^$\cdot$^
> l o ^$\rightarrow$^ lo
> lo w ^$\rightarrow$^ low
> e r^$\cdot$^ ^$\rightarrow$^ er^$\cdot$^
\end{python}
\caption{Learn BPE operations \\ Extract from paper \textbf{Neural Machine Translation of Rare Words with Subword Units} by \citep{sennrich2016}}
\label{bpe-algorithm}
\end{algorithm}
