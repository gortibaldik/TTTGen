\chapter{Preprocessing and Statistics of the Datasets} \label{chapPreproc}

To generate text from structured data I choose the Deep Neural Networks and specifically the Encoder-Decoder neural architecture (section \ref{neural_nets_chapter}). These ap\-\linebreak[4]proaches are suited to process sequential one dimensional data, however we cope with two dimensional tables. In this chapter I describe how I handle the problem. In addition I further analyze the data and propose cleaning methods and reason about motivations for selected techniques.

\section{Transforming Tables to Records} \label{table_to_record_trans}

A table is a two dimensional data structure, where the information is stored not only in the actual values in cells, but also in the positional information. To feed the table to the record I flatten it to a sequence of records. When transforming the table, the goal is to keep as much of the positional information as possible.

At first let's define a table. Values in the same column have the same type, whereas values in the same row belong to the same entity. An example of the table as we have defined it is in figure \ref{ex_struct}.

\begin{table}[h]
    \centering
    \begin{tabular}{llll}
        \toprule
        {} & field$_1$ & field$_2$ \dots \\
        \midrule
        entity$_1$ & value$_{1,1}$ &  value$_{1,2}$ \dots \\
        entity$_2$ & value$_{2,1}$ & value$_{2,2}$ \dots \\
        \dots &&
    \end{tabular}
    \caption{An example of structured data} \label{ex_struct}
\end{table}

\subsection{Notation}

I use the same notation as \citep{liang-etal-2009-learning}. Table $\mathcal{T}$ is transformed into a sequence of records $ \mathbf{s} = \{ r_i \}_{i=1}^{J} $, where $r_i$ denotes i-th record. To fulfill our goal of keeping the most of the positional information from the table, each record contains field $r.type$ denoting the type of the value, the actual value $r.value$ and the entity $r.entity$ to which the record belongs.

\tikzstyle{example_style} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, fill=yellow!10, align=left]

\section{RotoWire} \label{rotowire_preproc_section}

Firstly I present the statistics of the dataset \emph{before any preprocessing}. Next I elaborate about the particularities of the input tables from the dataset. After working with the dataset for a while I found many of its deficiencies. Although there are datasets based on this one, which contain summaries corresponding better to input tables \citep{wang-2019-revisiting}, \citep{thomson-2020-sportsett} I choose to continue with this one, as I was already accustomed to the format of the data.

\subsection{Dataset Statistics} \label{assumptions_ref}

Keďže transformácie sumárov sú výrazne ovplyvnené štatistikami datasetu, najprv si dovolím predstaviť to, ako vyzerajú tieto štatistiky. Rovnako ako pri datasete WikiBIO, aj teraz používam originálny train-valid-test split (3397, 727, 728). Pri spracovaní datasetu vychádzam z 2 predpokladov:
\begin{itemize}
\item Neurónová sieť sa môže naučiť generovať určitý token, pokiaľ sa v\linebreak[4]tréningových dátach vyskytuje aspoň päťkrát.
\item Je jednoduchšou úlohou kopírovať dáta z tabuľky, ako generovať zo skrytého stavu.
\end{itemize}

Prezentované štatistiky sú s preprocessingom, ktorý používa \citep{wiseman2017} \footnote{V datasete sa však vyskytla jedna nezdokumentovaná chyba, ktorú som odstránil, jeden sumár bol známym textom Lorem Ipsum, ten je z datasetu, ktorý používam odstránený a teda trénovacie dáta obsahujú 3397 položiek a nie 3398, ako uvádza \citep{wiseman2017} \label{footnote_wis_fault_li}}, neskôr, v sekcii \ref{bpeSection} ukážem štatistiky, ktoré sú po aplikácii všetkých úprav na datasete.

\subsubsection{Length-wise Statistics}

Jeden hráč je v tabuľke zastúpený 24 záznamami, jeden tím 15 záznamami. Každá tabuľka obsahuje záznamy o 2 tímoch. Podľa dát z tabuľky \ref{stats_tables_orig_rw} teda ostáva 528 až 720 záznamov o hráčoch, teda v jednej tabuľke sa hovorí o 22 - 30 hráčoch. Z toho vyplýva, že generačný systém nemá úlohu ešte sťaženú rôznorodosťou tabuliek, keďže úseky, kde sa rozpráva o hráčoch a o tímoch sú relatívne rovnaké vo všetkých tabuľkách.

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}    & \textbf{Max} & \textbf{Min} & \textbf{Avegage}& {} \\
        \textbf{Set} & \textbf{Summary} & \textbf{Summary} & \textbf{Summary} & \textbf{Size} \\
        {} & \textbf{Length} & \textbf{Length} & \textbf{Length} & {} \\
        \midrule
        train      & 750 & 558 & 644.65 & 3397  \\
        validation & 702 & 582 & 644.66 & 727 \\
        test       & 702 & 558 & 645.03 & 728
    \end{tabular}
    \caption{Statistics of tables as used by \citep{wiseman2017}\textsuperscript{\ref{footnote_wis_fault_li}}} \label{stats_tables_orig_rw}
\end{table}

Na rozdiel od vstupných tabuliek, výstupné sumáre sú čo sa týka dĺžky oveľa rôznorodejšie (ako je vidieť v tabuľke \ref{stats_sums_orig_rw}). Tu je podstatné hlavne to, že priemerná dĺžka sumáru je viac ako 330 tokenov a pokiaľ chceme generačný systém trénovať aj na dlhších sumároch, tak dĺžka tabuľky a sumáru nám vytvorí netriviálne pamäťové nároky na grafickú kartu a na RAM. (\emph{tu by mohol byť odkaz na sekciu o implementačných problémoch})

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}    & \textbf{Max} & \textbf{Min} & \textbf{Avegage}& {} \\
        \textbf{Set} & \textbf{Summary} & \textbf{Summary} & \textbf{Summary} & \textbf{Size} \\
        {} & \textbf{Length} & \textbf{Length} & \textbf{Length} & {} \\
        \midrule
        train      & 762 & 149 & 334.41 & 3397  \\
        validation & 813 & 154 & 339.97 & 727 \\
        test       & 782 & 149 & 346.83 & 728
    \end{tabular}
    \caption{Statistics of summaries as used by \citep{wiseman2017}\textsuperscript{\ref{footnote_wis_fault_li}}} \label{stats_sums_orig_rw}
\end{table}


\subsubsection{Frequency of Unique Tokens}

Teraz sa zameriam na štatistiky dôležité pre pochopenie motivácie pre ďalšie transformácie datasetu vzhľadom k skôr prezentovaným predpokladom \ref{assumptions_ref}. V tabuľke \ref{stats_occur_rw} je vidieť, že viac ako 61\% tokenov, ktoré nie sú číslami, menami hráčov, tímov, alebo miest sa v tréningovom datasete vyskytne menej ako 5-krát. Z toho sme usúdili, že je zrejme potrebné urobiť ďalšie transformácie na datasete, čím by sme zvýšili pravdepodobnosť, že sa generačný systém lepšie datasetu uspôsobí. 

\begin{table}[h!]
    \centering
    \begin{tabular}{cccc}
        \toprule
        {}    & \textbf{Unique} & \textbf{$>= 5$} & \textbf{$>= 5$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} & \textbf{Absolute} & \textbf{Relative}\\
        \midrule
        train      & 9779 & 4158 & 42.52\% \\
        train\_wop$_1$ & 8604 & 3296 & 38.31\% \\
        train\_wopl$_2$ & 8031 & 3119 & 38.84\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize \textit{Note:} $_1$ train\_wop is training set with all the player names, city names, } \\
        \multicolumn{4}{l}{\footnotesize team names and numbers extracted $_2$ train\_wopl is train\_wop lowercased}
    \end{tabular}
    \caption{Occurrences of tokens in summaries from dataset RotoWire} \label{stats_occur_rw}
\end{table}

Podobné závery možno urobiť aj zo štatistík prekryvu slovníkov trénovacích a validačných (resp. testovacích) dát.

\begin{table}[h!]
    \centering
    \begin{tabular}{cccc}
        \toprule
        {}    &  \textbf{Unique} &\textbf{Train} & \textbf{Train$_{>=5}$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} &\textbf{Overlap} & \textbf{Overlap} \\
        \midrule
        valid      & 5625 & 88.18\% & 66.63\% \\
        test       & 5741 & 87.46\% & 65.72\% \\
        \hline
        valid\_wop$_1$      & 4714 & 86.36\% & 61.92\% \\
        test\_wop$_2$       & 4803 & 86.03\% & 61.13\% \\
        \hline
        valid\_wopl$_3$      & 4442 & 86.74\% & 62.36\% \\
        test\_wopl$_4$       & 4531 & 86.32\% & 61.37\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize \textit{Note:} train$_{>=5}$ is a set of all the tokens with more than} \\
        \multicolumn{4}{l}{\footnotesize 5 occurrences in the train dataset summaries $_1$, $_2$, $_3$, $_4$} \\
        \multicolumn{4}{l}{\footnotesize have the same meaning as in table \ref{stats_occur_rw}}
    \end{tabular}
    \caption{Overlap of train dataset summaries and valid/test dataset summaries} \label{stats_overlap_rw}
\end{table}

\subsection{Transformations of Input Tables} \label{trans_in_tb_rw}

Ako je spomenuté v sekcii \ref{structured_data_rotowire}, hodnoty v tabuľke môžu byť buď mená tímov, hráčov, miest, alebo celé číslo oznamujúce percentuálnu, či absolútnu hodnotu nejakej štatistiky.

Preto má zmysel použiť rozdielny prístup ku spracovaniu hodnôt ako pri datasete WikiBIO. Konkrétne budeme každú hodnotu považovať za jeden token. Väčšina hodnôt je číselná (konkrétne $\frac{13}{15}$ z typov charakterizujúcich tímy a $\frac{19}{25}$ z typov charakterizujúcich hráčov. Následne mená tímov sú väčšinou dlhé práve jeden token, až na jednu výnimku, z ktorej sa vyrobí jeden token ( \emph{Trail Blazers $\rightarrow$ Trail\_Blazers}). Podobne pre mená tímov a výnimky ( \emph{Oklahoma City $\rightarrow$ Oklahoma\_City}, \emph{Golden State $\rightarrow$ Golden\_State}, \dots).

\subsubsection{Transformations of Player Names} \label{trans_p_nms}

Mená hráčov sú všetky, až na jednu výnimku (\emph{Nene}) viac-tokenové a autori, ktorých prístup ku danému datasetu som bral za referenciu (\citep{wiseman2017}, \citep{puduppully2019datatotext}) zvolili odlišný prístup ako ten, ktorý používam ja.

Referenčný prístup používa 2 špeciálne typy, \emph{first\_name} a \emph{last\_name}. Vďaka nim sa úloha pochopiť, či token \emph{James} odkazuje na krstné meno hviezdneho \emph{James Harden}, alebo na priezvisko legendy \emph{LeBron James} necháva na generačný systém.

Môj prístup je založený na myšlienke, že už na tak veľmi ťažkom datasete je dôležité vytvoriť čo najjednoduchšiu úlohu pre neurónovú sieť. Preto transformujem sumáre tak, aby meno každého hráča bolo reprezentované práve jedným tokenom. Práve preto strácajú typy \emph{first\_name} a \emph{last\_name} zmysel a vo vstupnej tabuľke ich nemám.

\subsubsection{Entities}

Keďže vstupná tabuľka obsahuje informácie o obidvoch hrajúcich tímoch a\linebreak[4]všetkých hráčoch na súpiskách, každý záznam musí obsahovať aj hodnotu~$r.e$ popisujúcu, ktorú entitu záznam charakterizuje. Vďaka transformácii mien\linebreak[4]hráčov a tímov, môžeme ako entitu použiť názov tímu, resp. meno hráča. Okrem toho však ku každému záznamu pridáme špeciálnu položku $r.ha$, ktorá symbolizuje, či sa záznam vzťahuje ku domácemu, alebo hosťujúcemu tímu. 

\subsubsection{Record Format}

Nakoniec teda používam záznamy, ktoré obsahujú položky $r.f$ - typ záznamu, $r.e$ - entita, $r.v$ - hodnota a $r.ha$ - domáci/hostia. (tu by mal byť pridaný príklad, ako to vyzerá)

Kvôli tomu vykonávame také transformácie, aby sa jednak vyskytovalo čo najviac tokenov čo najčastejšie, a aby v sumároch boli používané rovnaké tokeny ako v tabuľke.

\subsubsection{Number Transformations} \label{num_trans_rw}

Rovnako ako \citep{wiseman2017} a \citep{puduppully2019datatotext}, reprezentujeme čísla len pomocou číslic. Táto transformácia je motivovaná druhým predpokladom a teda tým, že dúfame, že väčšinu čísel bude neurónová sieť kopírovať a nie generovať. To znamená, že napríklad token $five$ transformujeme na $5$. Používame na to knižnicu text2num \footnote[1]{\url{https://github.com/allo-media/text2num}}. Niektoré čísla však majú v basketbalovej terminológii špeciálny význam. Špecificky teda netransformujeme číslo $three$, pretože sa často vyskytuje ako súčasť slovných spojení \emph{three pointer, three pt range \dots}

\subsubsection{Player name transformations}

Ako už vyplýva z \ref{trans_p_nms} a z predpokladov, ktoré sme si stanovili, chceme v texte reprezentovať jednoho hráča práve jedným tokenom. To platí aj pre extrémne prípady ako \emph{Luc Richard Mbah a Moute}, ktorý bol v datasete reprezentovaný šiestimi rôznymi kombináciami elíps v jeho mene. Najčastejšie však vychádza, že v texte sa najprv hráč spomenie a následne sa už oslovuje len priezviskom a až na 17 prípadov (čo činí 2.4\% zo 668 hráčov spomenutých aspoň v jednej tabuľke) je meno hráča vždy zložené z 2 tokenov. Na transformáciu mien hráčov používam jednoduchý algoritmus, ktorý napriek tomu, že nerieši všetky referencie úplne presne, dosahuje na oko slušnú presnosť.

\begin{figure}[!h]
\centering
\usetikzlibrary{shapes.multipart}
\begin{tikzpicture}
\node (original) [example_style, text width = 0.95*\columnwidth] {
    While King James struggled , James Harden was busy putting up a triple - double on the Detroit Pistons on Friday
};
\node (transformed) [example_style, text width = 0.95*\columnwidth, below=5mm of original]{
    while king LeBron\_James struggled , James\_Harden was busy putting up a triple - double on the Detroit Pistons on friday.
};
\draw [->] (original) edge (transformed);
\end{tikzpicture}
\caption{Example of transformation of player names leveraging the knowledge of players on the rosters} \label{cmp_original_vs_mine}
\end{figure}

Transformácia prebieha v troch krokoch. Teraz sa ich pokúsim popísať na príklade transformácie vety z figure \ref{cmp_original_vs_mine}:
\begin{itemize}
    \item \textbf{1. Extrakcia mien hráčov zo sumáru} \hfill \\
    Vytiahneme zo sumáru mená \emph{James} a \emph{James Harden}, ktoré sú súčasťou mena nejakého z hráčov, ktorí sú známi z korpusu
    \item \textbf{2. Vyriešenie referencií pre daný sumár a vytvorenie slovníka transformácií} \hfill \\
    Pokúsime sa zistiť, na čo odkazuje jednotokenové meno \emph{James}. Explicitne zakazujeme, aby sa vnímalo ako krstné meno a v boxscore daného zápasu nájdeme, že doň zasiahol niekto s menom \emph{LeBron James}. Do slovníka transformácií teda umiestnime transformáciu \emph{James $\rightarrow$ LeBron\_James}. \emph{James Harden} je viac tokenové meno, ktoré sme v prvom kroku rozoznali ako meno hráča, teda do slovníka pridáme len prepis \emph{James Harden $\rightarrow$ James\_Harden}
    \item 3. Aplikácia transformácií na sumár \hfill \\
    Prechádzame text a na najdlhšiu namatchovanú postupnosť aplikujeme transformáciu.
\end{itemize}

\subsection{Byte Pair Encoding} \label{bpeSection}

Pokiaľ by sme sa rozhodli, že ako slovník použijeme len slová, ktoré generačný systém pozná z tréningového datasetu, museli by sme nahradiť viac ako 10\% unikátnych slov z validačného, či testovacieho datasetu špeciálnymi \emph{UNK} tokenmi. Namiesto toho, sme sa rozhodli použiť prístup, ktorý predstavil \citep{sennrich2016}. Najprv popíšeme algoritmus, následne ukážeme, ako pomohol prekonať problém, o ktorom rozprávame.

\subsubsection{Algorithm}

Používame implementáciu algoritmu priamo od autorov\footnote{\url{https://github.com/rsennrich/subword-nmt}}, ktorá sa dá stiahnuť aj ako samostatný python package. Minimálny príklad možnej implementácie v pythone je ukázaný ako algoritmus \ref{bpe-algorithm}. 

Ako prvý krok sa každé slovo rozdelí na znaky a na koniec každého slova sa pridá špeciálny znak  \emph{\textless eow\textgreater}, ktorý umožní detokenizáciu na pôvodný text. Vstupný slovník sa inicializuje na množinu unikátnych znakov v texte (okrem bielych znakov, algoritmus sa týka len slov). Algoritmus mnohokrát prechádza text a pri každom prechode nájde najčastejšiu dvojicu za sebou idúcich znakov a spojí ju do jednoho nového znaku. Tento znak sa pridá do slovníka a všetky výskyty daných dvoch znakov za sebou sa nahradia novým znakom. 

Veľkosť výstupného slovníka je teda počet unikátnych znakov v pôvodnom texte ($+1$ kvôli \emph{\textless eow\textgreater} tokenu) $+$ počet prebehnutých iterácií.

Algoritmus má teda jeden hyperparameter a to počet iterácií, a teda veľkosť výstupného slovníka.

\subsubsection{Statistics of Transformed Dataset}

Vyskúšali sme tri možnosti nastavenia hyperparametru BPE\footnote{Možnosti nastavenia počtu iterácií BPE spolu so štatistikami s nimi spojenými sú prístupné na \url{https://github.com/gortibaldik/TTTGen/blob/master/rotowire/dataset_stats.md}}. Nakoniec sme zvolili 2000 iterácií algoritmu. Z procesu spájania tokenov sú vybrané čísla a mená hráčov, keďže chceme, aby tieto tokeny generačný systém priamo kopíroval zo vstupnej tabuľky. Tým pádom prienik tréningového a validačného (resp. testovacieho) datasetu nie je 100\%.\footnote{Konkrétne spolu 92 tokenov (31 unikátnych tokenov) z validačného a 87 tokenov (34 unikátnych tokenov) je nahradených UNK tokenom vo validačných dátach.}Výsledné štatistiky po všetkých transformáciách sú v tabuľkách \ref{table_train_final_rw} a \ref{table_vt_final_rw}.

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{cccc}
        \toprule
        {}    & \textbf{Unique} & \textbf{$>= 5$} & \textbf{$>= 5$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} & \textbf{Absolute} & \textbf{Relative}\\
        \midrule
        train      & 2839 & 2531 & 89.15\%
    \end{tabular}
    \caption{\small Occurrences of tokens in transformed summaries from dataset RotoWire} \label{table_train_final_rw}
\end{table}

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{cccc}
        \toprule
        {}    &  \textbf{Unique} &\textbf{Train} & \textbf{Train$_{>=5}$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} &\textbf{Overlap} & \textbf{Overlap} \\
        \midrule
        valid                & 2582 & 98.80\% & 95.70\% \\
        test                 & 5741 & 98.69\% & 95.45\%
    \end{tabular}
    \caption{\small Overlap of transformed train dataset summaries and valid/test dataset summaries} \label{table_vt_final_rw}
\end{table}

% nice formatting of python code
% https://tex.stackexchange.com/questions/105662/default-value-for-basicstyle-in-lstlisting/122916#122916
\lstdefinestyle{shared}
{
    numbers=left,
    numbersep=1em,
    numberstyle=\tiny\color{red},
    frame=single,
    framesep=\fboxsep,
    framerule=\fboxrule,
    rulecolor=\color{red!20},
    linewidth=13.7cm,
    breaklines=true,
    tabsize=2,
    columns=flexible,
}

\lstdefinestyle{python}
{
    style=shared,
    escapechar=\^,
    language={Python},
    basicstyle=\small\tt,
    keywordstyle=\color{blue},
    commentstyle=\color[rgb]{0.13,0.54,0.13},
    backgroundcolor=\color{cyan!5},
}

\lstnewenvironment{python}
{\lstset{style=python}}
{}

\begin{algorithm}[!h]
\begin{python}
import re, collections

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
    symbols = word.split()
    for i in range(len(symbols)-1):
        pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
    w_out = p.sub(''.join(pair), word)
    v_out[w_out] = v_in[word]
    return v_out

vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,
            'n e w e s t </w>':6, 'w i d e s t </w>':3}
num_merges = 10
for i in range(num_merges):
    pairs = get_stats(vocab)
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(best)

> r ^$\cdot$^ ^$\rightarrow$^ r^$\cdot$^
> l o ^$\rightarrow$^ lo
> lo w ^$\rightarrow$^ low
> e r^$\cdot$^ ^$\rightarrow$^ er^$\cdot$^
\end{python}
\caption{Learn BPE operations \\ Extract from paper \textbf{Neural Machine Translation of Rare Words with Subword Units} by \citep{sennrich2016}}
\label{bpe-algorithm}
\end{algorithm}
