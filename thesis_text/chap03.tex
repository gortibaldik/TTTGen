\chapter{Preprocessing and Statistics of the Datasets} \label{chapPreproc}

To generate text from structured data I choose the Deep Neural Networks and specifically the Encoder-Decoder (ED) neural architecture (section \ref{neural_nets_chapter}). The ED suited to process sequential one dimensional data, however we cope with two dimensional tables. In this chapter I describe how I handle the problem. In addition I further analyze the data and propose cleaning methods and reason about motivations for selected techniques.

\section{Transforming Tables to Records} \label{table_to_record_trans}

At first let's define a table. A table is a two dimensional data structure, where the information is stored not only in the actual values in cells, but also in the positional information. Values in the same column have the same type, whereas values in the same row belong to the same entity. An example of the table as we have defined it is in figure \ref{ex_struct}.

\begin{table}[h]
    \centering
    \begin{tabular}{llll}
        \toprule
        {} & type$_1$ & type$_2$ \dots \\
        \midrule
        entity$_1$ & value$_{1,1}$ &  value$_{1,2}$ \dots \\
        entity$_2$ & value$_{2,1}$ & value$_{2,2}$ \dots \\
        \dots &&
    \end{tabular}
    \caption{An example of structured data} \label{ex_struct}
\end{table}

I use the same notation as \citep{liang-etal-2009-learning}. Table $\mathcal{T}$ is transformed into a sequence of records $ \mathbf{s} = \{ r_i \}_{i=1}^{J} $, where $r_i$ denotes i-th record. To fulfill our goal of keeping the most of the positional information from the table, each record contains field $r.type$ denoting the type of the value, the actual value $r.value$ and the entity $r.entity$ to which the record belongs. At the end, I transform table \ref{ex_struct} to sequence of records \ref{ex_seq_rec}.

\tikzstyle{example_style} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, fill=yellow!10, align=left]

\begin{figure}[!h]
    \centering
    \usetikzlibrary{shapes.multipart}
    \begin{tikzpicture}
    \node (r1) [example_style] {
        \{\emph{type}: type$_1$; \emph{entity}: entity$_1$, \emph{value}: value$_{1,1}$\}
    };
    \node (r2) [example_style, below=2mm of r1]{
        \{\emph{type}: type$_2$; \emph{entity}: entity$_1$, \emph{value}: value$_{1,2}$\}
    };
    \node (r3) [example_style, below=2mm of r2]{
        \{\emph{type}: type$_1$; \emph{entity}: entity$_2$, \emph{value}: value$_{2,1}$\}
    };
    \end{tikzpicture}
    \\ \dots
    \caption{An example of records obtained by transforming table \ref{ex_seq_rec}} \label{ex_seq_rec}
\end{figure}

\section{RotoWire} \label{rotowire_preproc_section}

Firstly I present the statistics of the dataset \emph{before any preprocessing}. Next I elaborate about the particularities of the input tables. After working with the dataset for a while I found many of its deficiencies. I'm not the first one who spotted the flaws and there exist datasets based on RotoWire, which contain cleaner data and summaries corresponding better to input tables \citep{wang-2019-revisiting}, \citep{thomson-2020-sportsett}. However I choose to continue with the RotoWire dataset, as I am already accustomed to the format of the data.

\subsection{Dataset Statistics} \label{assumptions_ref}

I believe that the challenges posed by the RotoWire dataset can be summarized in a set of statistics. In this subsection I want to present the most important ones to help reader to understand the nature of the problem. 

Firstly, the target summaries as well as the sequences of input records are really long compared to other datasets modelling the same task (e.g. WikiBIO, WeatherGOV, RoboCup \emph{XXXXXX}).

Secondly, many words occur rarely and the generation system cannot learn a good representation of them (it is known as a \emph{rare word problem}). It should be noted that the problem can be resolved by the means of clever preprocessing (section \ref{bpeSection}) or with help of advanced neural architectures (section \ref{copy_mech_sec}).

Thirdly, there are many values which represent facts, e.g. values that cannot be deduced from the context (e.g. points a player scored in a match etc.) but must be selected and copied from the table.

The original dataset as prepared by \citep{wiseman2017}, is already divided to train (3398 samples), development (727 samples) and test (728 samples) sets. \emph{In the statistics presented below I state that there are only 3397 samples in the train set because one of the samples is the famous Lorem Ipsum template.}

\subsubsection{Length-wise Statistics}

The input tables contain huge amount of information. 2 teams and up to 30 players participate in a match of basketball. After transformation to a sequence, a player is represented by 24 and a team by 15 records. The type field $r.type$ is the only trait distinguishing the team and player records. Table \ref{stats_tables_orig_rw} summarizes the length statistics of the input sequences. 

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}    & \textbf{Max} & \textbf{Min} & \textbf{Avegage}& {} \\
        \textbf{Set} & \textbf{Number of} & \textbf{Number of} & \textbf{Number of} & \textbf{Size} \\
        {} & \textbf{Records} & \textbf{Records} & \textbf{Records} & {} \\
        \midrule
        train       & 750 & 558 & 644.65 & 3397  \\
        development & 702 & 582 & 644.66 & 727 \\
        test        & 702 & 558 & 645.03 & 728
    \end{tabular}
    \caption{Statistics of tables as used by \citep{wiseman2017}} \label{stats_tables_orig_rw}
\end{table}

There is much greater variance in the lengths of output summaries. While the longest input sequence is 134\% of the shortest one, from the data in table \ref{stats_sums_orig_rw}, we can see that the factor between the outputs is 511\%. The size of the inputs and the outputs places high memory and computation demands on the GPUs used for training, and needs a special treatment (as will be explained in section \ref{truncated_backprop_subsection}).

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}    & \textbf{Max} & \textbf{Min} & \textbf{Avegage}& {} \\
        \textbf{Set} & \textbf{Summary} & \textbf{Summary} & \textbf{Summary} & \textbf{Size} \\
        {} & \textbf{Length} & \textbf{Length} & \textbf{Length} & {} \\
        \midrule
        train      & 762 & 149 & 334.41 & 3397  \\
        validation & 813 & 154 & 339.97 & 727 \\
        test       & 782 & 149 & 346.83 & 728
    \end{tabular}
    \caption{Statistics of summaries as used by \citep{wiseman2017}} \label{stats_sums_orig_rw}
\end{table}


\subsubsection{Occurrences of Unique Tokens}

While the length of the inputs and the outputs increases computational demands, the low occurrency of certain token causes the \emph{rare word problem}. After discussions with my advisor I think it is reasonable to expect that the system should learn a good representation of a token if it appears at least 5 times in the train set.

There is about 11 300 unique tokens in the dataset (in the union of train, development and test set). In table \ref{stats_occur_rw} I present the statistics regarding the occurrences of the tokens. We can see that only 42 \% of all the tokens appear at least 5 times in the train part of the dataset.

However I expect that even if some anomaly in the real word happens (e.g. team scores 200 points, although at the time of writing no team in the history of NBA scored more than 186) the system should be able to simply \emph{copy} the value of \emph{TEAM-PTS} record without reasoning about the actual value. Consequently we are interested in tokens that cannot be copied from the table. Since most of the named entities are directly copiable, there is no need to preserve casing. All the aforementioned statistics are summarized in table \ref{stats_occur_rw}.

In the end we see that under our assumptions about 60 \% of all the unique tokens cannot be learned by the generation system.

\begin{table}[h!]
    \centering
    \begin{tabular}{cccc}
        \toprule
        {}    & \textbf{Unique} & \textbf{$>= 5$} & \textbf{$>= 5$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} & \textbf{Absolute} & \textbf{Relative}\\
        \midrule
        train      & 9779 & 4158 & 42.52\% \\
        train\_wop$_1$ & 8604 & 3296 & 38.31\% \\
        train\_wopl$_2$ & 8031 & 3119 & 38.84\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize \textit{Note:} $_1$ train\_wop is training set with all the player names, city names, } \\
        \multicolumn{4}{l}{\footnotesize team names and numbers extracted $_2$ train\_wopl is train\_wop lowercased}
    \end{tabular}
    \caption{Occurrences of tokens in summaries from dataset RotoWire} \label{stats_occur_rw}
\end{table}

In table \ref{stats_overlap_rw} we can see how many of the unique tokens learned during training can be found in the respective development and test datasets. Under our assumptions we can expect the generated text to share less than 65 \% of the vocabulary with the gold references. 

\begin{table}[h!]
    \centering
    \begin{tabular}{cccc}
        \toprule
        {}    & Â \textbf{Unique} &\textbf{Train} & \textbf{Train$_{>=5}$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} &\textbf{Overlap} & \textbf{Overlap} \\
        \midrule
        valid      & 5625 & 88.18\% & 66.63\% \\
        test       & 5741 & 87.46\% & 65.72\% \\
        \hline
        valid\_wop$_1$      & 4714 & 86.36\% & 61.92\% \\
        test\_wop$_2$       & 4803 & 86.03\% & 61.13\% \\
        \hline
        valid\_wopl$_3$      & 4442 & 86.74\% & 62.36\% \\
        test\_wopl$_4$       & 4531 & 86.32\% & 61.37\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize \textit{Note:} train$_{>=5}$ is a set of all the tokens with more than} \\
        \multicolumn{4}{l}{\footnotesize 5 occurrences in the train dataset summaries $_1$, $_2$, $_3$, $_4$} \\
        \multicolumn{4}{l}{\footnotesize have the same meaning as in table \ref{stats_occur_rw}}
    \end{tabular}
    \caption{Overlap of train dataset summaries and valid/test dataset summaries} \label{stats_overlap_rw}
\end{table}

\subsection{Transformations of Input Tables} \label{trans_in_tb_rw}

Firstly I want to talk about the preprocessing of actual values stored in the cells of the table. After that I present the format of a record which is fed to the generation system.

\subsubsection{Tabular Types}

There are 39 types (different headers of columns as discussed in section \ref{table_to_record_trans}). A type is associated to textual or integer value which describes either a team or an individual. There are only 7 types bound to textual values, out of which 2 are related to teams (\emph{TEAM-NAME}, \emph{TEAM-CITY}) and 5 to individuals (\emph{FIRST\_NA\-ME}$^*$, \emph{SE\-COND\_NAME}$^*$, \emph{PLAYER\_NAME}$^*$, \emph{START\_POSITION}$^*$, \emph{TEAM\_CI\-TY}$^*$)

\subsubsection{Numerical values}

The other 32 types desribe absolute (\emph{TEAM-PTS}, \emph{FTM}\footnote{Number of converted free throws by an individual, \emph{"free throws made"}} \dots) or relative integer values (\emph{TEAM-FT\_PCT}, \emph{FT\_PCT}\footnote{\emph{team/player free throw percentage}}, \dots). During preprocessing no changes are made to any tabular numerical value\footnote{As described in \citep{wiseman2017} all the relative values are converted to integers. I don't consider this as a preprocessing since only the converted values are available in the dataset.}.

\subsubsection{Textual values} \label{trans_p_nms}

Regarding the textual values, I consider each as a single token. Since the names of teams are already one word long (with one exception needing a transformation \emph{Trail Blazers $\rightarrow$ Trail\_Blazers}) the transformation is rather trivial. The similar observation applies to names of cities (with 6 exceptions: \emph{Oklahoma\_City}, \emph{San\_Antonio}, \emph{New\_Orleans}, \emph{Los\_Angeles}, \emph{Golden\_State}, \emph{New\_York}), and start positions.

Out of three types connected to player credentials, only \emph{PLAYER\_NAME} (describing player full name with all the attributes e.g. \emph{Johnny O'Bryant III}) is multi-token.

The original take on the problem is different to mine. The authors of the dataset \citep{wiseman2017}, as well as the authors of one of the more successful approaches to the task \citep{puduppully2019datatotext} make use of three special types, \emph{PLAYER\_NAME}, \emph{FIRST\_NAME} and \emph{SECOND\_NAME} which allow to distinguish if \emph{James} refers to the first name of star player \emph{James Harden} or to a second name of the legend of \emph{LeBron James}.

My approach is based on the idea of making data as dense as possible to make the learning of the generation system easier. Therefore I transform the name of each player to a single token. To make the copying possible a preprocessing of the output summaries as described in \ref{player_nm_trans_summary} must take place. 

Consequently \emph{FIRST\_NAME} and \emph{SECOND\_NAME} records aren't needed anymore which results in another benefit, shorter inputs. (as there are 22 - 30 players involved in the match, the size of inputs becomes about 10 \% (44 - 60 records) shorter)

\subsubsection{Entities}

The type information tells us what does the number or text in the value field represent. However it is the entity field which brings together all the records describing the same player or team. Let's show an example of records about a star player, \emph{Stephen Curry}. His name is stored in a record of type \emph{PLAYER\_NAME}, which has a value \emph{Stephen\_Curry}. To link all the information about him together, each record has an entity field labelled \emph{Stephen\_Curry}. Similarly all the records of a team with \emph{TEAM\_NAME} : \emph{A} have the same entity field, \emph{A}.

At last, we should notice that the team isn't only represented by the accumulated team statistics, but also by the players playing for it. Therefore the record also contains \emph{HOME/AWAY} field which brings together all the statistics about the home side and the away side.

\subsubsection{Record Format}

The records fed into the generation system contains the following fields:

\begin{itemize}
    \item \emph{Type}
    \item \emph{Value}
    \item \emph{Entity}
    \item \emph{Home/Away flag}
\end{itemize}

The generation system should be able to understand the meaning of a record and shouldn't rely on a specific organisation of the table. This is modelled by emplacing the team records at the end, so that the system will need to search for the team statistics. Since the size of the input sequence isn't uniform, the team records can start anywhere between 500th and 720th record. The organization of the table will be further explained in the chapter about the experiments \ref{experiments_chapter}.


\begin{figure}[!h]
    \centering
    \usetikzlibrary{shapes.multipart}
    \begin{tikzpicture}
    \node (r1) [example_style] {
        \{\emph{type}: PTS; \emph{entity}: Stephen\_Curry, \emph{value}: 25; \emph{ha}: HOME \}
    };
    \node (r2) [example_style, below=2mm of r1]{
        \{\emph{type}: TEAM-PTS; \emph{entity}: Warriors, \emph{value}: 122; \emph{ha}: HOME\}
    };
    \end{tikzpicture}
    \\ \dots
    \caption{An example of a player and a team record.} \label{rotowire_record_example}
\end{figure}

\subsection{Preprocessing of Summaries}

I would like to reiterate that our motivation is to avoid the \emph{rare word problem} and to make copying words from the sequences of input records as easy as possible. Therefore we opt for methods which reduce the number of tokens, increase their average frequency (because the system couldn't learn the most sporadic ones anyway), and transform the tokens describing the tabular data to the same form as is used in the table.

\subsubsection{Number Transformations} \label{num_trans_rw}

Just as \citep{wiseman2017} and \citep{puduppully2019datatotext}, we represent the numbers only by numerals. This preprocessing method partially fulfills both of our goals. Obviously it decreases the unique token count, but on top of that it makes copying easier. E. g. the sentence \emph{"Isaiah Thomas once again excelled , scoring 23 points, \textbf{three} assists and \textbf{three} rebounds."} is transformed to \emph{"Isaiah Thomas once again excelled , scoring 23 points, \textbf{3} assists and \textbf{3} rebounds."}. Under this setting, the network still has to learn the correspondence \emph{"assist"} $\cong$ \emph{"assists"} but without the need of linking \emph{"three"} to \emph{"3"} the connection of the phrase with record \emph{\{AST; 3; Isaiah\_Thomas; Home\}} should be much clearer. However some number words must be preserved as they form a part of a basketball terminology (e.g. \emph{three pointer}). The transformations are done with the help of the \emph{text2num} library\footnote{\url{https://github.com/allo-media/text2num}} which is also used by the authors cited above.

\subsubsection{Player name transformations} \label{player_nm_trans_summary}

The generation system should be able to create a summary of player's actions in the game based on the records describing his match-statistics. It is common that at first a player is mentioned by his full name (e.g. \emph{Stephen Curry}) and after that only by his second name (\emph{Curry}). Also more than 97 \% of all the players have exactly 2 names (first, last), that leaves out 17 players with longer names with the most extreme case being \emph{Luc Richard Mbah a Moute}. This player is represented in the whole dataset by 6 different combinations of his names.

Since only the full name concatenated to a single token is contained in the input records I developed an algorithm which transforms all \footnote{Although technically speaking this is not true as I don't transform any pronouns and the transformations follow simple path: \emph{some part of a name} $\rightarrow$ \emph{full name}} the references to a player to that specific token.

I haven't measured the accuracy of the algorithm, however it passes an eye-test as during the development of neural models I have inspected a great amount of produced summaries which haven't contained any discrepancies.

The transformation happens in three steps, which are described on the example sentence from figure \ref{cmp_original_vs_mine}.:
\begin{itemize}
    \item \textbf{1. Extraction of player names from the summary} \hfill \\
    We traverse the sentence and collect the longest subsequences of tokens that appear as a part of any player name from the corpus. Vytiahneme zo sumÃ¡ru menÃ¡ \emph{James} a \emph{James Harden}, ktorÃ© sÃº sÃºÄasÅ¥ou mena nejakÃ©ho z hrÃ¡Äov, ktorÃ­ sÃº znÃ¡mi z korpusu
    \item \textbf{2. VyrieÅ¡enie referenciÃ­ pre danÃ½ sumÃ¡r a vytvorenie slovnÃ­ka transformÃ¡ciÃ­} \hfill \\
    PokÃºsime sa zistiÅ¥, na Äo odkazuje jednotokenovÃ© meno \emph{James}. Explicitne zakazujeme, aby sa vnÃ­malo ako krstnÃ© meno a v boxscore danÃ©ho zÃ¡pasu nÃ¡jdeme, Å¾e doÅ zasiahol niekto s menom \emph{LeBron James}. Do slovnÃ­ka transformÃ¡ciÃ­ teda umiestnime transformÃ¡ciu \emph{James $\rightarrow$ LeBron\_James}. \emph{James Harden} je viac tokenovÃ© meno, ktorÃ© sme v prvom kroku rozoznali ako meno hrÃ¡Äa, teda do slovnÃ­ka pridÃ¡me len prepis \emph{James Harden $\rightarrow$ James\_Harden}
    \item 3. AplikÃ¡cia transformÃ¡ciÃ­ na sumÃ¡r \hfill \\
    PrechÃ¡dzame text a na najdlhÅ¡iu namatchovanÃº postupnosÅ¥ aplikujeme transformÃ¡ciu.
\end{itemize}

\begin{figure}[!h]
    \centering
    \usetikzlibrary{shapes.multipart}
    \begin{tikzpicture}
    \node (original) [example_style, text width = 0.95*\columnwidth] {
        While King James struggled , James Harden was busy putting up a triple - double on the Detroit Pistons on Friday
    };
    \node (transformed) [example_style, text width = 0.95*\columnwidth, below=5mm of original]{
        while king LeBron\_James struggled , James\_Harden was busy putting up a triple - double on the Detroit Pistons on friday.
    };
    \draw [->] (original) edge (transformed);
    \end{tikzpicture}
    \caption{Example of transformation of player names leveraging the knowledge of players on the rosters} \label{cmp_original_vs_mine}
\end{figure}

\subsection{Byte Pair Encoding} \label{bpeSection}

PokiaÄ¾ by sme sa rozhodli, Å¾e ako slovnÃ­k pouÅ¾ijeme len slovÃ¡, ktorÃ© generaÄnÃ½ systÃ©m poznÃ¡ z trÃ©ningovÃ©ho datasetu, museli by sme nahradiÅ¥ viac ako 10\% unikÃ¡tnych slov z validaÄnÃ©ho, Äi testovacieho datasetu Å¡peciÃ¡lnymi \emph{UNK} tokenmi. Namiesto toho, sme sa rozhodli pouÅ¾iÅ¥ prÃ­stup, ktorÃ½ predstavil \citep{sennrich2016}. Najprv popÃ­Å¡eme algoritmus, nÃ¡sledne ukÃ¡Å¾eme, ako pomohol prekonaÅ¥ problÃ©m, o ktorom rozprÃ¡vame.

\subsubsection{Algorithm}

PouÅ¾Ã­vame implementÃ¡ciu algoritmu priamo od autorov\footnote{\url{https://github.com/rsennrich/subword-nmt}}, ktorÃ¡ sa dÃ¡ stiahnuÅ¥ aj ako samostatnÃ½ python package. MinimÃ¡lny prÃ­klad moÅ¾nej implementÃ¡cie v pythone je ukÃ¡zanÃ½ ako algoritmus \ref{bpe-algorithm}. 

Ako prvÃ½ krok sa kaÅ¾dÃ© slovo rozdelÃ­ na znaky a na koniec kaÅ¾dÃ©ho slova sa pridÃ¡ Å¡peciÃ¡lny znak  \emph{\textless eow\textgreater}, ktorÃ½ umoÅ¾nÃ­ detokenizÃ¡ciu na pÃ´vodnÃ½ text. VstupnÃ½ slovnÃ­k sa inicializuje na mnoÅ¾inu unikÃ¡tnych znakov v texte (okrem bielych znakov, algoritmus sa tÃ½ka len slov). Algoritmus mnohokrÃ¡t prechÃ¡dza text a pri kaÅ¾dom prechode nÃ¡jde najÄastejÅ¡iu dvojicu za sebou idÃºcich znakov a spojÃ­ ju do jednoho novÃ©ho znaku. Tento znak sa pridÃ¡ do slovnÃ­ka a vÅ¡etky vÃ½skyty danÃ½ch dvoch znakov za sebou sa nahradia novÃ½m znakom. 

VeÄ¾kosÅ¥ vÃ½stupnÃ©ho slovnÃ­ka je teda poÄet unikÃ¡tnych znakov v pÃ´vodnom texte ($+1$ kvÃ´li \emph{\textless eow\textgreater} tokenu) $+$ poÄet prebehnutÃ½ch iterÃ¡ciÃ­.

Algoritmus mÃ¡ teda jeden hyperparameter a to poÄet iterÃ¡ciÃ­, a teda veÄ¾kosÅ¥ vÃ½stupnÃ©ho slovnÃ­ka.

\subsubsection{Statistics of Transformed Dataset}

VyskÃºÅ¡ali sme tri moÅ¾nosti nastavenia hyperparametru BPE\footnote{MoÅ¾nosti nastavenia poÄtu iterÃ¡ciÃ­ BPE spolu so Å¡tatistikami s nimi spojenÃ½mi sÃº prÃ­stupnÃ© na \url{https://github.com/gortibaldik/TTTGen/blob/master/rotowire/dataset_stats.md}}. Nakoniec sme zvolili 2000 iterÃ¡ciÃ­ algoritmu. Z procesu spÃ¡jania tokenov sÃº vybranÃ© ÄÃ­sla a menÃ¡ hrÃ¡Äov, keÄÅ¾e chceme, aby tieto tokeny generaÄnÃ½ systÃ©m priamo kopÃ­roval zo vstupnej tabuÄ¾ky. TÃ½m pÃ¡dom prienik trÃ©ningovÃ©ho a validaÄnÃ©ho (resp. testovacieho) datasetu nie je 100\%.\footnote{KonkrÃ©tne spolu 92 tokenov (31 unikÃ¡tnych tokenov) z validaÄnÃ©ho a 87 tokenov (34 unikÃ¡tnych tokenov) je nahradenÃ½ch UNK tokenom vo validaÄnÃ½ch dÃ¡tach.}VÃ½slednÃ© Å¡tatistiky po vÅ¡etkÃ½ch transformÃ¡ciÃ¡ch sÃº v tabuÄ¾kÃ¡ch \ref{table_train_final_rw} a \ref{table_vt_final_rw}.

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{cccc}
        \toprule
        {}    & \textbf{Unique} & \textbf{$>= 5$} & \textbf{$>= 5$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} & \textbf{Absolute} & \textbf{Relative}\\
        \midrule
        train      & 2839 & 2531 & 89.15\%
    \end{tabular}
    \caption{\small Occurrences of tokens in transformed summaries from dataset RotoWire} \label{table_train_final_rw}
\end{table}

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{cccc}
        \toprule
        {}    & Â \textbf{Unique} &\textbf{Train} & \textbf{Train$_{>=5}$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} &\textbf{Overlap} & \textbf{Overlap} \\
        \midrule
        valid                & 2582 & 98.80\% & 95.70\% \\
        test                 & 5741 & 98.69\% & 95.45\%
    \end{tabular}
    \caption{\small Overlap of transformed train dataset summaries and valid/test dataset summaries} \label{table_vt_final_rw}
\end{table}

% nice formatting of python code
% https://tex.stackexchange.com/questions/105662/default-value-for-basicstyle-in-lstlisting/122916#122916
\lstdefinestyle{shared}
{
    numbers=left,
    numbersep=1em,
    numberstyle=\tiny\color{red},
    frame=single,
    framesep=\fboxsep,
    framerule=\fboxrule,
    rulecolor=\color{red!20},
    linewidth=13.7cm,
    breaklines=true,
    tabsize=2,
    columns=flexible,
}

\lstdefinestyle{python}
{
    style=shared,
    escapechar=\^,
    language={Python},
    basicstyle=\small\tt,
    keywordstyle=\color{blue},
    commentstyle=\color[rgb]{0.13,0.54,0.13},
    backgroundcolor=\color{cyan!5},
}

\lstnewenvironment{python}
{\lstset{style=python}}
{}

\begin{algorithm}[!h]
\begin{python}
import re, collections

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
    symbols = word.split()
    for i in range(len(symbols)-1):
        pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
    w_out = p.sub(''.join(pair), word)
    v_out[w_out] = v_in[word]
    return v_out

vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,
            'n e w e s t </w>':6, 'w i d e s t </w>':3}
num_merges = 10
for i in range(num_merges):
    pairs = get_stats(vocab)
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(best)

> r ^$\cdot$^ ^$\rightarrow$^ r^$\cdot$^
> l o ^$\rightarrow$^ lo
> lo w ^$\rightarrow$^ low
> e r^$\cdot$^ ^$\rightarrow$^ er^$\cdot$^
\end{python}
\caption{Learn BPE operations \\ Extract from paper \textbf{Neural Machine Translation of Rare Words with Subword Units} by \citep{sennrich2016}}
\label{bpe-algorithm}
\end{algorithm}
