\chapter{Preprocessing and Statistics of the Datasets} \label{chapPreproc}

To generate text from structured data I choose the Deep Neural Networks and specifically the Encoder-Decoder (ED) neural architecture (section \ref{neural_nets_chapter}). The ED suited to process sequential one dimensional data, however we cope with two dimensional tables. In this chapter I describe how I handle the problem. In addition I further analyze the data and propose cleaning methods and reason about motivations for selected techniques.

\section{Transforming Tables to Records} \label{table_to_record_trans}

At first let's define a table. A table is a two dimensional data structure, where the information is stored not only in the actual values in cells, but also in the positional information. Values in the same column have the same type, whereas values in the same row belong to the same entity. An example of the table as we have defined it is in figure \ref{ex_struct}.

\begin{table}[h]
    \centering
    \begin{tabular}{llll}
        \toprule
        {} & type$_1$ & type$_2$ \dots \\
        \midrule
        entity$_1$ & value$_{1,1}$ &  value$_{1,2}$ \dots \\
        entity$_2$ & value$_{2,1}$ & value$_{2,2}$ \dots \\
        \dots &&
    \end{tabular}
    \caption{An example of structured data} \label{ex_struct}
\end{table}

I use the same notation as \citep{liang-etal-2009-learning}. Table $\mathcal{T}$ is transformed into a sequence of records $ \mathbf{s} = \{ r_i \}_{i=1}^{J} $, where $r_i$ denotes i-th record. To fulfill our goal of keeping the most of the positional information from the table, each record contains field $r.type$ denoting the type of the value, the actual value $r.value$ and the entity $r.entity$ to which the record belongs.

\tikzstyle{example_style} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, fill=yellow!10, align=left]

\section{RotoWire} \label{rotowire_preproc_section}

Firstly I present the statistics of the dataset \emph{before any preprocessing}. Next I elaborate about the particularities of the input tables. After working with the dataset for a while I found many of its deficiencies. I'm not the first one who spotted the flaws and there exist datasets based on RotoWire, which contain cleaner data and summaries corresponding better to input tables \citep{wang-2019-revisiting}, \citep{thomson-2020-sportsett}. However I choose to continue with the RotoWire dataset, as I am already accustomed to the format of the data.

\subsection{Dataset Statistics} \label{assumptions_ref}

I believe that the challenges posed by the RotoWire dataset can be summarized in a set of statistics. In this subsection I want to present the most important ones to help reader to understand the nature of the problem. 

Firstly, the target summaries as well as the sequences of input records are really long compared to other datasets modelling the same task (e.g. WikiBIO, WeatherGOV, RoboCup \emph{XXXXXX}).

Secondly, many words occur rarely and the generation system cannot learn a good representation of them. It should be noted that the problem can be resolved by the means of clever preprocessing (section \ref{bpeSection}) or with help of advanced neural architectures (section \ref{copy_mech_sec}).

Thirdly, there are many values which represent facts, e.g. values that cannot be deduced from the context (e.g. points a player scored in a match etc.) but must be selected and copied from the table.

The original dataset as prepared by \citep{wiseman2017}, is already divided to train (3398 samples), development (727 samples) and test (728 samples) sets. \emph{In the statistics presented below I state that there are only 3397 samples in the train set because one of the samples is the famous Lorem Ipsum template.}

\subsubsection{Length-wise Statistics}

The input tables contain huge amount of information. 2 teams and up to 30 players participate in a match of basketball. After transformation to a sequence, a player is represented by 24 and a team by 15 records. The type field $r.type$ is the only trait distinguishing the team and player records. Table \ref{stats_tables_orig_rw} summarizes the length statistics of the input sequences. 

The generation system should be able to understand the meaning of a record and shouldn't rely on specific organisation of the table. This is modelled by emplacing the team records at the end, so that the system will need to search for the team statistics. Since the size of the input sequence isn't uniform, the team records can start anywhere between 500th and 720th record. The organization of the table will be further explained in the chapter about the experiments \ref{experiments_chapter}.

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}    & \textbf{Max} & \textbf{Min} & \textbf{Avegage}& {} \\
        \textbf{Set} & \textbf{Number of} & \textbf{Number of} & \textbf{Number of} & \textbf{Size} \\
        {} & \textbf{Records} & \textbf{Records} & \textbf{Records} & {} \\
        \midrule
        train       & 750 & 558 & 644.65 & 3397  \\
        development & 702 & 582 & 644.66 & 727 \\
        test        & 702 & 558 & 645.03 & 728
    \end{tabular}
    \caption{Statistics of tables as used by \citep{wiseman2017}} \label{stats_tables_orig_rw}
\end{table}

There is much greater variance in the lengths of output summaries. While the longest input sequence is 134\% of the shortest one, from the data in table \ref{stats_sums_orig_rw}, we can see that the factor between the outputs is 511\%. The size of the inputs and the outputs places high memory and computation demands on the GPUs used for training, and needs a special treatment (as will be explained in section \ref{truncated_backprop_subsection}).

\begin{table}[h!]
    \centering
    \begin{tabular}{ccccc}
        \toprule
        {}    & \textbf{Max} & \textbf{Min} & \textbf{Avegage}& {} \\
        \textbf{Set} & \textbf{Summary} & \textbf{Summary} & \textbf{Summary} & \textbf{Size} \\
        {} & \textbf{Length} & \textbf{Length} & \textbf{Length} & {} \\
        \midrule
        train      & 762 & 149 & 334.41 & 3397  \\
        validation & 813 & 154 & 339.97 & 727 \\
        test       & 782 & 149 & 346.83 & 728
    \end{tabular}
    \caption{Statistics of summaries as used by \citep{wiseman2017}} \label{stats_sums_orig_rw}
\end{table}


\subsubsection{Occurrences of Unique Tokens}

While the length of the inputs and the outputs increases computational demands, the low occurrency of certain token causes the \emph{rare word problem}. After discussions with my advisor I think it is reasonable to expect that the system should learn a good representation of a token if it appears at least 5 times in the train set.

There is about 11 300 unique tokens in the dataset (in the union of train, development and test set). In table \ref{stats_occur_rw} I present the statistics regarding the occurrences of the tokens. We can see that only 42 \% of all the tokens appear at least 5 times in the train part of the dataset.

However I expect that even if some anomaly in the real word happens (e.g. team scores 200 points, although at the time of writing no team in the history of NBA scored more than 186) the system should be able to simply \emph{copy} the value of \emph{TEAM-PTS} record without reasoning about the actual value. Consequently I present also the statistics of tokens that cannot be copied from the table. At last, since most of the named entities can be directly copied, there is no need to keep casing as it doesn't represent any valuable information.

In the end we see that under our assumptions about 60 \% of all the unique tokens cannot be learned by the generation system.

\begin{table}[h!]
    \centering
    \begin{tabular}{cccc}
        \toprule
        {}    & \textbf{Unique} & \textbf{$>= 5$} & \textbf{$>= 5$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} & \textbf{Absolute} & \textbf{Relative}\\
        \midrule
        train      & 9779 & 4158 & 42.52\% \\
        train\_wop$_1$ & 8604 & 3296 & 38.31\% \\
        train\_wopl$_2$ & 8031 & 3119 & 38.84\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize \textit{Note:} $_1$ train\_wop is training set with all the player names, city names, } \\
        \multicolumn{4}{l}{\footnotesize team names and numbers extracted $_2$ train\_wopl is train\_wop lowercased}
    \end{tabular}
    \caption{Occurrences of tokens in summaries from dataset RotoWire} \label{stats_occur_rw}
\end{table}

In table \ref{stats_overlap_rw} we can see how many of the unique tokens learned during training can be found in the respective development and test datasets. Under our assumptions we can expect the generated text to share less than 65 \% of the vocabulary with the gold references. 

\begin{table}[h!]
    \centering
    \begin{tabular}{cccc}
        \toprule
        {}    & Â \textbf{Unique} &\textbf{Train} & \textbf{Train$_{>=5}$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} &\textbf{Overlap} & \textbf{Overlap} \\
        \midrule
        valid      & 5625 & 88.18\% & 66.63\% \\
        test       & 5741 & 87.46\% & 65.72\% \\
        \hline
        valid\_wop$_1$      & 4714 & 86.36\% & 61.92\% \\
        test\_wop$_2$       & 4803 & 86.03\% & 61.13\% \\
        \hline
        valid\_wopl$_3$      & 4442 & 86.74\% & 62.36\% \\
        test\_wopl$_4$       & 4531 & 86.32\% & 61.37\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize \textit{Note:} train$_{>=5}$ is a set of all the tokens with more than} \\
        \multicolumn{4}{l}{\footnotesize 5 occurrences in the train dataset summaries $_1$, $_2$, $_3$, $_4$} \\
        \multicolumn{4}{l}{\footnotesize have the same meaning as in table \ref{stats_occur_rw}}
    \end{tabular}
    \caption{Overlap of train dataset summaries and valid/test dataset summaries} \label{stats_overlap_rw}
\end{table}

\subsection{Transformations of Input Tables} \label{trans_in_tb_rw}

Firstly I want to talk about the preprocessing of actual values stored in the cells of the table. After that I present the format of a record which is fed to the generation system.

\subsubsection{Tabular Types}

There are 39 types (different headers of columns as discussed in section \ref{table_to_record_trans}). A type is associated to textual or integer value which describes either a team or an individual. There are only 7 types bound to textual values (\emph{TEAM-NAME}, \emph{TEAM-CITY}, \emph{FIRST\_NAME}$^*$, \emph{SE\-COND\-\linebreak[4]\_NAME}$^*$, \emph{PLAYER\_NAME}$^*$, \emph{START\_POSITION}$^*$, \emph{TEAM\_CITY}$^*$)\footnote{$^*$ asterisk labels types bound to an individual}

\subsubsection{Numerical values}

The other 33 types desribe absolute integer values (\emph{TEAM-PTS}, \emph{FTM}\footnote{Number of converted free throws by an individual, \emph{"free throws made"}} \dots) or relative integer values (\emph{TEAM-FT\_PCT}, \emph{FT\_PCT}\footnote{\emph{player free throw percentage}}, \dots). During preprocessing no changes are made to any tabular numerical value\footnote{As described in \citep{wiseman2017} all the relative values are converted to integers. I don't consider this as a preprocessing since only the converted values are available in the dataset.}.

\subsubsection{Textual values} \label{trans_p_nms}

Regarding the textual values, I consider each as a single token. Since the names of teams are already one word long (with one exception needing a transformation \emph{Trail Blazers $\rightarrow$ Trail\_Blazers}) the transformation is rather trivial. The similar observation applies to names of cities (with 6 exceptions: \emph{Oklahoma\_City}, \emph{San\_Antonio}, \emph{New\_Orleans}, \emph{Los\_Angeles}, \emph{Golden\_State}, \emph{New\_York}), and start positions.

All the player names consist of at least 2 words (with one exception: \emph{Nene}). As being discussed earlier, we want as many values from the table to be copiable as possible.

The authors of the dataset \citep{wiseman2017}, as well as the authors of one of the more successful approaches to the dataset \citep{puduppully2019datatotext} make use of three special types, \emph{PLAYER\_NAME}, \emph{FIRST\_NAME} and \emph{SECOND\_NAME} which allow to distinguish if \emph{James} refers to the first name of star player \emph{James Harden} or to a second name of the legend of \emph{LeBron James}.

My approach is based on the idea of making data as dense as possible to make the learning of the generation system easier. Therefore I transform the name of each player to a single token. To make the copying possible a preprocessing of the output summaries as described in \ref{player_nm_trans_summary} must take place. 

Consequently \emph{FIRST\_NAME} and \emph{SECOND\_NAME} records aren't needed anymore which results in another benefit, shorter inputs. (as there are 22 - 30 players involved in the match, the size of inputs becomes about 10 \% (44 - 60 records) shorter)

\subsubsection{Entities}

KeÄÅ¾e vstupnÃ¡ tabuÄ¾ka obsahuje informÃ¡cie o obidvoch hrajÃºcich tÃ­moch a\linebreak[4]vÅ¡etkÃ½ch hrÃ¡Äoch na sÃºpiskÃ¡ch, kaÅ¾dÃ½ zÃ¡znam musÃ­ obsahovaÅ¥ aj hodnotu~$r.e$ popisujÃºcu, ktorÃº entitu zÃ¡znam charakterizuje. VÄaka transformÃ¡cii mien\linebreak[4]hrÃ¡Äov a tÃ­mov, mÃ´Å¾eme ako entitu pouÅ¾iÅ¥ nÃ¡zov tÃ­mu, resp. meno hrÃ¡Äa. Okrem toho vÅ¡ak ku kaÅ¾dÃ©mu zÃ¡znamu pridÃ¡me Å¡peciÃ¡lnu poloÅ¾ku $r.ha$, ktorÃ¡ symbolizuje, Äi sa zÃ¡znam vzÅ¥ahuje ku domÃ¡cemu, alebo hosÅ¥ujÃºcemu tÃ­mu. 

\subsubsection{Record Format}

Nakoniec teda pouÅ¾Ã­vam zÃ¡znamy, ktorÃ© obsahujÃº poloÅ¾ky $r.f$ - typ zÃ¡znamu, $r.e$ - entita, $r.v$ - hodnota a $r.ha$ - domÃ¡ci/hostia. (tu by mal byÅ¥ pridanÃ½ prÃ­klad, ako to vyzerÃ¡)

KvÃ´li tomu vykonÃ¡vame takÃ© transformÃ¡cie, aby sa jednak vyskytovalo Äo najviac tokenov Äo najÄastejÅ¡ie, a aby v sumÃ¡roch boli pouÅ¾Ã­vanÃ© rovnakÃ© tokeny ako v tabuÄ¾ke.

\subsubsection{Number Transformations} \label{num_trans_rw}

Rovnako ako \citep{wiseman2017} a \citep{puduppully2019datatotext}, reprezentujeme ÄÃ­sla len pomocou ÄÃ­slic. TÃ¡to transformÃ¡cia je motivovanÃ¡ druhÃ½m predpokladom a teda tÃ½m, Å¾e dÃºfame, Å¾e vÃ¤ÄÅ¡inu ÄÃ­sel bude neurÃ³novÃ¡ sieÅ¥ kopÃ­rovaÅ¥ a nie generovaÅ¥. To znamenÃ¡, Å¾e naprÃ­klad token $five$ transformujeme na $5$. PouÅ¾Ã­vame na to kniÅ¾nicu text2num \footnote[1]{\url{https://github.com/allo-media/text2num}}. NiektorÃ© ÄÃ­sla vÅ¡ak majÃº v basketbalovej terminolÃ³gii Å¡peciÃ¡lny vÃ½znam. Å pecificky teda netransformujeme ÄÃ­slo $three$, pretoÅ¾e sa Äasto vyskytuje ako sÃºÄasÅ¥ slovnÃ½ch spojenÃ­ \emph{three pointer, three pt range \dots}

\subsubsection{Player name transformations} \label{player_nm_trans_summary}

Ako uÅ¾ vyplÃ½va z \ref{trans_p_nms} a z predpokladov, ktorÃ© sme si stanovili, chceme v texte reprezentovaÅ¥ jednoho hrÃ¡Äa prÃ¡ve jednÃ½m tokenom. To platÃ­ aj pre extrÃ©mne prÃ­pady ako \emph{Luc Richard Mbah a Moute}, ktorÃ½ bol v datasete reprezentovanÃ½ Å¡iestimi rÃ´znymi kombinÃ¡ciami elÃ­ps v jeho mene. NajÄastejÅ¡ie vÅ¡ak vychÃ¡dza, Å¾e v texte sa najprv hrÃ¡Ä spomenie a nÃ¡sledne sa uÅ¾ oslovuje len priezviskom a aÅ¾ na 17 prÃ­padov (Äo ÄinÃ­ 2.4\% zo 668 hrÃ¡Äov spomenutÃ½ch aspoÅ v jednej tabuÄ¾ke) je meno hrÃ¡Äa vÅ¾dy zloÅ¾enÃ© z 2 tokenov. Na transformÃ¡ciu mien hrÃ¡Äov pouÅ¾Ã­vam jednoduchÃ½ algoritmus, ktorÃ½ napriek tomu, Å¾e nerieÅ¡i vÅ¡etky referencie Ãºplne presne, dosahuje na oko sluÅ¡nÃº presnosÅ¥.

\begin{figure}[!h]
\centering
\usetikzlibrary{shapes.multipart}
\begin{tikzpicture}
\node (original) [example_style, text width = 0.95*\columnwidth] {
    While King James struggled , James Harden was busy putting up a triple - double on the Detroit Pistons on Friday
};
\node (transformed) [example_style, text width = 0.95*\columnwidth, below=5mm of original]{
    while king LeBron\_James struggled , James\_Harden was busy putting up a triple - double on the Detroit Pistons on friday.
};
\draw [->] (original) edge (transformed);
\end{tikzpicture}
\caption{Example of transformation of player names leveraging the knowledge of players on the rosters} \label{cmp_original_vs_mine}
\end{figure}

TransformÃ¡cia prebieha v troch krokoch. Teraz sa ich pokÃºsim popÃ­saÅ¥ na prÃ­klade transformÃ¡cie vety z figure \ref{cmp_original_vs_mine}:
\begin{itemize}
    \item \textbf{1. Extrakcia mien hrÃ¡Äov zo sumÃ¡ru} \hfill \\
    Vytiahneme zo sumÃ¡ru menÃ¡ \emph{James} a \emph{James Harden}, ktorÃ© sÃº sÃºÄasÅ¥ou mena nejakÃ©ho z hrÃ¡Äov, ktorÃ­ sÃº znÃ¡mi z korpusu
    \item \textbf{2. VyrieÅ¡enie referenciÃ­ pre danÃ½ sumÃ¡r a vytvorenie slovnÃ­ka transformÃ¡ciÃ­} \hfill \\
    PokÃºsime sa zistiÅ¥, na Äo odkazuje jednotokenovÃ© meno \emph{James}. Explicitne zakazujeme, aby sa vnÃ­malo ako krstnÃ© meno a v boxscore danÃ©ho zÃ¡pasu nÃ¡jdeme, Å¾e doÅ zasiahol niekto s menom \emph{LeBron James}. Do slovnÃ­ka transformÃ¡ciÃ­ teda umiestnime transformÃ¡ciu \emph{James $\rightarrow$ LeBron\_James}. \emph{James Harden} je viac tokenovÃ© meno, ktorÃ© sme v prvom kroku rozoznali ako meno hrÃ¡Äa, teda do slovnÃ­ka pridÃ¡me len prepis \emph{James Harden $\rightarrow$ James\_Harden}
    \item 3. AplikÃ¡cia transformÃ¡ciÃ­ na sumÃ¡r \hfill \\
    PrechÃ¡dzame text a na najdlhÅ¡iu namatchovanÃº postupnosÅ¥ aplikujeme transformÃ¡ciu.
\end{itemize}

\subsection{Byte Pair Encoding} \label{bpeSection}

PokiaÄ¾ by sme sa rozhodli, Å¾e ako slovnÃ­k pouÅ¾ijeme len slovÃ¡, ktorÃ© generaÄnÃ½ systÃ©m poznÃ¡ z trÃ©ningovÃ©ho datasetu, museli by sme nahradiÅ¥ viac ako 10\% unikÃ¡tnych slov z validaÄnÃ©ho, Äi testovacieho datasetu Å¡peciÃ¡lnymi \emph{UNK} tokenmi. Namiesto toho, sme sa rozhodli pouÅ¾iÅ¥ prÃ­stup, ktorÃ½ predstavil \citep{sennrich2016}. Najprv popÃ­Å¡eme algoritmus, nÃ¡sledne ukÃ¡Å¾eme, ako pomohol prekonaÅ¥ problÃ©m, o ktorom rozprÃ¡vame.

\subsubsection{Algorithm}

PouÅ¾Ã­vame implementÃ¡ciu algoritmu priamo od autorov\footnote{\url{https://github.com/rsennrich/subword-nmt}}, ktorÃ¡ sa dÃ¡ stiahnuÅ¥ aj ako samostatnÃ½ python package. MinimÃ¡lny prÃ­klad moÅ¾nej implementÃ¡cie v pythone je ukÃ¡zanÃ½ ako algoritmus \ref{bpe-algorithm}. 

Ako prvÃ½ krok sa kaÅ¾dÃ© slovo rozdelÃ­ na znaky a na koniec kaÅ¾dÃ©ho slova sa pridÃ¡ Å¡peciÃ¡lny znak  \emph{\textless eow\textgreater}, ktorÃ½ umoÅ¾nÃ­ detokenizÃ¡ciu na pÃ´vodnÃ½ text. VstupnÃ½ slovnÃ­k sa inicializuje na mnoÅ¾inu unikÃ¡tnych znakov v texte (okrem bielych znakov, algoritmus sa tÃ½ka len slov). Algoritmus mnohokrÃ¡t prechÃ¡dza text a pri kaÅ¾dom prechode nÃ¡jde najÄastejÅ¡iu dvojicu za sebou idÃºcich znakov a spojÃ­ ju do jednoho novÃ©ho znaku. Tento znak sa pridÃ¡ do slovnÃ­ka a vÅ¡etky vÃ½skyty danÃ½ch dvoch znakov za sebou sa nahradia novÃ½m znakom. 

VeÄ¾kosÅ¥ vÃ½stupnÃ©ho slovnÃ­ka je teda poÄet unikÃ¡tnych znakov v pÃ´vodnom texte ($+1$ kvÃ´li \emph{\textless eow\textgreater} tokenu) $+$ poÄet prebehnutÃ½ch iterÃ¡ciÃ­.

Algoritmus mÃ¡ teda jeden hyperparameter a to poÄet iterÃ¡ciÃ­, a teda veÄ¾kosÅ¥ vÃ½stupnÃ©ho slovnÃ­ka.

\subsubsection{Statistics of Transformed Dataset}

VyskÃºÅ¡ali sme tri moÅ¾nosti nastavenia hyperparametru BPE\footnote{MoÅ¾nosti nastavenia poÄtu iterÃ¡ciÃ­ BPE spolu so Å¡tatistikami s nimi spojenÃ½mi sÃº prÃ­stupnÃ© na \url{https://github.com/gortibaldik/TTTGen/blob/master/rotowire/dataset_stats.md}}. Nakoniec sme zvolili 2000 iterÃ¡ciÃ­ algoritmu. Z procesu spÃ¡jania tokenov sÃº vybranÃ© ÄÃ­sla a menÃ¡ hrÃ¡Äov, keÄÅ¾e chceme, aby tieto tokeny generaÄnÃ½ systÃ©m priamo kopÃ­roval zo vstupnej tabuÄ¾ky. TÃ½m pÃ¡dom prienik trÃ©ningovÃ©ho a validaÄnÃ©ho (resp. testovacieho) datasetu nie je 100\%.\footnote{KonkrÃ©tne spolu 92 tokenov (31 unikÃ¡tnych tokenov) z validaÄnÃ©ho a 87 tokenov (34 unikÃ¡tnych tokenov) je nahradenÃ½ch UNK tokenom vo validaÄnÃ½ch dÃ¡tach.}VÃ½slednÃ© Å¡tatistiky po vÅ¡etkÃ½ch transformÃ¡ciÃ¡ch sÃº v tabuÄ¾kÃ¡ch \ref{table_train_final_rw} a \ref{table_vt_final_rw}.

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{cccc}
        \toprule
        {}    & \textbf{Unique} & \textbf{$>= 5$} & \textbf{$>= 5$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} & \textbf{Absolute} & \textbf{Relative}\\
        \midrule
        train      & 2839 & 2531 & 89.15\%
    \end{tabular}
    \caption{\small Occurrences of tokens in transformed summaries from dataset RotoWire} \label{table_train_final_rw}
\end{table}

\begin{table}[h!]
    \centering
    \small
    \begin{tabular}{cccc}
        \toprule
        {}    & Â \textbf{Unique} &\textbf{Train} & \textbf{Train$_{>=5}$} \\
        \pulrad{\textbf{Set}} & \textbf{Tokens} &\textbf{Overlap} & \textbf{Overlap} \\
        \midrule
        valid                & 2582 & 98.80\% & 95.70\% \\
        test                 & 5741 & 98.69\% & 95.45\%
    \end{tabular}
    \caption{\small Overlap of transformed train dataset summaries and valid/test dataset summaries} \label{table_vt_final_rw}
\end{table}

% nice formatting of python code
% https://tex.stackexchange.com/questions/105662/default-value-for-basicstyle-in-lstlisting/122916#122916
\lstdefinestyle{shared}
{
    numbers=left,
    numbersep=1em,
    numberstyle=\tiny\color{red},
    frame=single,
    framesep=\fboxsep,
    framerule=\fboxrule,
    rulecolor=\color{red!20},
    linewidth=13.7cm,
    breaklines=true,
    tabsize=2,
    columns=flexible,
}

\lstdefinestyle{python}
{
    style=shared,
    escapechar=\^,
    language={Python},
    basicstyle=\small\tt,
    keywordstyle=\color{blue},
    commentstyle=\color[rgb]{0.13,0.54,0.13},
    backgroundcolor=\color{cyan!5},
}

\lstnewenvironment{python}
{\lstset{style=python}}
{}

\begin{algorithm}[!h]
\begin{python}
import re, collections

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
    symbols = word.split()
    for i in range(len(symbols)-1):
        pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
    w_out = p.sub(''.join(pair), word)
    v_out[w_out] = v_in[word]
    return v_out

vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,
            'n e w e s t </w>':6, 'w i d e s t </w>':3}
num_merges = 10
for i in range(num_merges):
    pairs = get_stats(vocab)
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print(best)

> r ^$\cdot$^ ^$\rightarrow$^ r^$\cdot$^
> l o ^$\rightarrow$^ lo
> lo w ^$\rightarrow$^ low
> e r^$\cdot$^ ^$\rightarrow$^ er^$\cdot$^
\end{python}
\caption{Learn BPE operations \\ Extract from paper \textbf{Neural Machine Translation of Rare Words with Subword Units} by \citep{sennrich2016}}
\label{bpe-algorithm}
\end{algorithm}
