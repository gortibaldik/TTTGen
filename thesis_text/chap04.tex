\chapter{Neural Network Architectures} \label{neural_nets_chapter}
To generate text from structured data I make use of deep neural networks. In previous chapter (\ref{chapPreproc}) I've shown how to transform the structured tabular data into a sequence of records, therefore I've reduced the problem to an instance of the famous sequence to sequence problem. Now, I show how to create a system that transforms the input sequential data (structured records) to the output sequential data (natural language).

The most common way to tackle the sequence to sequence problem is to use the Encoder-Decoder architecture proposed by \citep{sutskever2014sequence}. It is the main approach I used throughout this thesis. In this chapter I introduce the concepts behind the encoder-decoder architecture, its shortcomings (fixed vocabulary and thus problems with generation of words unseen during training, divergence and hallucinations) and ways to overcome these shortcomings (the attention mechanism, the copy mechanisms, the further transformations of input sequences). Since it is not the purpose of this work, only the basics of the concepts are presented, and I provide links to papers, books and tutorials which helped me on my path to understanding.

\subsubsection{Notation}
Many papers diverge on the notation and naming conventions of the architectures. Therefore I choosed to adopt the notation used in \emph{Tensorflow Keras API, version 2.x} \citep{tensorflow2015-whitepaper}. Specifically in the field of recurrencies it is discutable if the paper refers to \emph{tf.keras.layers.RNNCell} or to \emph{tf.keras.layers.RNN}. I believe that they can be used interchangeably in the context of this chapter, hence I (deliberately) choose the latter notation (\emph{without 'Cell'}).

\subsubsection{A Note About Embeddings}

An embedding is a low dimensional continuous representation of discrete variables\footnote{I took the sentence from the accepted answer on stack overflow as it is the best description I have found. It is the first and the last time I "cite" from stack overflow. Source : \url{https://datascience.stackexchange.com/questions/53995/what-does-embedding-mean-in-machine-learning}}. In this work I don't experiment with pretrained word embeddings, and I tune only the embedding-dimension hyperparameter, which adjusts the dimensionality of the trained continuous representation of the input.

\section {The Encoder-Decoder Architecture}

Proposed by \citep{sutskever2014sequence} the Encoder-Decoder is composed of 2 recurrent units, called Encoder and Decoder. In this section I briefly introduce the Recurrent Neural Network (\emph{tf.keras.layers.SimpleRNN}), its modification, the Long Short-Term Memory (\emph{tf.keras.layers.LSTM}) \citep{hochreiter1997} and the high-level overview of the Encoder-Decoder architecture.

\subsection{Recurrent Neural Network}

Let $\boldsymbol{x}=(x^{(1)},\dots,x^{(t)})$ be the input. The standard Feed-Forward Network (\emph{tf.keras.layers.Dense}) has a different set of weights for each input time-step $x^{(t)}$, therefore the number of time-steps of the input needs to be known in advance.

\begin{figure}[!h]
\centering
The Feed-Forward Neural Network
\begin{equation}
\boldsymbol{y} = activation(W\boldsymbol{x} + b) \mbox{}
\end{equation}
\end{figure}
\interfootnotelinepenalty=10000
On the contrary, the Recurrent Neural Network (RNN) \citep{rumelhart_rnn1988} (\emph{tf.keras.layers.SimpleRNN}) shares the same set of weights between time-steps and in addition it keeps a hidden state. At each time-step the hidden state is updated and used to calculate the output as in equation \ref{basic_rnn}. The computation can be visualized either as a loop, or as a feed-forward network with shared weights \ref{vis_rnn}.


\begin{figure}[!h]
    \centering
    The Recurrent Neural Network
    \begin{align} \label{basic_rnn}
    \begin{split}
        &h_t = f_h(x_t, h_{t-1}) \\
        &y_t = f_t(h_t)
    \end{split}
    \end{align}
    \footnotesize{\textit{Note:} $h_t$ is the hidden state; $y_t$ is the output at $t$-th timestep; tf.keras.layers.SimpleRNN uses $f_t = id$ as default.}
    \includegraphics[width=121.41mm, height=41.4mm]{img/simple_rnn.jpg}
    \caption{Visualizations of the RNN} \label{vis_rnn}
\end{figure}

The network is trained by back-propagation through time (BPPT) \citep{bpptWerbos1990}. It has been shown that the RNN suffers from vanishing / exploding gradient problems \citep{hochreiter1997}.\footnote{I believe that discussion about BPPT and exploding/vanishing gradient problems is beyond the scope of this work. Therefore I refer the reader craving for further explanation to \citep{Goodfellow-et-al-2016} and to referenced papers.} Which cause that either the RNN cannot learn anything or it is really unstable. These difficulties are adressed by more sophisticated architectures such as Gated Recurrent Unit\footnote{Although it is one of the most known architectures I used only LSTM for my experiments. } \citep{cho2014learning} or Long Short-Term Memory \citep{hochreiter1997}.


\subsection{Long Short-Term Memory}

The Long Short-Term Memory (\emph{tf.ke\-ras.lay\-ers.LSTM}) addresses the vanishing gradient problem. It does so by adding a special cell state for capturing long range context and series of gating mechanisms. The latter update the cell state and regulate the flow of gradient through the network (as shown in figure \ref{lstm}). The more in-depth explanation can be found in \citep{Olah2015}.

\begin{figure}[!ht]
    \begin{gather}
        y_t = W_{hy}h_t + b_y \\
        h_t = o_t tanh(c_t) \\
        o_t = \sigma(W_o[h_{t-1};x_t] + b_o) \\
        c_t = f_t * c_{t-1} + i_t * tanh(W_c[h_{t-1}; x_t] + b_c)\\
        i_t = \sigma(W_i[h_{t-1}; x_t] + b_i)\\
        f_t = \sigma(W_f[h_{t-1}; x_t] + b_f)
    \end{gather}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=113.44mm, height=42.64mm]{img/LSTM3-chain.png}
    \caption{Visualization of LSTM \citep{Olah2015}} \label{lstm}
\end{figure}

\subsection{High-level Overview of Encoder-Decoder Architecture} \label{high-level_enc_dec_section}

As stated by \citep{sutskever2014sequence}, the main goal of the encoder-decoder architecture is to model the conditional probability $p(y_1,\dots,y_n | x_1,\dots,x_m)$ of the output sequence $y_1,\dots,y_n$ conditioned on the input sequence $x_1,\dots,x_m$. It uses two separate \emph{recurrent networks}\footnote{Here I refer to \emph{recurrent network} as to a complex consisting of at least one RNN/LSTM/GRU/\dots rather than to a single recurrent layer.}. The first, called Encoder, produces a fixed-dimensional representation $r$ of the input sequence. $r$ is then used to initialize the hidden states of the second recurrent network, called a Decoder. The Decoder then generates the output sequence as in equation \ref{enc_dec_base}.

\begin{equation} \label{enc_dec_base}
p(y_1,\dots,y_n | x_1,\dots,x_m) = \prod_{t=1}^{n}{p(y_t | r, y_1,\dots,y_{t-1})}
\end{equation}

The Encoder part is straight-forward, however in the Decoder part two important questions arise:
\begin{enumerate}
    \item What should be the inputs to the Decoder
    \item When should we stop decoding (generating the output sequence)
\end{enumerate}

As visualized in figure \ref{enc_dec_visualization}, to tell the Decoder when to \emph{start} generation, a special \emph{\textless BOS \textgreater} token (\emph{beginning of sequence}) is fed in. The decoding phase is slightly different during training and inference. During training, the inputs of the decoder are targets from the \emph{previous} time-step. This process is called \emph{teacher forcing}. After the last time-step, the target is the special \emph{\textless EOS \textgreater} token (\emph{end of sequence}). In the inference phase, the last output of the Decoder is fed in as the input, and the decoding finishes after producing the \emph{\textless EOS \textgreater} token.

\begin{figure}[!h]
    \centering
    \includegraphics[width=84.07mm, height=40.89mm]{img/enc_dec_basic.jpg}

    \footnotesize{\textit{Note:} Encoder is red, Decoder is blue \\ }
    \footnotesize{none of the Encoder's outputs is used}
    \caption{\centering Visualization of the training of the Encoder-Decoder Architecture with \emph{teacher forcing}.} \label{enc_dec_visualization}
\end{figure}

\subsection{Problems of the Encoder-Decoder Architecture}

Despite having many advantages (variable length of the input and output sequence, possibility of extending the number of recurrent layers in the encoder and the decoder) there are some major flaws that need to be overcome in order to generate text from structured data.

\subsubsection{Fixed-dimensional Representation of the Input Sequence} \label{fixed_repre_problem}

It has been shown by \citep{cho2014properties} that the performance of the Encoder-Decoder architecture "suffers significantly from the length of sentences". \citep{bahdanau2016neural} hypothesize that it may be because all of the information from the source sequence is encoded to the fixed-dimensional vector. Both mentioned papers understand word \emph{"long"} as \emph{longer than 30 tokens}. From the chapter about the preprocessing \ref{chapPreproc}, we know that there are more than 300 records in the average input from the RotoWire dataset. Consequently this particular problem should be seen in our task. 

\subsubsection{Rare-word Problem and Hallucinations} \label{rare_word_problem}

In the standard Encoder-Decoder, the output is a distribution over the output vocabulary. At the start of the training each word is equally probable in any given context (assuming reasonable weights initialization). During training, model learns the language model on the training data. There are several flaws in the design:

\begin{enumerate}
    \item It essentially means that e.g. words 'the' and 'Roberta' compete against each other, although one depends purely on the language skill (perhaps the next token after 'the' would be superlative) and the other one on the input sequence (which probably mentions some AI research).
    \item As pointed out by e.g. \citep{gulcehre2016pointing}, (although not on this particular example) word 'Roberta' occurs less frequently in the training data than the word 'the', thus it is "difficult to learn a good representation of the word, which results in poor performance"\footnote{The problem is called \emph{The Rare-Word Problem.}}
    \item Networks tend to \emph{hallucinate} the facts. E.g. from the record \emph{\{type:transfor\-mer; value:GPT\}} network generates a sentence "The famous example of transformer architecture is BERT." To put it simply, the network knows it should talk mention a transformer, therefore each word describing some kind of a transformer is somewhat probable, even if it wasn't seen in the actual input.
\end{enumerate}

I have already discussed how to increase the average frequency of a token to minimize the Rare-Word Problem through preprocessing (section \ref{bpeSection}). In the following sections I show the methods which handle hallucinations (section \ref{copy_mech_sec}).

\section{Attention Mechanism}

The Attention mechanism should cope with the issue of fixed-dimensional representation of the input sequence \ref{fixed_repre_problem}. As stated by \citep{bahdanau2016neural} "The most important distinguishing feature of this approach from the basic encoder-decoder is that it does not attempt to encode a whole input sequence into a single fixed-length vector".

The encoder is a recurrent neural network\footnote{From now on, I'll stick to refer to \emph{recurrent neural network} as to the neural network consisting of at least one \emph{tf.keras.layers.RNN} or relatives.}. From the overall architecture (figure \ref{enc_dec_visualization}) we can see that only the last hidden state of the encoder is used, although there are encoder outputs for each time-step. \citep{bahdanau2016neural} propose an architecture which takes advantage of this simple observation. In my work I use a little refinement, proposed by \citep{luong2015effective}.\footnote{Since I don't experiment with the original Bahdanau attention, I only show the Luong's approach. \citep{luong2015effective} shows all the differences between his and Bahdanau's approach in section 3.1 of the paper.}

\subsection{Computation}

Let's start with the description of the computation that produces the attention output.

As stated previously, the Encoder encodes the input sequence to \emph{encoder outputs} $\boldsymbol{e} = (e_1, \dots, e_m)$ and the Decoder RNN is initialized with the last hidden state of the Encoder.

Let $d_t$ be the output of the Decoder RNN at $t$-th time-step. At first we calculate the \emph{score vector} $\boldsymbol{s_t} = (s_{t,1},\dots,s_{t,m})$. Its elements are computed using \emph{a score function} (which we will talk about below \ref{score_input_feeding}):
\begin{equation}
    s_{t,i} = score(e_i, d_t)
\end{equation}
According to \citep{bahdanau2016neural}, the alignment vector $\boldsymbol{a_t}$
\begin{equation}
    \boldsymbol{a_t} = softmax(\boldsymbol{s_t})
\end{equation}
"scores how well the inputs around position $i$ and the output at position $t$ match". The weighted sum of the outputs of the encoder, is called a \emph{context vector} for time-step $t$.
\begin{equation}
    c_t = \sum_{i=1}^m{a_{t,i}e_i}
\end{equation}
Unlike in the standard Encoder-Decoder architecture, the output of the decoder also depends on the  context vector.
\begin{gather}
    att_t = tanh(W_c[c_t;d_t]) \\
    p(y_t | y_{<t}, x)= softmax(W_y att_t)
\end{gather}

\subsection{Score Functions and Input Feeding} \label{score_input_feeding}
\citep{luong2015effective} experimented with three different types of score functions. We adopted two of them, the \emph{dot} and \emph{concat} ones. (The following equations are directly extracted from the Luong's paper)
\begin{equation*}
score(e_i, d_t)\!=\!\begin{cases}
    e_i^\top d_t & \mbox{{\it dot}}\\
    v_s^\top tanh(W_s[e_i ; d_t]) & \mbox{{\it concat}}
\end{cases}
\end{equation*}

The same author also states that the fact that the attentional decisions are made independently is \emph{suboptimal}. Hence the \emph{Input Feeding} approach is proposed to allow the model to take into acount its previous decisions. It simply means that the next input is the concatenation $[y_t, att_t]$ (as shown in figure \ref{input_feeding_attention}).

\begin{figure}[!h]
    \centering
    \includegraphics[width=89.92mm, height=104.65mm]{img/att_luong.jpg}
    \caption{\centering The Attention mechanism at the second time-step. Dotted line represents the input feeding approach.} \label{input_feeding_attention}
\end{figure}


\section{Copy mechanism} \label{copy_mech_sec}
The copy mechanism is a further extension of the attention mechanism. In this section I discuss the Pointer networks \citep{vinyals2015pointer}, which are trained to \emph{point} to some position in the input sequence and the Copy Mechanisms \citep{gulcehre2016pointing}, \citep{gu2016incorporating}, \citep{yang2016referenceaware} which model the decision making (whether to copy from the pointed location or to generate from the actual context).

\subsection{Pointer Networks}

The Pointer networks \citep{vinyals2015pointer} leverage the fact that the alignment vector $\boldsymbol{a_t}$  can be seen as \emph{a pointer} to the input sequence. Consequently instead of computing the weighted sum (\emph{context vector}) and an MLP on top of that as the Attention models, they utilize the \emph{alignment vector} as an output.

\subsection{Approaches to Copying}

\citep{gulcehre2016pointing} note that the ability to point is useless in the generation task if the network is always forced to point. Therefore they introduce a new switching network that outputs a binary variable $z_t$ , which models the probability of the required action being pointing or generating.

Let $\boldsymbol{e} = (e_1, \dots, e_m)$ be the encoder outputs, $\boldsymbol{x} = (x_1, \dots, x_m)$ the input sequence, $d_t$ the output of the Decoder RNN at the actual time-step, and $ATTN$ the Attention as presented in the previous section.

The probability of gold output $y_t$ and gold switch decision $z_t$ is decomposed to $p^{gen}(y_t|\boldsymbol{x})$ (the probability that $y_t$ should be generated), $p^{switch}(z_t|\boldsymbol{x})$ (the probability that we should copy) and $p^{copy\_pos}(y_t = x_i |\boldsymbol{x})$ (the probability that $y_t$ should be copied from the input position i).:
\begin{gather}
    \boldsymbol{a_t} = ATTN(\boldsymbol{e}, d_t) \\
    p^{copy\_pos}(y_t = x_i | \boldsymbol{x}) = a_{t, i} \\
    \boldsymbol{c_t} = \sum_{i=1}^m{e_i * a_{t, i}} \\
    p^{switch}(z_t | \boldsymbol{s}) = sigmoid(W_{switch}[\boldsymbol{c_t}, d_t]) \\
    p^{gen}(y_t | \boldsymbol{s}) = softmax(W_{gen}[\boldsymbol{c_t}, d_t])
\end{gather}

\citep{gulcehre2016pointing} explicitly model each of these probabilities (therefore the targets contain 3 values for each time-step). \citep{yang2016referenceaware} marginalize out the switch probability $z_t$, and they model $p = p^{copy} * p^{switch} + p^{gen} * (1 - p^{switch})$.

To be able to follow their path, I take the one-hot encoding of each input and compute the weighted sum:
\begin{equation}
    p^{copy}(y_t|\boldsymbol{x}) = \sum_{i=1}^m{p^{copy\_pos}(y_t = x_i | \boldsymbol{x}) * x_i}
\end{equation}

Throughout our task the input and output vocabularies are shared, therefore the weighted sum of the inputs has the same dimensionality as the output generation distribution $p^{gen}$.

Consequently, the probability of the gold output $y_t$ at time-step $t$ is computed as follows:
\begin{equation}
    p(y_t|\boldsymbol{x}) = p^{gen}(y_t|\boldsymbol{x})*(1-p^{switch}(1|\boldsymbol{x})) + p^{copy}(y_t|\boldsymbol{x})*p^{switch}(1|\boldsymbol{x})
\end{equation}

\begin{figure}[hb]
    \centering
    \includegraphics[scale=0.65]{img/attention_alignment.pdf}
    \caption{Excerpt from \textbf{Pointing the Unknown Words} paper by \citep{gulcehre2016pointing}, showing how the attention alignment can be utilized as a pointer information}
\end{figure}