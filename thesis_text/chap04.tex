\chapter{Neural Network Architectures}
To generate text from structured data I make use of deep neural networks. In previous chapter (\ref{chapPreproc}) I've shown how to transform the structured tabular data into a sequence of records, therefore I've reduced the problem to an instance of the famous sequence to sequence problem. Now, I show how to create a system that transforms the input sequential data (structured records) to the output sequential data (natural language).

The most common way to tackle the sequence to sequence problem is to use the Encoder-Decoder architecture proposed by \citep{sutskever2014sequence}. It is the main approach I used throughout this thesis. In this chapter I introduce the concepts behind the encoder-decoder architecture, its shortcomings (fixed vocabulary and thus problems with generation of words unseen during training, divergence and hallucinations) and ways to overcome these shortcomings (the attention mechanism, the copy mechanisms, the further transformations of input sequences). Since it is not the purpose of this work, only the basics of the concepts are presented, and I provide links to papers, books and tutorials which helped me on my path to understanding.

Many papers diverge on the notation and calling conventions of the architectures. Therefore I choosed to adopt the notation used in \emph{Tensorflow Keras API, version 2.x} (\citep{tensorflow2015-whitepaper}). Specifically in the field of recurrencies it is discutable if the paper refers to \emph{tf.keras.layers.RNNCell} or to \emph{tf.keras.layers.RNN}. I believe that they can be used interchangeably in the context of this chapter, hence I (deliberately) choose the latter notation (\emph{without 'Cell'}).

\section {The Encoder-Decoder Architecture}

Proposed by \citep{sutskever2014sequence} the Encoder-Decoder is composed of 2 recurrent units, called Encoder and Decoder. In this section I briefly introduce the Recurrent Neural Network (\emph{tf.keras.layers.SimpleRNN}), its modification, the Long Short-Term Memory (\emph{tf.keras.layers.LSTM})(\citep{hochreiter1997}) and the high-level overview of the Encoder-Decoder architecture.

\subsection{Recurrent Neural Network}

Let $\mathbf{x}=(x^{(1)},\dots,x^{(t)})$ be the input. The standard Feed-Forward Network (\emph{tf.keras.layers.Dense}) has different set of weights for each input time-step $x^{(t)}$, therefore the number of time-steps of the input needs to be known in advance.

\begin{figure}[!h]
\centering
\begin{equation}
y = activation(W\mathbf{x} + b)
\end{equation}
The Feed-Forward Neural Network
\begin{align} \label{basic_rnn}
\begin{split}
    &h_t = f_h(x_t, h_{t-1}) \\
    &y_t = h_t
\end{split}
\end{align}
The Recurrent Neural Network \\
\footnotesize{\textit{Note:} $h_t$ is the hidden state, and $y_t$ is the output at $t$-th timestep}
\end{figure}
\interfootnotelinepenalty=10000
On the contrary, the Recurrent Neural Network (RNN) (\citep{rumelhart_rnn1988}) (\emph{tf.keras.layers.SimpleRNN}) shares the same set of weights for each time-step and in addition it keeps a hidden state. At each time-step we update the hidden state and compute the output as in equation \ref{basic_rnn}. The computation can be visualised either as a loop, or as a feed-forward network with shared weights \ref{vis_rnn}.


\begin{figure}[!h]
    \includegraphics[width=138mm, height=35.6mm]{img/RNN-unrolled.png}
    \caption{Possible visualisations of RNN, with $f_o \cong id$ (\citep{Olah2015})} \label{vis_rnn}
\end{figure}

The network is trained by back-propagation through time (BPPT) \citep{bpptWerbos1990}. It has been shown that the RNN suffers from vanishing / exploding gradient problems \citep{hochreiter1997}.\footnote{I believe that discussion about BPPT and exploding/vanishing gradient problems is beyond the scope of this work. Therefore I refer the reader craving for further explanation to \citep{Goodfellow-et-al-2016} and to referenced papers.} Which cause that either the RNN cannot learn anything or it is really unstable. These difficulties are adressed by more sophisticated architectures such as Gated Recurrent Unit or Long Short-Term Memory.


\subsection{Long Short-Term Memory}

The Long Short-Term Memory (\citep{hochreiter1997})(\emph{tf.ke\-ras.lay\-ers.LSTM}) addresses the problem of vanishing gradient. It does so by adding a special cell state for capturing long range context and a series of gating mechanisms. The latter update the cell state and regulate the flow of gradient through the network (as shown in figure \ref{lstm}). The more in-depth explanation can be found in \citep{Olah2015}.

\begin{figure}[!h]
    \begin{gather}
        y_t = W_{hy}h_t + b_y \\
        h_t = o_t tanh(c_t) \\
        o_t = \sigma(W_o[h_{t-1};x_t] + b_o) \\
        c_t = f_t * c_{t-1} + i_t * tanh(W_c[h_{t-1}; x_t] + b_c)\\
        i_t = \sigma(W_i[h_{t-1}; x_t] + b_i)\\
        f_t = \sigma(W_f[h_{t-1}; x_t] + b_f)
    \end{gather}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=113.44mm, height=42.64mm]{img/LSTM3-chain.png}
    \caption{Visualisation of LSTM (\citep{Olah2015})} \label{lstm}
\end{figure}

\subsection{High-level Overview of Encoder-Decoder Architecture}

As stated by \citep{sutskever2014sequence}, the main goal of the encoder-decoder architecture is to model the conditional probability $p(y_1,\dots,y_n | x_1,\dots,x_m)$ of the output sequence $y_1,\dots,y_n$ conditioned on the input sequence $x_1,\dots,x_m$. It uses two separate \emph{recurrent networks}\footnote{Here I refer to \emph{recurrent network} as to a complex consisting of at least one RNN/LSTM/GRU/\dots rather than to a single recurrent layer.}. The first, called Encoder, produces a fixed-dimensional representation $r$ of the input sequence. $r$ is then used to initialize the hidden states of the second recurrent network, called a Decoder. The Decoder then generates the output sequence as in equation \ref{enc_dec_base}.

\begin{equation} \label{enc_dec_base}
p(y_1,\dots,y_n | x_1,\dots,x_m) = \prod_{t=1}^{n}{p(y_t | r, y_1,\dots,y_{t-1})}
\end{equation}

The Encoder part is straight-forward, however in the Decoder part two important questions arise:
\begin{itemize}
    \item 1. What should be the inputs to the Decoder
    \item 2. When should we stop decoding (generating the output sequence)
\end{itemize}

To tell the Decoder when to \emph{start} generation, a special \emph{\textless BOS \textgreater} token (\emph{beginning of sequence}) is fed in. The decoding phase is slightly different during training and inference. During training, the inputs of the decoder are targets from the \emph{previous} time-step. This process is called \emph{teacher forcing}. After the last time-step, the target is the special \emph{\textless EOS \textgreater} token (\emph{end of sequence}). In the inference phase, the last output of the Decoder is fed in as the input, and the decoding finishes after producing the \emph{\textless EOS \textgreater} token.

\begin{figure}[!h]
    \centering
    \includegraphics[width=84.07mm, height=40.89mm]{img/enc_dec_basic.jpg}

    \footnotesize{\textit{Note:} Encoder is red, Decoder is blue}
    \caption{Visualisation of the computation of the Encoder-Decoder Architecture} \label{enc_dec_visualisation}
\end{figure}

\subsection{Problems of the Encoder-Decoder Architecture}

Despite having many advantages (variable length of the input and output sequence, possibility of extending the number of recurrent layers in the encoder and the decoder) there are some major flaws that need to be overcome in order to generate text from structured data.

\subsubsection{Fixed-dimensional Representation of the Input Sequence} \label{fixed_repre_problem}

It has been shown by \citep{cho2014properties} that the performance of the Encoder-Decoder architecture "suffers significantly from the length of sentences". \citep{bahdanau2016neural} hypothesize that it may be because all of the information from the source sequence is encoded to the fixed-dimensional vector. Both mentioned papers understand word \emph{"long"} as \emph{longer than 30 tokens}. From the chapter about the preprocessing \ref{chapPreproc}, we know that there are more than 40 records in the average input from the WikiBIO dataset and more than 300 records in the average input from the RotoWire dataset.

\subsubsection{Named Entities and Unknown Words}

In the standard Encoder-Decoder, the output is a distribution over the output vocabulary. The distribution is modelled with \emph{the softmax function}. There are two flaws in the design.

Firstly, \emph{the softmax function} is computationally expensive. Therefore only the most frequent words in the training dataset are included to the vocabulary and all the others are replaced by the \emph{\textless UNK \textgreater} token. Consequently we loose information about the words which were substitued out. I've already shown one way to overcome this issue in subsection \ref{bpeSection}. However in the subsection \emph{reference should be there} I'll show how to overcome the issue in other ways than through clever preprocessing.

Secondly, it essentially means that e.g. words 'the' and 'Roberta' compete against each other, although one depends purely on the language skill (perhaps the next token after 'the' would be superlative) and the other one on the input sequence (which probably mentions some AI research).

Thirdly, as pointed out by e.g. \citep{gulcehre2016pointing}, (although not on this particular example) word 'Roberta' occurs less frequently in the training data than the word 'the', thus it is "difficult to learn a good representation of the word, which results in poor performance".

\section{Attention}

As stated by \citep{bahdanau2016neural} "The most important distinguishing feature of this approach from the basic encoder-decoder is that it does not attempt to encode a whole input sequence into a single fixed-length vector". Therefore it solves the first problem (\ref{fixed_repre_problem}) and as I will show later, I believe it partly solves the second one too.

The encoder is an recurrent neural network\footnote{From now on, I'll stick to refer to \emph{recurrent neural network} as to the neural network consisting of at least one \emph{tf.keras.layers.RNN} or relatives.}. From the overall architecture (figure \ref{enc_dec_visualisation}) we can see that only the last hidden state of the encoder is used, although there are encoder outputs for each time-step. \citep{bahdanau2016neural} proposes an architecture which takes advantage of this simple observation.

Particularly, 

\section{Copy mechanism}
What it is, right now I've implemented only Joint Copy mechanism, possibly add Conditional Copy

\section{Truncated Backpropagation Through Time}
Why it is infeasible to generate sequences of average 350 tokens with full backpropagation. Which types of truncated BPTT exist and which I've chosen.

\section{Beam search}
Why greedy search isn't enough, what is beam search, when is it used.

\section{Transformers}
Right now I don't think I'll get this far in my exploration and implementation of DNN architectures.
