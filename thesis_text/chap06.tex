\chapter{Experiments} \label{chapter:experiments}

In previous chapters we have presented the dataset (chapter \ref{chapter:data}), and discussed challenges which arise from its properties (chapter \ref{chapter:preprocessing_and_statistics_of_the_dataset}). Next, we described the neural network architectures (chapter \ref{chapter:neural_network_architectures}) which served as building blocks for the models introduced in chapter \ref{chapter:experimental_setup}. Now we want to connect all parts together, elaborate on the evaluation methods and analyze the results of different models.

Let us recap the main challenges we face, and hypothesize about the ideal generation system. Firstly, the target summaries are really long. The ideal system should remember what has already been generated and shouldn't produce duplications. Secondly, the targets contain a lot of facts based on the input structured data. The ideal system should copy these facts from the input and reduce \emph{hallucinations}. Lastly, the generated text should be as close to English as possible \emph{while meeting the requirements described previously}.

\section{Evaluation Methods}

During evaluation we want to measure which model resembles the hypothetical ideal model the most. To do so, we report the BLEU score \citep{papineni2002} of the postprocessed generated summaries on the validation and test sets. Although it is the gold standard, there are many people arguing against its usage as a performance metric \citep{celikyilmaz2021evaluation}. Since the BLEU score does not explicitly penalize hallucinated statements, or reward the right selection of entities to talk about, we also manually evaluate a subset of the generated summaries.

\subsection{Manual Evaluation}

We used a pseudo random generator to pick 8 different data points, 4 from the validation and 4 from test set\footnote{Specifically samples $132$, $319$, $475$ and $709$ from the validation set and samples $16$, $247$, $585$ and $671$ from the test set}. We looked at the factual correctness (how many of the generated numbers describing team and player statistics are based on the input tabular data), and entity recall (how many of the entities mentioned in the gold summaries are present in the generated ones). Due to time-requirements of the process, only the best model\footnote{In the respective sections we explain why we think the selected model is the best one.} is manually evaluated. Therefore manual evaluation should show the differences between approaches and architectures rather than between variations of the same model.

\subsection{Ommited Evaluation Methods}

\citet{wiseman2017} proposed three custom automated metrics to evaluate the performance of their models. They call them \emph{Content Selection} ("how well the generated document matches the gold document in terms of selecting which records to generate"), \emph{Relation Generation} ("how well the system is able to generate text containing factual (i.e., correct) records") and \emph{Content Ordering} ("how well the system orders the records it chooses to discuss"). The metrics are implemented in the outdated neural network framework \emph{Torch} and we were not able to execute it on my computers. After discussions with my advisor we agreed to not adopt these methods.

\subsection{Comparison to Related Work}

Due to massive preprocessing we have introduced, our results are not directly comparable to any related work. (E.g. the name transformations introduced in section \ref{subsection:player_name_transformations_summary} and lowercasing increase the BLEU score, while being practically irreversible\footnote{It is impossible to tell if the player should be called by his full name or only by his second name.})

\section{Baseline Model} \label{section:experiments_baseline_model}

At first we would like to summarize our expectations. \citet{wiseman2017} note that even models with explicit copy mechanisms tend to \emph{hallucinate} the facts. However they do not show the results of any baseline without copying. We expect the baseline to generate a fairly good English, even though the summaries should not be factually correct.

We searched through the hyperparameter space (possible combinations of learning rates, dropout probability $p$, hidden state dimensionality etc.) until the generated language as well as the numerical results haven't fulfilled the expectations at least by half.

The overall architecture of the baseline model is discussed in section \ref{section:baseline_model}. Figure \ref{figure:hyperparameters_baseline} shows the hyperparameter choice for discussed baseline models.

\begin{figure}[h]
    \scalebox{0.8}{
    \begin{tikzpicture}
    \node(embeddings) [] {
        \small
        \begin{tabular}{ll}
            \toprule
            {} & \textbf{Embedding} \\
            \pulrad{\textbf{Token}} & \textbf{Dimensionality} \\
            \midrule
            \textbf{$record.type$} & 300 \\
            \textbf{$record.home\_away$} & 300 \\
            \textbf{$record.value$} & 600 \\
            \textbf{$record.entity$} & 600 \\
            \textbf{$summary\ token$} & 600
        \end{tabular}
    };
    \node(hidden) [above left=-20.7mm and 5mm of embeddings] {
        \small
        \begin{tabular}{ccc}
            \toprule
            \textbf{Hidden States} & {} & {} \\
            \textbf{Dimensionality} & \pulrad{\textbf{Learning Rate}} & \pulrad{\textbf{Batch Size}} \\
            \midrule
            600 & 0.001 & 16
        \end{tabular}
    };
    \end{tikzpicture}
    }
    \caption{Hyperparameter settings for baseline models.} \label{figure:hyperparameters_baseline}
\end{figure}

The first baseline model is trained without any regularization\footnote{We trained the model just to see how far we can get without any advanced methods.}, the second one with dropout on the output LSTM units with $p_{dropout} = 0.3$ (the probability that the unit is dropped is $0.3$) and scheduled sampling with constant rate of $0.8$ (the probability that the gold output from the previous timestep is used as the actual input is $0.8$). We discussed two possibile implementations of the score function in the Attention mechanism in section \ref{subsection:score_input_feeding}. In our experiments we have not seen any difference in performance of the methods, thus in the following text we always use the \emph{dot} score function.

\subsection{Results}

Baseline model wasn't able to capture the relationships between the input tabular data and the output summaries (the best configuration achieved perplexity $10.59$). We found that dropout helps to reduce overfitting while scheduled sampling improved the quality of the generated summaries. (We present only models that achieved reasonable performance.) The quality of generated summaries is further improved with beam search. Although the statement contradicts the calculated BLEU scores (as the score decreased after introducing beam search), the summaries contain more natural language and less repetitions. The expectations are fulfilled in terms of \emph{hallucinations}, as the model makes up \emph{almost all the factual statements}.

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
        {} & \textbf{Validation} & \textbf{Validation} & \textbf{Test} & \textbf{Entity} & \textbf{Factual} \\
        \pulrad{\textbf{Model}} & \textbf{Perplexity} & \textbf{BLEU} & \textbf{BLEU} & \textbf{Recall} & \textbf{Correctness} \\
        \midrule
        BN$_G$ & {} & 9.04 & 9.10 & -- & -- \\
        BN$_{B5}$ & \pulrad{12.68} & 8.73 & 8.55 & -- & -- \\
        \hline
        BR$_G$ & {} & 10.0 & 10.47 & -- & -- \\
        BR$_{B5}$ & \pulrad{10.59} & 9.99 & 10.6 & 37.35\% & 8.03\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize{$_{G}$ - Greedy Decoding}} \\
        \multicolumn{4}{l}{\footnotesize{$_{B5}$ - Beam search decoding, beam size $=5$}} \\
        \multicolumn{4}{l}{\footnotesize{BN - Baseline, non-regularized}} \\
        \multicolumn{4}{l}{\footnotesize{BR - Baseline, regularized (dropout $0.3$, scheduled sampling $0.8$)}}
    \end{tabular}
    }
    \caption{Performance metrics on the baseline models.} \label{table:metrics_baseline}
\end{table}

Figure \ref{figure:baseline_generated} shows an example generated by the regularized baseline model with beam search decoding. We observe\footnote{We opted to not show the same tabular information multiple times. A part of the table corresponding to the generated summary can be found in figure \ref{figure:samplesummary}} that the score, the winning-loosing records\footnote{\emph{"The Raptors ( 21 - 15)"} means that Raptors have won $21$ matches and lost $15$ matches this season.} as well as all the individual statistics are \emph{hallucinated}. We found that the model repeatedly (over multiple observed summaries) mentions the same players and uses the same numbers. This experiment thus supports our assumptions. Statistics in table \ref{table:metrics_baseline} show that only about $8$ \% of the numbers aren't \emph{hallucinated} and that the recall is lower than $38$ \%. Therefore we can conclude that model learned to produce probable numbers and names of star players of the respective teams. 


\begin{figure}[h]
    \scalebox{0.85}{
    \begin{tikzpicture}
    \node(summary) [rectangle, draw,thick,fill=blue!0,text width=39em, rounded corners, inner sep =8pt, minimum height=1em]{
        \baselineskip=100pt
        \small
        The Toronto Raptors defeated the Philadelphia 76ers , 109 - 95 , at air canada centre on tuesday . The Raptors ( 21 - 15 ) checked in to saturday 's contest trying campaign , having won 7 of their last 7 games . However , Philadelphia was able to pull away in the second half , as they outscored Philadelphia by a 59 - 44 margin over the second and third periods . DeMar DeRozan led the way with a points - point , 10 - rebound double - double that also included an assist , a steal and a block . Kyle Lowry was right behind with a points - point , 10 - rebound double - double that also included an assist , a steal and a block . Jonas Valanciunas posted a near double - double with 17 points , 9 rebounds , 2 assists and a steal . Jonas Valanciunas posted a near double - double with 11 points , 9 rebounds , 2 assists and a steal . Ersan Ilyasova was productive in a reserve role as well with 11 points , 4 rebounds , an assists and a steal . The Raptors head back home to take on the Toronto Raptors on saturday night , while the Raptors remain home to tangle with the Brooklyn Nets on tuesday evening as well .
    };
    \end{tikzpicture} }
    \caption{\centering A summary generated by the baseline model. The corresponding gold summary and input table is shown in figure \ref{figure:samplesummary}.} \label{figure:baseline_generated}
\end{figure}

\section{Joint-Copy Model}

The Joint-Copy Model is expected to produce more factually correct statements. \citet{wiseman2017} select this model as their baseline. We would like to reiterate that due to massive preprocessing that we introduced, the results we obtained aren't comparable to theirs. (E.g. Wiseman et al. report BLEU $10.41$ for the Joint-Copy model while we obtained similar scores with \emph{simpler baseline model}.)

The additional complexity of the model increases the memory demands. The\-refore we use lower dimensional embeddings, hidden states and smaller batch size to be able to train on the GPUs on the AIC. The hyperparameter settings are further discussed in figure \ref{figure:hyperparameters_copy_low_lr}. Compared to the baseline model we obtained the best results when training with $5$-times lower learning rate.

\begin{figure}[h]
    \scalebox{0.8}{
    \begin{tikzpicture}
    \node(embeddings) [] {
        \small
        \begin{tabular}{ll}
            \toprule
            {} & \textbf{Embedding} \\
            \pulrad{\textbf{Token}} & \textbf{Dimensionality} \\
            \midrule
            \textbf{$record.type$} & 300 \\
            \textbf{$record.home\_away$} & 300 \\
            \textbf{$record.value$} & 500 \\
            \textbf{$record.entity$} & 500 \\
            \textbf{$summary\ token$} & 500
        \end{tabular}
    };
    \node(hidden) [above left=-20.7mm and 5mm of embeddings] {
        \small
        \begin{tabular}{ccc}
            \toprule
            \textbf{Hidden States} & {} & {} \\
            \textbf{Dimensionality} & \pulrad{\textbf{Learning Rate}} & \pulrad{\textbf{Batch Size}} \\
            \midrule
            500 & 0.0002 & 8
        \end{tabular}
    };
    \end{tikzpicture}
    }
    \caption{Hyperparameter settings for joint-copy models.} \label{figure:hyperparameters_copy_low_lr}
\end{figure}

\subsection{Results}

We see the first major improvement in the BLEU score (more than 2 points) as well as in the manually evaluated metrics (almost $6$-times better factual correctness). However the model fails in recognition of the structure of the table.

While it is able to copy facts, many times it copies statistics of wrong players (e.g. it learns that most of the time $5$-th player from the input records is the most important one, therefore it mentions the $5$-th player even if he scored unremarkable amount of points).

We also spotted that model learned that it is more probable that home team wins. Therefore it always produces sentence similar to \emph{"The Denver Nuggets ( 11 - 17 ) defeated the Los Angeles Lakers ( 5 - 23 ) 111 - 107 on friday at the pepsi center in Denver . "} even if the score was reversed (\emph{107 - 111} in favour of the Lakers).

Our observations of the generated summaries support the motivation for the Scheduled Sampling (section \ref{subsection:regularization}). The Copy model often generates a sequence of tokens so different from any in the training set that it gets lost in the generation. We can see numerous problems, e.g \emph{cycling} (model generates one sentence multiple times), or unability to generate the \emph{\textless EOS\textgreater} token which results in exceeding the maximal allowed length of a summary. (We don't allow the model to generate longer sequence than the longest one seen in the dataset (849 tokens)). Both phenomena can be seen in figure \ref{figure:copy_low_lr_generated}.

Contrary to observations on the baseline model neither greedy decoding nor beam search managed to significantly outperform the other method, and regularization methods did not provide any significant advantage over the unregularized case. (Therefore only results of unregularized models are presented.)

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
        {} & \textbf{Validation} & \textbf{Validation} & \textbf{Test} & \textbf{Entity} & \textbf{Factual} \\
        \pulrad{\textbf{Model}} & \textbf{Perplexity} & \textbf{BLEU} & \textbf{BLEU} & \textbf{Recall} & \textbf{Correctness} \\
        \midrule
        Copy$_G$ & {} & 12.48 & 12.6 & 37.35\% & 47.29\% \\
        Copy$_{B5}$ & \pulrad{9.87} & 12.19 & 12.5 & 39.76\% & 47.13\% \\
        \bottomrule
        \multicolumn{6}{l}{\footnotesize{Copy - Joint-Copy model}} \\
        \multicolumn{6}{l}{\footnotesize{$_{G}$ - Greedy Decoding}} \\
        \multicolumn{6}{l}{\footnotesize{$_{B5}$ - Beam search decoding, beam size $=5$}} \\
        \multicolumn{6}{l}{\footnotesize{After observations of the generated summaries we have chosen the Beam Search decoded ones}} \\
        \multicolumn{6}{l}{\footnotesize{for the manual evaluation.}}
    \end{tabular}
    }
    \caption{Performance metrics on Copy models.} \label{table:metrics_copy_low_lr}
\end{table}

\begin{figure}[h]
    \scalebox{0.85}{
    \begin{tikzpicture}
    \node(summary) [rectangle, draw,thick,fill=blue!0,text width=39em, rounded corners, inner sep =8pt, minimum height=1em]{
        \baselineskip=100pt
        \small
        The Toronto Raptors defeated the Philadelphia 76ers , 122 - 95 , at air canada centre on wednesday . The Raptors ( 11 - 14 ) were in the first quarter for the Raptors ( 4 - 14 ) , as the sixers ( 4 - 14 ) were able to pull away in the fourth quarter , out - scoring the 76ers 31 - 26 in the first quarter , while also holding the Raptors ( 4 - 14 ) by a 59 - 38 margin over the second and third quarters . However , the Raptors ( 11 - 14 ) shot 42 percent from the field and 68 percent from the three - point line , while the Raptors went 42 percent from the floor and 68 percent from the free - throw line . The Raptors also out - rebounded the Raptors 42 - 38 , while also holding the Raptors to just 42 percent . The Raptors were led by Joel Embiid , who finished with 24 points ( 7 - 14 fg , 6 - 6 ft ) , 8 assists and 4 rebounds , in 32 minutes . Kyle Lowry was the only other starter to reach double figures , as he finished with 24 points ( 7 - 14 fg , 1 - 6 3pt , 4 - 5 ft ) , 8 assists , 4 rebounds and 1 steal , in 32 minutes . Kyle Lowry was the only other starter to score in double figures , as he finished with 15 points ( 7 - 9 fg , 1 - 2 ft ) , 5 rebounds and 8 assists , in 25 minutes . The other starter to reach double figures in scoring was DeMar DeRozan , who finished with 14 points ( 4 - 13 fg , 0 - 3 3pt , 6 - 6 ft ) , along with 5 rebounds , 4 assists , 1 steal and 1 block , in 31 minutes . The other starter to reach double figures in scoring was DeMar DeRozan , who finished with 14 points ( 4 -
    };
    \end{tikzpicture} }
    \caption{\centering A summary generated by the joint-copy model. The corresponding gold summary and input table is shown in figure \ref{figure:samplesummary}.} \label{figure:copy_low_lr_generated}
\end{figure}

\section{Content Selection Encoder with Joint-Copy Decoder}

In section \ref{subsection:content_selection} we have built the \emph{CopyCS} model. \citet{puduppully2019datatotext} argue that Content Selection encoder contributes to improvement of the quality of the generated summaries. We expect the \emph{CopyCS} model to resolve the problem of mixing statistical information of multiple players.

The hyperparameter configuration remains the same as in the Copy model (figure \ref{figure:hyperparameters_copy_low_lr}).

\subsection{Results}

The summaries produced by the model are richer in structure, as they mention more entities from the input tables ($44.58$ \% vs $39.76$ \% for the Copy model). However we observe that many times model doesn't copy the players name (it only learns to copy numbers). E.g. it generates a sentence \emph{"The Raptors were led by DeMar DeRozan , who finished with a team - high of 26 points ( 9 - 16 fg , 0 - 2 3pt , 8 - 9 ft ) , to go along with 7 rebounds and 5 assists ."} where there would be $8 / 9$ numbers correct if the name wasn't \emph{DeMar DeRozan} but \emph{Marc Gasol}. This causes the model to score less on the factual correctness metric ($42.22$ \% vs $47.13$ \% for the Copy Model).

We hypothesize that it is due to marginalizing out the decision whether to copy or not. While every summary contains different numbers and model gets penalized for hallucinating them, it is often the case that star players are mentioned in each summary about a match where their team participated. Therefore model learns to mention the same players when describing a particular team.

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
        {} & \textbf{Validation} & \textbf{Validation} & \textbf{Test} & \textbf{Entity} & \textbf{Factual} \\
        \pulrad{\textbf{Model}} & \textbf{Perplexity} & \textbf{BLEU} & \textbf{BLEU} & \textbf{Recall} & \textbf{Correctness} \\
        \midrule
        CopyCS$_G$ & {} & 13.53 & 13.62 & -- & -- \\
        CopyCS$_{B5}$ & \pulrad{9.93} & 13.54 & 13.96 & 44.58\% & 42.22\% \\
        \bottomrule
        \multicolumn{4}{l}{\footnotesize{CopyCS - Joint-Copy decoder $+$ Content Selection encoder}} \\
        \multicolumn{4}{l}{\footnotesize{$_{G}$ - Greedy Decoding}} \\
        \multicolumn{4}{l}{\footnotesize{$_{B5}$ - Beam search decoding, beam size $=5$}} \\
    \end{tabular}
    }
    \caption{Performance metrics on the joint-copy model with content selection encoder.} \label{table:metrics_copy_content_selection}
\end{table}

Regarding BLEU score we see an improvement of about $1$ point, however we would like to note that we don't think we came even close to the apex of the performance of this configuration. Many summaries still show indications of undertraining (e.g. sequences like \emph{"TEAM-A defeated TEAM-A"}). We tried Scheduled Sampling, Dropout, different learning rates, different dimensionalities of the hidden layers, but we could not find the sweet spot. Therefore we do not show an example of the summary generated by the model (as it contains sentences of similar quality to the ones generated by the Copy model).

\section{Content Selection and Planning} \label{section:experiments_csap}

The biggest model we train is the \emph{CS\&P} model introduced in section \ref{section:content_selection_and_planning}. Multitude of added architectures (content selection attention, content planning attention, content planning LSTM) increased even more the memory demands. Therefore we again reduced the embedding and hidden state dimensionalities (figure \ref{figure:hyperparameters_content_selection_and_planning}).

\begin{figure}[h]
    \scalebox{0.8}{
    \begin{tikzpicture}
    \node(embeddings) [] {
        \small
        \begin{tabular}{ll}
            \toprule
            {} & \textbf{Embedding} \\
            \pulrad{\textbf{Token}} & \textbf{Dimensionality} \\
            \midrule
            \textbf{$record.type$} & 300 \\
            \textbf{$record.home\_away$} & 300 \\
            \textbf{$record.value$} & 450 \\
            \textbf{$record.entity$} & 450 \\
            \textbf{$summary\ token$} & 450
        \end{tabular}
    };
    \node(hidden) [above left=-20.7mm and 5mm of embeddings] {
        \small
        \begin{tabular}{ccc}
            \toprule
            \textbf{Hidden States} & {} & {} \\
            \textbf{Dimensionality} & \pulrad{\textbf{Learning Rate}} & \pulrad{\textbf{Batch Size}} \\
            \midrule
            450 & 0.0002 & 8
        \end{tabular}
    };
    \end{tikzpicture}
    }
    \caption{Hyperparameter settings for content selection and planning models.} \label{figure:hyperparameters_content_selection_and_planning}
\end{figure}

Content Planning reduces the size of the table, and \emph{arranges} the records into the same order in which they appear in the gold summary. There are two aspects of this solution which should help the model to generate better summaries. Firstly, the input sequences of records become less complex (the maximal length of content plan is $92$ compared to more than $800$ for the original tables). Secondly, the records are ordered in the same way as they should appear in the summary. \citet{puduppully2019datatotext} show that this approach led them to really promising results (e.g. they obtained BLEU score of about $16$ which was an improvement of about $2$ points over their Copy baseline).

\subsection{Results}

During generation we applied two different settings. Firstly we ignore the outputs from the Content Planning Decoder and use the \emph{gold content plan}. (Since we have the gold content plans only for the training and validation parts of the dataset, the reported metrics are calculated only on the validation part) We can see that with gold content plans model mentions more than $70$\% of the entities that also appear in the gold summaries (we believe that the number would increase with better quality of the gold content plans\footnote{\citet{puduppully2019datatotext} extract the content plans with specialized information extraction system, and they claim that "strictly speaking, we cannot talk about gold content plans".}).

Figure \ref{figure:gold_cp_generated} is a good example of the summaries which are generated from the \emph{gold content plans}. Gold summary mentions 13 entities, out of which 12 are mentionned in the generated one. The generated text contains 39 numerical values out of which 31 are correct. The model learned to place the information about the entites in right\footnote{The same order as in the gold summaries.} order, and to copy numbers related to entites. Many flaws still persist. The model generated 3 times the same information about \emph{Richaun Holmes}\footnote{Let me note that there is a numerical value \emph{"$11$ points"} which is counted 3 times. (Therefore $3$ is added to both \emph{all the numerical values} and \emph{correct numerical values}. This causes that reduplicated information has a big effect on the factual correctness metric.)} Another problem demonstrated in the summary is that model mixed up records about Toronto and about Philadelphia (e.g. model generates sentence : \emph{"Defense was key for Toronto , as they held Philadelphia to 55 percent from the field and 68 percent from three - point range ."}, however 55\% shooting for 2pt and 68\% shooting for 3pt are the statistics related to Toronto).

\begin{figure}[h]
    \scalebox{0.85}{
    \begin{tikzpicture}
    \node(summary) [rectangle, draw,thick,fill=blue!0,text width=39em, rounded corners, inner sep =8pt, minimum height=1em]{
        \baselineskip=100pt
        \small
        The Toronto Raptors defeated the Philadelphia 76ers , 122 - 95 , at philips arena on saturday . The Raptors were able to prevail with a win , as they outscored the sixers by 17 in the second quarter to pull out the win . Defense was key for Toronto , as they held Philadelphia to 55 percent from the field and 68 percent from three - point range . The sixers also dominated the rebounding , winning that battle , 44 - 30 . The sixers ( 4 - 14 ) have now lost 5 of their last 6 games , as this marks their fifth win in their last 6 games . Robert Covington was the lone bright spot for the sixers , as he tallied 20 points , 5 rebounds , 2 assists and 2 steals on 7 - of - 11 shooting . Jahlil Okafor was the only other blazer to score double figures , as he totaled 15 points and 5 rebounds . Sergio Rodriguez and Ersan Ilyasova each scored 11 points off the bench . Nik Stauskas was the only other starter in double figures , as he dropped 11 points . Richaun Holmes was the only other starter in double figures , as he dropped 11 points . Richaun Holmes was the only other starter in double figures , as he dropped 11 points . Richaun Holmes was the only other starter in double figures , as he dropped 11 points . Toronto will look to bounce back on sunday in a road matchup against the Orlando Magic . Toronto ( 11 - 6 ) have been dominant all season , but a win like this was their best performance of the season . Kyle Lowry once again carried the load , as he accumulated 24 points , 4 rebounds and 8 assists . Terrence Ross was second on the team , as he dropped 22 points on 8 - of - 11 shooting . DeMar DeRozan was the only other starter in double figures , as he accumulated 14 points , 5 rebounds and 5 assists . Jonas Valanciunas recorded a double - double , totaling 12 points and 11 rebounds .
    };
    \end{tikzpicture} }
    \caption{\centering A summary generated by the Content Selection and Planning model from the gold content plans. The corresponding gold summary and input table is shown in figure \ref{figure:samplesummary}.} \label{figure:gold_cp_generated}
\end{figure}

The other generation setting is to use the generated content plans. However this is where we failed to obtain the same results as \citet{puduppully2019datatotext}. We observed that the network fails to learn when to place \emph{\textless EOS\textgreater} token in the content plan and the generated content plans end with a sequence of repetitions. Therefore the part of the model responsible for text generation (Content Plan Encoder and Text Decoder) fails to produce reasonable texts as it generates from content plans with characteristics different to anything seen during training. Table \ref{table:metrics_csap} shows that model still achieved reasonable entity recall (in fact better than any previous model) and the best correctness, although the correctness is due to large number of repetitions of the accurate information.

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
        {} & \textbf{Validation} & \textbf{Validation} & \textbf{Test} & \textbf{Entity} & \textbf{Factual} \\
        \pulrad{\textbf{Model}} & \textbf{Perplexity$_1$} & \textbf{BLEU} & \textbf{BLEU} & \textbf{Recall} & \textbf{Correctness} \\
        \midrule
        CS\&P$_G$ & {} & 13.07 & 13.08 & 44.58\% & 67.76\% \\
        CS\&P$_{G}^{*}$ & \pulrad{8.76} & 22.8$^{*}$ & -- & 71.43\%$^{*}$ & 54.69\%$^{*}$ \\
        \bottomrule
        \multicolumn{6}{l}{\footnotesize{CS\&P - Content Selection and Planning model}} \\
        \multicolumn{6}{l}{\footnotesize{$_{G}$ - Greedy Decoding}} \\
        \multicolumn{6}{l}{\footnotesize{$_1$ - Validation Perplexity of text decoded from gold content plans}} \\
        \multicolumn{6}{l}{\footnotesize{$^{*}$ - All the metrics are computed only on the validation part of the dataset, with \emph{gold content plans}}}
    \end{tabular}
    }
    \caption{Performance metrics on the Content Selection and Planning model.} \label{table:metrics_csap}
\end{table}

Since we do not implement the Beam Search decoder for the content plans (due to shortness of time) we try to artificially add \emph{\textless EOS\textgreater} token at some position in the generated content plan and mask everything after, however the results do not show any improvement. The same applies for using Beam Search in Text Decoder. (Therefore the results presented in table \ref{table:metrics_csap} are calculated on greedy decoded summaries generated from unmasked greedy generated content plans.)


\section{Ordered Tables}

Since we haven't managed to obtain satisfactory results using any of the previously proposed methods we try to make the task easier. Looking at the results of the CS\&P model generating from the \emph{gold content plans}, we may draw a conclusion that the performance bottleneck is in the understanding of the tabular structure.

Going through the gold content plans as well as the ones generated by Content Selection and Planning model we spotted that their structure follows a simple pattern. At first the teams are presented, then there is some information about the best players and about players who performed surprisingly well.

We order the input records as described in section \ref{subsubsection:ordered_records} and train all of our models on the ordered sequences.

\subsection{Results}

We present only the results of the Copy model (at the time of writing the CopyCS model is on the verge of outperforming the Copy model according to validation perplexity, however we do not include its results due to shortness of time) on the ordered tables. In the following text we call the Copy model trained on the ordered tables \emph{Copy$_{ordered}$}.

We cannot make any clear conclusions. We see a raise in terms of the BLEU score (of about $2$ points compared to Copy model) but also a drop in both manually evaluated metrics. The overal performance of \emph{Copy$_{ordered}$} is comparable to CopyCS model and it is really unfortunate that we did not manage to train \emph{CopyCS$_{ordered}$}\footnote{\emph{CopyCS$_{ordered}$} is the \emph{CopyCS} model trained on the ordered tables} in time.

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
        {} & \textbf{Validation} & \textbf{Validation} & \textbf{Test} & \textbf{Entity} & \textbf{Factual} \\
        \pulrad{\textbf{Model}} & \textbf{Perplexity} & \textbf{BLEU} & \textbf{BLEU} & \textbf{Recall} & \textbf{Correctness} \\
        \midrule
        Copy$_{orderedG}$ & {} & 12.98 & 13.32 & -- & -- \\
        Copy$_{orderedB5}$ & \pulrad{9.21} & 13.75 & 14.01 & 33.73\% & 46.84\% \\
        \bottomrule
        \multicolumn{6}{l}{\footnotesize{Copy$_{ordered}$ - Joint-Copy model operating on the ordered tables}} \\
        \multicolumn{6}{l}{\footnotesize{$_{G}$ - Greedy Decoding}} \\
        \multicolumn{6}{l}{\footnotesize{$_{B5}$ - Beam search decoding, beam size $ = 5 $}} \\
    \end{tabular}
    }
    \caption{Performance metrics on the Content Selection and Planning model.} \label{table:metrics_copy_ordered}
\end{table}

\section{Shortened Tables} \label{section:experiments_shortened_tables}

Our second effort to simplify the input tables is described in section \ref{subsubsection:shortened records}. Basically we order the records and remove the unsignificant ones to obtain \emph{a sequence of records similar to a content planned one}. We opted to train the \emph{CopyCSBidir} model (discussed in section \ref{subsection:content_planning}) as it is similar in architecture to \emph{CS\&P} model, which achieved promising results on the gold content plans. The hyperparameter configuration is shown in figure \ref{figure:hyperparameters_copy_prunned}.

\begin{figure}[h]
    \scalebox{0.8}{
    \begin{tikzpicture}
    \node(embeddings) [] {
        \small
        \begin{tabular}{ll}
            \toprule
            {} & \textbf{Embedding} \\
            \pulrad{\textbf{Token}} & \textbf{Dimensionality} \\
            \midrule
            \textbf{$record.type$} & 300 \\
            \textbf{$record.home\_away$} & 300 \\
            \textbf{$record.value$} & 500 \\
            \textbf{$record.entity$} & 500 \\
            \textbf{$summary\ token$} & 500
        \end{tabular}
    };
    \node(hidden) [above left=-20.7mm and 5mm of embeddings] {
        \small
        \begin{tabular}{ccc}
            \toprule
            \textbf{Hidden States} & {} & {} \\
            \textbf{Dimensionality} & \pulrad{\textbf{Learning Rate}} & \pulrad{\textbf{Batch Size}} \\
            \midrule
            500 & 0.0001 & 8
        \end{tabular}
    };
    \end{tikzpicture}
    }
    \caption{Hyperparameter settings for \emph{CopyCSBidir}} \label{figure:hyperparameters_copy_prunned}
\end{figure}

\subsection{Results}

This configuration (more sophisticated model, input table that is easier to understand) achieved the best BLEU score of all the models (excluding the one generating from the gold content plans). Table \ref{table:metrics_prunned} shows that we managed to improve the BLEU score by more than $1$ point compared to both CopyCS and Copy$_{ordered}$ models. Model also learned to mention more players, however not with correct statistics. Since we reduced the amount of input records related to all but the best three players, the model learned to \emph{hallucinate} the statements about the remainng ones. We haven't experimented with other possible combinations but we expect that the model performance would improve when about five best players would be fully described in the input tables (compared to actual three).

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
        {} & \textbf{Validation} & \textbf{Validation} & \textbf{Test} & \textbf{Entity} & \textbf{Factual} \\
        \pulrad{\textbf{Model}} & \textbf{Perplexity} & \textbf{BLEU} & \textbf{BLEU} & \textbf{Recall} & \textbf{Correctness} \\
        \midrule
        CopyCSBidir$_{G}$ & {} & 14.31 & 14.45 & -- & -- \\
        CopyCSBidir$_{B5}$ & \pulrad{11.24} & 15.18 & 15.63 & 50.6\% & 49.87\% \\
        \bottomrule
        \multicolumn{6}{l}{\footnotesize{CopyCSBidir - model introduced in section \ref{subsection:content_planning} trained on shortened sequences of input records.}} \\
        \multicolumn{6}{l}{\footnotesize{$_{G}$ - Greedy Decoding}} \\
        \multicolumn{6}{l}{\footnotesize{$_{B5}$ - Beam search decoding, beam size $ = 5 $}} \\
    \end{tabular}
    }
    \caption{Performance metrics on the Content Selection and Planning model.} \label{table:metrics_prunned}
\end{table}

This is our final experiment, using the most advanced preprocessing as well as the most advanced model architecture. Therefore I would like to show how far is our best model from the ideal generation system described in the introduction. The generated text in figure \ref{figure:copy_prunned_generated} starts by introducing the teams that have played as well as the score and the location where the match was played. Next it contains some "interesting" events that happened, followed by statistics of the players. Many generated summaries (although not this one) end with a phrase about future schedule of both teams. Since \emph{Jahlil Okafor} isn't between the best three players of the match, \emph{all his statistics excluding his points and assists are hallucinated}. Model mixed up \emph{Robert Covington} with \emph{Terrence Ross}, thus the presented statistics are not correct. We think that further hyperparameter tuning (adjusting the length of the sequence of input records, advanced learning rate strategies) may lead to improvements in all the metrics we measured.

\begin{figure}[h]
    \scalebox{0.85}{
    \begin{tikzpicture}
    \node(summary) [rectangle, draw,thick,fill=blue!0,text width=39em, rounded corners, inner sep =8pt, minimum height=1em]{
        \baselineskip=100pt
        \small
        The Toronto Raptors defeated the Philadelphia 76ers , 122 - 95 , at wells fargo center on saturday evening . The Raptors ( 11 - 6 ) came away with a 33 - point second quarter , where the Raptors ( 11 - 14 ) opened up a 12 - point lead by the end of the first quarter . The game remained close in the third quarter , as the Raptors outscored the sixers , 31 - 24 . Toronto was led by Terrence Ross , who poured in 22 points on 7 - of - 11 shooting , to go along with 8 assists and 4 rebounds , in 32 minutes . Robert Covington followed up with 22 points on 8 - of - 11 shooting , including 3 - of - 5 from long range , in 23 minutes off the bench . Jonas Valanciunas was solid with 12 points and 5 assists , while Terrence Ross chipped in 22 points on 8 - of - 11 shooting , including 3 - of - 5 from long range , in 23 minutes off the bench . The Raptors shot 55 percent from the field , while the 76ers shot just 42 percent from the field and 41 percent from long range . Jahlil Okafor was the high - point man for Philadelphia , with 24 points on 7 - of - 9 shooting , to go along with 8 assists and 4 rebounds , in 32 minutes . Jahlil Okafor followed up with 15 points , 5 rebounds , 5 assists and 2 steals , in 32 minutes .
    };
    \end{tikzpicture} }
    \caption{\centering A summary generated by the Copy$_{prunned}$ model. The corresponding gold summary and input table is shown in figure \ref{figure:samplesummary}.} \label{figure:copy_prunned_generated}
\end{figure}


\section{Overall Comparison}

We start with the Baseline model (section \ref{section:baseline_model}) which produces fairly good language but many hallucinated statements (only 8\% of the produced statements are correct).

Next we experiment with the Copy model (section \ref{section:copy_mechanism_intro}). There we see the number of correct statements to increase to more than 40\% and the BLEU score to rise by more than 2 points.

We move on to the \emph{CopyCS} model ( section \ref{subsection:content_selection}). We see another improvement in the BLEU score (by 1 point) and the entity recall (by 5\%) however the factual correctness decreases and the produced language still contains many contradictions.

The CS\&P model (section \ref{section:content_selection_and_planning}) is the last one we use for the task. We see that the model achieves great performance on reduced and reorganized tables (the BLEU score bigger than 22) although the part of the model creating the reduced tables does not cooperate well with the part generating text, therefore we do not see any improvement over the \emph{CopyCS} model when generating \emph{end-to-end}.

Our last experiments aim to make the sequence of input records more organized and shorter. We train the \emph{CopyCSBidir} model on the simplified input sequences, and manage to obtain the best BLEU score (increase by more than $1$ point over \emph{CopyCS} trained on unordered tables) and the best score on all manually evaluated metrics.

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{lccccc}
        \toprule
        {} & \textbf{Validation} & \textbf{Validation} & \textbf{Test} & \textbf{Entity} & \textbf{Factual} \\
        \pulrad{\textbf{Model}} & \textbf{Perplexity} & \textbf{BLEU} & \textbf{BLEU} & \textbf{Recall} & \textbf{Correctness} \\
        \midrule
        BR$_{B5}$ & 10.59 & 9.99 & 10.6 & 37.35\% & 8.03\% \\
        Copy$_{B5}$ & 9.87 & 12.19 & 12.5 & 39.76\% & 47.13\% \\
        CopyCS$_{B5}$ & 9.93 & 13.54 & 13.96 & 44.58\% & 42.22\% \\
        CS\&P$_G$ & 8.76$^{*}$ & 13.07 & 13.08 & 44.58\% & 67.76\% \\
        Copy$_{orderedB5}$ & 9.21 & 13.75 & 14.01 & 33.73\% & 46.84\% \\
        CopyCSBidir${B5}$ & 11.24 & 15.18 & 15.63 & 50.6\% & 49.87\% \\
        \bottomrule
        \multicolumn{6}{l}{\footnotesize{BR - Baseline, regularized}} \\
        \multicolumn{6}{l}{\footnotesize{Copy - Joint-Copy model}} \\
        \multicolumn{6}{l}{\footnotesize{CopyCS - Joint-Copy decoder $+$ Content Selection encoder}} \\
        \multicolumn{6}{l}{\footnotesize{CS\&P - Content Selection and Planning model}} \\
        \multicolumn{6}{l}{\footnotesize{Copy$_{ordered}$ - Joint-Copy model operating on the ordered tables}} \\
        \multicolumn{6}{l}{\footnotesize{CopyCSBidir - the model introduced in section \ref{subsection:content_planning}}} \\
        \multicolumn{6}{l}{\footnotesize{$_{G}$ - Greedy Decoding}} \\
        \multicolumn{6}{l}{\footnotesize{$_{B5}$ - Beam search decoding, beam size $ = 5 $}} \\
        \multicolumn{6}{l}{\footnotesize{$^{*}$ - Validation Perplexity of text decoded from gold content plans}} \\
    \end{tabular}
    }
    \caption{Performance metrics on all the models and approaches discussed in this chapter.} \label{table:metrics_all}
\end{table}