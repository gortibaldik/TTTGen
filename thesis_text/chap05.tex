\chapter{Models}
In this chapter I aim to clarify how I set up the neural network models, the training and inference. Despite both datasets (WikiBIO, RotoWire) being created for modeling the same task (generation of text from structured data), I believe their characteristics are really different (as shown in chapter about preprocessing \ref{chapPreproc}). Therefore I present different set of models used for each dataset. At first I present a \emph{baseline model}. Model, which is sufficiently competitive to be able to generate reasonable texts. Then I present changes to the architecture, which aim to improve the generation.

\section{Rotowire}

It was really challenging to set up even the baseline model. Since the input sequences are about 600 records long and output sequences are more than 300 tokens in average, it wasn't possible to fit the model into the GPU memory. (The GPUs at \url{https://aic.ufal.mff.cuni.cz} have about 8GBs of memory\footnote{The exact GPU used on AIC cluster is NVIDIA GeForce GTX 1080})

\subsection{Truncated Backpropagation Through Time}

Computing and updating gradient for each time-step of a sequence of 800 tokens has approximately the same cost as forward and backward pass in a feed-forward network that has 800 layers. \citep{williamsTBPTT} proposes a less expensive method. Truncated Backpropagation Through Time (TBPTT) processes one time-step at a time, and every $t_1$ timesteps it runs BPTT for $t_2$ timesteps.

My implementation of the algorithm cuts the original sequence of size $N$ to $( N - t_2 ) / t_1 + 1$ chunks (with possibly one more chunk if $(N - t_2)$ isn't divisible by $t_1$). Then I run \emph{"normal"} BPTT on the first chunk, keeping the hidden state of the network after $t_1$ steps. Then I fed in the hidden state as the initial state at the start of processing second chunk. Similarly afterwards.

\subsection{Content Selection}

\emph{TBPTT} deals with output sequences. However a problem of too long input sequences still persists. Unfortunately no simple solution exists. Therefore instead of processing the input records with a recurrent cell, I try to create the best possible representation of each input record.