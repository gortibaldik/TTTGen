\chapter{Experimental Setup} \label{model_chapter}
In this chapter I aim to clarify how I set up the neural network models for the experiments (which will be discussed in the next chapter \ref{experiments_chapter}). At first I present a \emph{baseline model}. Model, which is sufficiently competitive to be able to generate reasonable texts. Then I present changes to the architecture, which aim to improve the generation.

I follow the path set up by \citep{wiseman2017}. I begin with purely end-to-end approach (Encoder-Decoder with attention).Next I try to improve it with the copy mechanism \ref{copy_mech_sec}. Next I choose to divide the task to \emph{content planning} and \emph{text generation} as proposed by \citep{puduppully2019datatotext}.

\section{Truncated Backpropagation Through Time} \label{truncated_backprop_subsubsection}

It was really challenging to set up even the baseline model. Since the input sequences are about 600 records long and output sequences are more than 300 tokens in average (and the outputs are padded with \emph{\textless PAD\textgreater} token to approximately 800 tokens), it wasn't possible to fit the model into the GPU memory. (The GPUs at \url{https://aic.ufal.mff.cuni.cz} have about 8GBs of memory\footnote{The exact GPU used on AIC cluster is NVIDIA GeForce GTX 1080})

Computing and updating gradient for each time-step of a sequence of 800 tokens has approximately the same cost as forward and backward pass in a feed-forward network that has 800 layers. \citep{williamsTBPTT} propose a less expensive method. Truncated Backpropagation Through Time (TBPTT) processes one time-step at a time, and every $t_1$ timesteps it runs BPTT for $t_2$ timesteps.

To illustrate my implementation of the algorithm, I show how I process an output sequence which is longer than 150 tokens. Let $t_1 = 50$, $t_2 = 100$. At first I let the network predict the first $100$ outputs and run the BPTT. I keep aside the $50$-th hidden state of the decoder. Next the network (initialized with the hidden state from the $50$-th time-step) predicts positions $51$ to $150$, and again the BPTT is run. Similarly afterwards.

\section{Baseline model} \label{section:baseline_model}

I use a non-recurrent Encoder and a two-layer LSTM decoder with Luong-style attention as the baseline model. At first I present how the encoding of input records works, then I talk about the text generation. I highlight all the differences between my approach and the one taken by \citep{wiseman2017}. The visualization of the model is shown in the figure \ref{rotowire_baseline_vis}.

\subsection{Encoder}

The Encoder should process the input records (the formation of a record is explained in section \ref{table_to_record_trans}) to create the partial outputs at each time-step and the initial hidden state for the Decoder.

I described in section \ref{trans_in_tb_rw} that each record $r$ consists of 4 different features: \emph{type}, \emph{value}, \emph{entity} and a \emph{home/away flag} depicting if the record belongs to home or away team.

At first each feature is embedded to a fixed-dimensional space. Next the embeddings are concatenated. I choose the same approach as \citep{wiseman2017} (who was inspired by \citep{yang2016referenceaware}), and I pass the concatenation through a one layer feed-forward network (\emph{tf.keras.layers.Dense}) with ReLU activation\footnote{the Rectified Linear Unit activation function $f(x) = max(0, x)$}, to create the encoding $e$ of the record $r$. Consequently, the input sequence of records $\{r_i\}_{i=1}^m$ is transformed to $\mathbf{e} = \{ e_i \}_{i=1}^m$ .

To create the initial state of the decoder, \citep{wiseman2017} calculated the mean of the records belonging to the particular entity and linearly transformed the concatenation of the means. (Simply, the concatenation is passed through \emph{tf.keras.la\-yers.Dense} without any activation function).

To make the implementation simpler I observe that each player entity is represented by 22 records and each team entity by 15 records \ref{trans_in_tb_rw}.  Consequently I approximate the approach taken by \citep{wiseman2017} and mean pool over $\mathbf{e}$ with stride 22. (Which means that while means of players are exact, the means of team records are less than approximated) During my experiments I haven't seen any indication that this modification became the performance bottleneck of the model.

\subsection{Decoder}

The Decoder is a 2-layer LSTM network with attention mechanism. The LSTMs are initialized with states prepared in the previous section. I opted to use the Luong style attention with input feeding \citep{luong2015effective}. I described in section \ref{score_input_feeding} that we use the concatenation of the last attentional decision and the last gold output as the actual input to the first layer of LSTMs. However at the first time-step, when the input is the \emph{\textless BOS\textgreater} token, there is no \emph{last attentional decision}. Hence for this purpose I use one of the initial states prepared by the Encoder.

\subsection{Training and Inference}

The model is trained end-to-end to minimize the cross-entropy loss (negative-log likelihood) of the gold output summary $\mathbf{y}$ given the sequence of input records $\mathbf{s}$.
\begin{equation}
    \min \sum_{(\mathbf{s}, \mathbf{y}) \in \mathcal{D} }{-log\ p(\mathbf{y}|\mathbf{s})}
\end{equation}

We use the \emph{teacher forcing} (as explained in section \ref{subsection:high_level_encoder_decoder}) during training and during the inference, we approximate the most probable output sequence (either greedily or by using beam-search, explained in chapter \ref{experiments_chapter}).
\begin{equation}
    \mathbf{\hat{y}} = \argmax_{\mathbf{y'}} p(\mathbf{y'} | \mathbf{s})
\end{equation}

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.195]{img/rotowire_baseline.jpg}
    \footnotesize{\\ \textit{Note:} green cells are embedding layers, orange cells are feed-forward networks}
    \caption{\centering The Rotowire baseline model at the second time-step.} \label{rotowire_baseline_vis}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.24]{img/rotowire_joint_copy.jpg}
    \footnotesize{\\ \textit{Note:} green cells are embedding layers, orange cells are feed-forward networks \\ $\alpha$ is the alignment vector produced by the copy attention; $r_i.value$ is the value portion of the $i$-th input record.}
    \caption{\centering The Joint-Copy extension of the baseline model at second time-step.} \label{rotowire_joint_copy_vis}
\end{figure}

\section{Improving the Baseline with Copy Mechanisms} \label{section:copy_mechanism_intro}

In the previous chapter \ref{rare_word_problem}, I underlined \emph{hallucinations} as one of the problems of the Encoder-Decoder architecture. In the task on RotoWire dataset it basically means that during training, the model learns that it should put \emph{some number} at a specific place in the sentence. We would like it to generate the \emph{exact value}.

Therefore as the next step I incorporate the \emph{Joint Copy mechanism} \citep{gu2016incorporating}, \citep{yang2016referenceaware} (already described in \ref{copy_mech_sec}). The encoder rests intact, however in the decoder I use another attention module to point to specific record from the input table and a feed-forward network which models if the model should generate or copy.

The generational path is the same as in the baseline model. In the copy path I use the alignment vector from the newly added attention as weights, to compute the weighted sum of the value portion of input records. The visualization can be seen in the figure \ref{rotowire_joint_copy_vis}.

\section{Content Selection and Planning}

Rather than training end-to-end, \citep{puduppully2019datatotext} suggest to divide the task to \emph{content planning} and \emph{text generation}. The \emph{content plan} describes "what to say, and in what order". During the \emph{content planning} stage the model selects some records from the input table and organizes them to a \emph{content plan}. During the \emph{text generation} stage, the model generates the text based on the records pointed to by the \emph{content plan}.

Both tasks are modelled with \emph{the same} Encoder and with \emph{separate} Decoders.

\subsection{Content Selection}

\citep{puduppully2019datatotext} improves the baseline encoder by incorporating \emph{context awareness}. At first the input records are encoded in the same way as in the baseline model \ref{section:baseline_model}, the self-attention is used to model the "importance vis-a-vis other records in the table".

Specifically, we compute
\begin{align*}
\forall k \neq t : \alpha_{t, k} &= score(\hat{r}_t, \hat{r}_k)                         &&\mbox{ \small \emph{the score vector}}\\
\boldsymbol{\beta_t}             &= softmax(\boldsymbol{\alpha_t})                      &&\mbox{ \small \emph{the alignment vector}}\\
\boldsymbol{\gamma_t}            &= \sum_{i=1}^m{\beta_{t, i} * \hat{r}_i}               &&\mbox{ \small \emph{the context vector}}\\
\hat{r}_t^{att}                  &= sigmoid( W_{cs} [\hat{r}_t, \boldsymbol{\gamma_t}] ) &&\mbox{ \small \emph{gating mechanism}}\\
\hat{r}_t^{cs}                   &= \hat{r}_t^{att} \bigodot \hat{r}_t                  &&\mbox{ \small \emph{the content selected representation}}
\end{align*}

The Encoder thus creates a sequence of \emph{context aware} representations $\{\hat{r}_t^{cs}\}_{t=1}^m$ (figure \ref{content_selection_pudupully}).

\begin{figure}[!h]
    \includegraphics{img/cs_gate-cropped.pdf}
    \caption{Content Selection mechanism (image is directly from \citep{puduppully2019datatotext})} \label{content_selection_pudupully}
\end{figure}

\subsection{Content Planning} \label{content_planning_subsubsection}

\citep{wiseman2017} experimented with conditional copy approach, in which the latent \emph{switch} probability isn't marginalized out. Hence, there exists pointer sequence for each summary in the train and validation dataset. The sequence corresponds to the order in which entities and values from the sequence of input records appear in the output summary. \citep{puduppully2019datatotext} suggested that instead of just modelling the switch probability, we can train a Decoder to extract these pointers from the original table.

As suggested, the Content Plan Decoder is a one layer LSTM which operates on the \emph{context aware} representations $\hat{\mathbf{r}}^{cs}$. Its hidden states are initialized with $avg(\{\hat{r}_t^{cs}\}_{t=1}^m)$. \citep{puduppully2019datatotext} haven't elaborated on the exact approach, hence it is probable that I have diverged a little bit from the intentions of original authors.

The Text Decoder is trained to start the generation when it sees the \emph{\textless BOS\textgreater} token. Since the Content Plan Decoder operates on $\hat{\mathbf{r}}^{cs}$ I've choosen to prepend a special \emph{\textless BOS\textgreater} record to the sequence of input records, and also a pointer to the \emph{\textless BOS\textgreater} record is prepended to each content plan. Therefore instead of teaching the Content Plan Decoder to start generating content plan when seeing a special value, I teach it to do so when seeing \emph{the encoded representation of the special value}. The same approach is taken at the end, with the \emph{\textless EOS\textgreater} record.

\begin{table}
    \centering \small
    \begin{tabular}{llll}
        \toprule
        \textbf{Type} & \textbf{Entity} & \textbf{Value} & \textbf{H/A flag} \\                  
        \midrule
        \textless{}\textless{}BOS\textgreater{}\textgreater{} & \textless{}\textless{}BOS\textgreater{}\textgreater{} & \textless{}\textless{}BOS\textgreater{}\textgreater{} & \textless{}\textless{}BOS\textgreater{}\textgreater{} \\
        TEAM-CITY                                             & Raptors                                               & Toronto                                               & HOME                                                  \\
        TEAM-NAME                                             & Raptors                                               & Raptors                                               & HOME                                                  \\
        TEAM-PTS                                              & Raptors                                               & 122                                                   & HOME                                                  \\
        TEAM-CITY                                             & 76ers                                                 & Philadelphia                                          & AWAY                                                  \\
        TEAM-NAME                                             & 76ers                                                 & 76ers                                                 & AWAY                                                  \\
        TEAM-NAME                                             & 76ers                                                 & 76ers                                                 & AWAY                                                  \\
        TEAM-PTS                                              & 76ers                                                 & 95                                                    & AWAY                                                  \\
        $\dots$                                               & $\dots$                                               & $\dots$                                               & $\dots$ \\                                        
        \bottomrule
        \multicolumn{4}{c}{\footnotesize \textit{Note:} The extract corresponds to sentence: "The host Toronto} \\
        \multicolumn{4}{c}{\footnotesize Raptors defeated the Philadelphia 76ers , 122 - 95\dots "}
    \end{tabular}
    \caption{An extract from the content plan corresponding to the summary from the figure \ref{figure:samplesummary}}
\end{table}

Either baseline Decoder or Joint-Copy Decoder can operate on the generated content plan, to generate the output summary.

\subsection{Training and Inference}

As oposed to Baseline and Joint-Copy models, the Content Planning model has multiple outputs. Therefore we train the model to minimize the joint negative log-likelihood of the gold summary $\mathbf{y}$ and the gold content plan $\mathbf{z}$ conditioned on the sequence of input records $\mathbf{s}$. It deserves a note that we use the gold content plans as the inputs to the text generation part of the model.
\begin{equation}
    \argmin_{\mathbf{y}, \mathbf{z}}{p(\mathbf{y}|\mathbf{z}, \mathbf{s}) + p(\mathbf{z} | \mathbf{s})}
\end{equation}
