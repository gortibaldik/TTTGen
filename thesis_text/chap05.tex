\chapter{Experimental Setup} \label{chapter:experimental_setup}
In this chapter we aim to clarify how we set up the neural network models for the experiments (which will be discussed in the next chapter \ref{chapter:experiments}). At first we present a \emph{baseline model}. Model, which is sufficiently competitive to be able to generate reasonable texts. Then we present changes to the architecture, which aim to improve the generation.

We follow the path set up by \citet{wiseman2017}. We begin with purely end-to-end approach (Encoder-Decoder with attention). Next we try to improve the Decoder with the copy mechanism \ref{section:copy_mechanism}. Inspired by \citet{puduppully2019datatotext}, we continue by enhancing the Encoder with Content Selection mechanism, and dividing the task to \emph{content planning} and \emph{text generation}.

\section{Truncated Backpropagation Through Time} \label{section:truncated_backpropagation}

It was really challenging to set up even the baseline model. Since the input sequences are about 600 records long and output sequences are more than 300 tokens in average (and the outputs are padded with \emph{\textless PAD\textgreater} token to approximately 800 tokens), it wasn't possible to fit the model into the GPU memory. (The GPUs at \url{https://aic.ufal.mff.cuni.cz} have about 8GBs of memory\footnote{The exact GPU used on AIC cluster is NVIDIA GeForce GTX 1080})

Computing and updating gradient for each time-step of a sequence of 800 tokens has approximately the same cost as forward and backward pass in a feed-forward network that has 800 layers. \citet{williamsTBPTT} propose a less expensive method. Truncated Backpropagation Through Time (TBPTT) processes one time-step at a time, and every $t_1$ timesteps it runs BPTT for $t_2$ timesteps.

To illustrate our implementation of the algorithm, we show how we process an output sequence which is longer than 150 tokens. Let $t_1 = 50$, $t_2 = 100$. At first we let the network predict the first $100$ outputs and run the BPTT. We keep aside the $50$-th hidden state of the decoder. Next the network (initialized with the hidden state from the $50$-th time-step) predicts positions $51$ to $150$, and again the BPTT is run. Similarly afterwards.

\section{Baseline model} \label{section:baseline_model}

We use a non-recurrent Encoder and a two-layer LSTM decoder with Luong-style attention as the baseline model. At first we present how the encoding of input records works, then we discuss the text generation. We highlight all the differences between our approach and the one taken by \citet{wiseman2017}. The visualization of the model is shown in the figure \ref{figure:basline_model_visualization}.

\subsection{Encoder} \label{subsection:baseline_model_encoder}

The Encoder should process the input records (the formation of a record is explained in section \ref{subsection:table_text_transformations}) to create the partial outputs at each time-step and the initial hidden state for the Decoder.

We described in section \ref{subsection:record_format} that each record $r$ consists of 4 different features: \emph{type}, \emph{value}, \emph{entity} and a \emph{home/away flag} depicting if the record belongs to home or away team.

First, each feature is embedded to a fixed-dimensional space. Next, the embeddings are concatenated. We choose the same approach as \citet{wiseman2017} (who was inspired by \citet{yang2016referenceaware}), and we pass the concatenation through a one layer feed-forward network (\emph{tf.keras.layers.Dense}) with ReLU activation\footnote{the Rectified Linear Unit activation function $f(x) = max(0, x)$}, to create the encoding $e$ of the record $r$. Consequently, the input sequence of records $\{r_i\}_{i=1}^m$ is transformed to $\boldsymbol{e} = \{ e_i \}_{i=1}^m$ .

To create the initial state of the decoder, \citet{wiseman2017} calculated the mean of the records belonging to the particular entity and linearly transformed the concatenation of the means. (the concatenation is passed through \emph{tf.keras.la\-yers.Dense} without any activation function).

To make the implementation simpler we observe that each player entity is represented by 22 records and each team entity by 15 records \ref{subsection:table_text_transformations}.  Consequently we approximate the approach taken by \citet{wiseman2017} and mean pool over $\boldsymbol{e}$ with stride 22. During our experiments we haven't seen any indication that this modification became the performance bottleneck of the model.

\subsection{Decoder}

The Decoder is a 2-layer LSTM network with attention mechanism. The LSTMs are initialized with states prepared in the previous section. We opted to use the Luong style attention with input feeding \citep{luong2015effective}. We described in section \ref{subsection:score_input_feeding} that we use the concatenation of the last attentional decision and the last gold output as the actual input to the first layer of LSTMs. However at the first time-step, when the input is the \emph{\textless BOS\textgreater} token, there is no \emph{last attentional decision}. Hence for this purpose we use one of the initial states prepared by the Encoder.

\subsection{Training}

Given an input table $\boldsymbol{x}$ and the corresponding gold output summary $\boldsymbol{y}$ the model approximates the conditional probability of the latter conditioned on the former $p(\boldsymbol{y} | \boldsymbol{x})$. Following the maximum likelihood principle (e.g. section 5.5 in \citep{Goodfellow-et-al-2016}) the model is trained by minimizing the cross-entropy loss (negative log likelihood) on the training set $\mathcal{D}$.(equation \ref{equation:negative_log_likelihood}).

\begin{equation} \label{equation:negative_log_likelihood}
    - \sum_{(\boldsymbol{x}, \boldsymbol{y}) \in \mathcal{D}}{log\ p_{model}(\boldsymbol{y} | \boldsymbol{x})}
\end{equation}

As explained in section \ref{subsection:high_level_encoder_decoder} the training happens under \emph{teacher-forcing} setting. The minimization is provided by stochastic gradient descent, specifically we opted to use one of the standard algorithms, Adam \citep{kingma2014adam}. Since this algorithm associates specific learning rate to each of the trainable network parameters, we modify only the initial learning rate parameter\footnote{It means that we do not try e.g. learning rate decay.}.

We report the loss value as well as the accuracy\footnote{the frequency with which the most probable token in the distribution generated by the model matches the gold token} of the model on the training set, to be able to detect \emph{underfitting} (this happens when a model is unable to achieve sufficiently low loss value on the training part of the dataset).

The main challenge is to obtain good results on previously unseen data. Therefore we report the loss value, and the accuracy of the model on the validation part of the dataset (also collected under \emph{teacher-forcing} setting). We consider the model with the lowest loss value and accuracy on the validation set to be the best\footnote{We also report the BLEU score \citep{papineni2002} of the text generated from a subset of the validation dataset. Frequently, there are multiple epochs where the model achieved similar performance. We pick the one which generated the best text according to BLEU.}.

\subsection{Regularization} \label{subsection:regularization}

Two regularization methods are used, Dropout and Scheduled Sampling.

"The key idea of Dropout is to randomly drop units (along with their connections) from the neural network during training." \citep{srivastavaDropout2014}. A unit is dropped with probability $p$ that can be set as a hyperparameter. (e.g. $p = 0$ would mean no dropout)  We apply the dropout on the outputs of the LSTM cells.

The Scheduled Sampling aims to minimize the difference between training and inference. \citet{bengio2015scheduled} note that using any of the two generation techniques described in sections \ref{subsubsection:greedy_decoding}, \ref{subsubsection:beam_search_decoding} the model can get to the "state space that is very different from those visited from the training distribution and for which it doesnâ€™t know what to do". They propose to "bridge the gap" by feeding either gold $y_{t-1}$ or the prediction of the model $\hat{y}_{t-1}$ as the input at $t$-th time-step. The decision whether to use gold is made independently for each input with probability $p'$ which is another hyperparameter of the training. They propose to decay $p'$ over time with similar strategies as used with learning rate.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.195]{img/rotowire_baseline.jpg}
    \footnotesize{\\ \textit{Note:} green cells are embedding layers, orange cells are feed-forward networks}
    \caption{\centering The Rotowire baseline model at the second time-step.} \label{figure:basline_model_visualization}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.24]{img/rotowire_joint_copy.jpg}
    \footnotesize{\\ \textit{Note:} green cells are embedding layers, orange cells are feed-forward networks \\ $\alpha$ is the alignment vector produced by the copy attention; $r_i.value$ is the value portion of the $i$-th input record.}
    \caption{\centering The Joint-Copy extension of the baseline model at second time-step.} \label{rotowire_joint_copy_vis}
\end{figure}

\section{Improving the Baseline with Copy Mechanisms} \label{section:copy_mechanism_intro}

In section \ref{subsubsection:rare_word_problem} we underlined \emph{hallucinations} as one of the problems of the Encoder-Decoder architecture. In the task on RotoWire dataset it basically means that during training, the model learns that it should put \emph{some number} at a specific place in the sentence. We would like it to generate the \emph{exact value}.

Therefore as the next step we incorporate the \emph{Joint Copy mechanism} \citep{gu2016incorporating}, \citep{yang2016referenceaware} (already described in \ref{section:copy_mechanism}). The encoder rests intact, however in the decoder we use another attention module to point to specific record from the input table and a feed-forward network which models if the model should generate or copy. In the following chapter we call this kind of model \emph{Copy} model.

The generational path is the same as in the baseline model. In the copy path we use the alignment vector from the newly added attention as weights to compute the weighted sum of the value portion of input records. The visualization can be seen in the figure \ref{rotowire_joint_copy_vis}.

\section{Content Selection and Planning} \label{section:content_selection_and_planning}

Rather than training end-to-end, \citep{puduppully2019datatotext} suggest to divide the task to \emph{content planning} and \emph{text generation}. The \emph{content plan} describes "what to say, and in what order". During the \emph{content planning} stage the model selects some records from the input table and organizes them to a \emph{content plan}. During the \emph{text generation} stage, the model generates the text based on the records pointed to by the \emph{content plan}.

Both tasks are modelled with \emph{the same} Encoder and with \emph{separate} Decoders.

\subsection{Content Selection} \label{subsection:content_selection}

\citep{puduppully2019datatotext} improves the baseline encoder by incorporating \emph{context awareness}. At first the input records are encoded in the same way as in the baseline model \ref{section:baseline_model}, the self-attention is used to model the "importance vis-a-vis other records in the table".

Specifically, we compute
\begin{align*}
\forall k \neq t : \alpha_{t, k} &= score(r_t, r_k)                         &&\mbox{ \small \emph{the score vector}}\\
\boldsymbol{\beta_t}             &= softmax(\boldsymbol{\alpha_t})                      &&\mbox{ \small \emph{the alignment vector}}\\
\boldsymbol{\gamma_t}            &= \sum_{i=1}^m{\beta_{t, i} * r_i}               &&\mbox{ \small \emph{the context vector}}\\
r_t^{att}                  &= W_{cs} [r_t, \boldsymbol{\gamma_t}]             \\
r_t^{cs}                   &= sigmoid(r_t^{att}) \bigodot r_t                  &&\mbox{ \small \emph{the content selected representation}}
\end{align*}

The Content Selection Encoder thus creates a sequence of \emph{context aware} representations $\{r_t^{cs}\}_{t=1}^m$ (figure \ref{figure:content_selection_pudupully}). We experiment with a model using Content Selection Encoder and Joint-Copy decoder ( we call it \emph{CopyCS}).

\begin{figure}[!h]
    \includegraphics{img/cs_gate-cropped.pdf}
    \caption{Content Selection mechanism (image is directly from \citep{puduppully2019datatotext})} \label{figure:content_selection_pudupully}
\end{figure}

\subsection{Content Planning} \label{subsection:content_planning}

\citet{wiseman2017} experimented with conditional copy approach, in which the latent \emph{switch} probability isn't marginalized out. Hence, there exists pointer sequence for each summary in the train and validation dataset. The sequence corresponds to the order in which entities and values from the sequence of input records appear in the output summary. \citet{puduppully2019datatotext} suggested that instead of just modelling the switch probability, we can train a Decoder to extract these pointers from the original table.

As suggested, the Content Planning Decoder is a one layer LSTM which operates on the \emph{context aware} representations $\boldsymbol{r}^{cs}$. Its hidden states are initialized with $avg(\{r_t^{cs}\}_{t=1}^m)$. \citet{puduppully2019datatotext} have not elaborated on the exact approach, hence it is probable that we have diverged a little bit from the intentions of original authors.

\begin{table}[h]
    \centering \small
    \begin{tabular}{llll}
        \toprule
        \textbf{Type} & \textbf{Entity} & \textbf{Value} & \textbf{H/A flag} \\                  
        \midrule
        \textless{}\textless{}BOS\textgreater{}\textgreater{} & \textless{}\textless{}BOS\textgreater{}\textgreater{} & \textless{}\textless{}BOS\textgreater{}\textgreater{} & \textless{}\textless{}BOS\textgreater{}\textgreater{} \\
        TEAM-CITY    & Raptors      & Toronto      & HOME                                                  \\
        TEAM-NAME    & Raptors      & Raptors      & HOME                                                  \\
        TEAM-PTS     & Raptors      & 122          & HOME                                                  \\
        TEAM-CITY    & 76ers        & Philadelphia & AWAY                                                  \\
        TEAM-NAME    & 76ers        & 76ers        & AWAY                                                  \\
        TEAM-NAME    & 76ers        & 76ers        & AWAY                                                  \\
        TEAM-PTS     & 76ers        & 95           & AWAY                                                  \\
        $\dots$      & $\dots$      & $\dots$      & $\dots$ \\                                        
        \bottomrule
        \multicolumn{4}{c}{\footnotesize \textit{Note:} The extract corresponds to sentence: "The host Toronto} \\
        \multicolumn{4}{c}{\footnotesize Raptors defeated the Philadelphia 76ers , 122 - 95\dots "}
    \end{tabular}
    \caption{An extract from the content plan corresponding to the summary from the figure \ref{figure:samplesummary}}
\end{table}

The Text Decoder is trained to start the generation when it sees the \emph{\textless BOS\textgreater} token. Since the Content Planning Decoder operates on $\boldsymbol{r}^{cs}$ we have chosen to prepend a special \emph{\textless BOS\textgreater} record to the sequence of input records, and also a pointer to the \emph{\textless BOS\textgreater} record is prepended to each content plan. Therefore instead of teaching the Content Planning Decoder to start generating content plan when seeing a special value, we teach it to do so when seeing \emph{the encoded representation of the special value}. The same approach is taken at the end, with the \emph{\textless EOS\textgreater} record.

\citet{puduppully2019datatotext} use a one layer bidirectional LSTM on top of the generated content plans as shown in figure \ref{figure:overal_architecture_csap} (in the following text we call this part of the model \emph{Content Plan Encoder}).

Joint-Copy Decoder operates as the Text Decoder to generate the output summary. In the following chapter we experiment with two models inspired by this approach. Firstly, we train Content Selection and Planning model as visualized in figure \ref{figure:overal_architecture_csap} (the model is called \emph{CS\&P}). Secondly, we simplify the input tables and train the exact same model without the Content Planning Decoder (the model is called \emph{CopyCSBidir}).

\subsection{Training and Inference}

At first let me elaborate on the training of \emph{CS\&P}. The model consists of two parts which are trained jointly. According to notation in figure \ref{figure:overal_architecture_csap} the model has two sets of outputs for which we calculate the content planning loss and the text decoding loss. These are added together to form the overall loss which is minimized by the optimizer. During training the inputs to the Content Plan Encoder are the encoded records pointed by the \emph{gold content plan}. Equation \ref{equation:argmin_training_csap_model} shows the loss value we aim to minimize during training, given the input sequences $\boldsymbol{x}$, gold content plans $\boldsymbol{z}$ and gold summaries $\boldsymbol{y}$.

\begin{equation} \label{equation:argmin_training_csap_model}
    \sum_{(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{z}) \in \mathcal{D}} - \log{p(\boldsymbol{y}|\boldsymbol{z}, \boldsymbol{x})} - \log{p(\boldsymbol{z} | \boldsymbol{x})}
\end{equation}

There are many possibilities what to do during inference. In section \ref{section:experiments_csap} we mention five options. The outputs of the Content Planning Decoder can be obtained using Greedy or Beam Search decoding, similarly for the Text Decoder. At last there is possibility to ignore the Content Planning Decoder and use the encoded records pointed to by the gold content plan. (if we have any gold content plan available). Equation \ref{equation:argmin_csap_model} shows the quantity we aim to maximize during the inference.

\begin{equation} \label{equation:argmin_csap_model}
    \argmin_{\boldsymbol{y'}, \boldsymbol{z'}}{p(\boldsymbol{y'}|\boldsymbol{z'}, \boldsymbol{x}) + p(\boldsymbol{z'} | \boldsymbol{x})}
\end{equation}

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.2]{img/content_selection_and_planning.jpg}
    \footnotesize{\\ \textit{Note:} $p_1 = 15$ means that the first generated pointer points to $15$-th encoded record (therefore $r^{cs}_{15}$ the first input to the Content Plan Encoder), similarly for $p_{k}$}
    \caption{Overall architecture of the Content Selection and Planning model with example outputs.} \label{figure:overal_architecture_csap}
\end{figure}

\section{Postprocessing}

The models are trained to generate lowercased Byte Pair Encoded sequences. During the postprocessing we merge the tokens belonging to one word together (e.g. \emph{"contribu$\star$ tor"} $\rightarrow$ \emph{"contributor"}), split the tokens of named entities (e.g. \emph{"LeBron\_James"} $\rightarrow$ \emph{"LeBron James"}) and uppercase the first letter of a sentence ( using NLTK\footnote{\url{https://www.nltk.org/}} library). Since the vast majority of unique tokens (more than 98.6\%) from the validation and test sets (section \ref{section:stats_transformed_dataset}) is in the vocabulary collected from the training set, we opted to ommit resolution of \emph{UNK} tokens\footnote{If we were forced to resolve the \emph{UNK} tokens, we would take the simplest possible approach. We would look at the last alignment produced by Copy Attention mechanism and copy the tabular value which is considered to be the most important according to the alignment. }.