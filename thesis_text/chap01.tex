\chapter{Problem statement}
In september 2020 I read a blog by \citep*{Karpathy2015}. He created a neural network consisting of only one LSTM cell and trained it to predict a character based on all the previous ones. The network was trained on a corpus of all the plays by Shakespeare. During inference the last predicted character was fed as the input to the network and this way it could create a really good looking Shakespeare-like text. Then I began to explore the possibilities of generating a text conditioned on some input parameters. How to construct a network that could be told to generate a sad, happy, or sarcastic sounding text?

\section{Data to text generation}
Known datasets (WIKIBIO, WeatherGov, RoboCup) -> short description, the generated summaries are one-two sentences long. Rotowire -> really long summaries although only a fraction of the number of unique tokens from the WIKIBIO dataset. Only short description of the dataset, the statistics and observations are in the second chapter.

\section{Fantasy sports}
What it is, where it originates, the relation to basic optimisation problems, why NLGenerated summaries could be added value.

\section{My goal}
Fluent text which captures the important statistics from the table