{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bewa_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1WUb-FbZHSa"
      },
      "source": [
        "# Basic experiments with seq2seq architecture and attention\n",
        "\n",
        "- based on tutorials:\n",
        "  - [official on tf 2.x api](https://www.tensorflow.org/tutorials/text/nmt_with_attention#write_the_encoder_and_decoder_model)\n",
        "  - [official on tf 1.x api](https://github.com/tensorflow/nmt)\n",
        "  - [thesis by Thang Luong](https://github.com/lmthang/thesis)\n",
        "\n",
        "- code for all the models is in [model.py](./model.py)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK2IpkqqaU2C"
      },
      "source": [
        "# Pure encoder-decoder architecture\n",
        "- 1024 GRU units both for encoder and decoder\n",
        "- Adam optimizer with basic settings\n",
        "- teacher forcing during training, greedy decoding during evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siMUtAwHje5Q"
      },
      "source": [
        "from main import *\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-u2PHotifR8",
        "outputId": "0a84cfe9-f6af-4f09-a047-4b3fc1174c4c"
      },
      "source": [
        "path = \"spa.txt\"\n",
        "num_examples = 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path, num_examples)\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n",
        "                                                                                                target_tensor,\n",
        "                                                                                                test_size=0.2)\n",
        "print(f\"training examples: {len(input_tensor_train)}\")\n",
        "print(f\"testing examples: {len(input_tensor_val)}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training examples: 24000\n",
            "testing examples: 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2Cl2lkGGK4o"
      },
      "source": [
        "- preparing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DgA0QVKi4hY"
      },
      "source": [
        "# create dataset\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "n_layers_m1 = 1\n",
        "n_layers_m2 = 2\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV34j1hikDla"
      },
      "source": [
        "- using decoder without attention `DecoderWA`\n",
        "- encoder creates a representation of the input sentence in its hidden state, the representation is then fed to the hidden state of the decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLc3i1egj53z",
        "outputId": "15afe328-415c-4d46-d7a4-e3e37b815629"
      },
      "source": [
        "# create encoder and decoderWA\n",
        "encoder_m1 = Encoder(vocab_inp_size, embedding_dim, n_layers_m1, units, BATCH_SIZE)\n",
        "decoder_m1 = DecoderWA(vocab_tar_size, embedding_dim, n_layers_m1, units, BATCH_SIZE)\n",
        "\n",
        "# create deeper encoder and decoderWA\n",
        "encoder_m2 = Encoder(vocab_inp_size, embedding_dim, n_layers_m2, units, BATCH_SIZE)\n",
        "decoder_m2 = DecoderWA(vocab_tar_size, embedding_dim, n_layers_m2, units, BATCH_SIZE)\n",
        "\n",
        "# create dummy variables just for the sake of showing actual\n",
        "# dimensions of tensors flowing through the network\n",
        "sample_hidden = encoder_m1.initialize_hidden_state()\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "sample_output, sample_hidden = encoder_m1(example_input_batch, sample_hidden)\n",
        "print(f'Encoder output shape: (batch size, sequence length, units) {sample_output.shape}')\n",
        "print(f'Encoder Hidden state shape: (batch size, units) {sample_hidden[0].shape}')\n",
        "\n",
        "sample_decoder_output, _, _ = decoder_m1(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print(f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 19, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "Decoder output shape: (batch_size, vocab size) (64, 4807)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y47XlAOmkp0F"
      },
      "source": [
        "- Adam optimizer without any finetuning\n",
        "- SparseCategoricalCrossentropy makes life easier, removes the need for creating one-hot representation from targets\n",
        "- checkpoint directory for saving partially trained models over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fp80Ri4kkpa"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "              from_logits=True, reduction='none')\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix_m1 = os.path.join(checkpoint_dir, 'ckpt_m1')\n",
        "checkpoint_prefix_m2 = os.path.join(checkpoint_dir, 'ckpt_m2')\n",
        "checkpoint_m1 = tf.train.Checkpoint( optimizer=optimizer\n",
        "                                   , encoder=encoder_m1\n",
        "                                   , decoder=decoder_m1)\n",
        "checkpoint_m2 = tf.train.Checkpoint( optimizer=optimizer\n",
        "                                   , encoder=encoder_m2\n",
        "                                   , decoder=decoder_m2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlY1nDVa9pjq"
      },
      "source": [
        "- training with teacher forcing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4xHAKCllS2S"
      },
      "source": [
        "EPOCHS = 10\n",
        "train( dataset\n",
        "     , encoder_m1\n",
        "     , decoder_m1\n",
        "     , loss_object\n",
        "     , optimizer\n",
        "     , EPOCHS\n",
        "     , BATCH_SIZE\n",
        "     , steps_per_epoch\n",
        "     , targ_lang\n",
        "     , checkpoint_m1\n",
        "     , checkpoint_prefix_m1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPXeET6FOIa9"
      },
      "source": [
        "EPOCHS = 40\n",
        "train( dataset\n",
        "     , encoder_m2\n",
        "     , decoder_m2\n",
        "     , loss_object\n",
        "     , optimizer\n",
        "     , EPOCHS\n",
        "     , BATCH_SIZE\n",
        "     , steps_per_epoch\n",
        "     , targ_lang\n",
        "     , checkpoint_m2\n",
        "     , checkpoint_prefix_m2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLPnKcXA6e0m",
        "outputId": "070b6b52-e115-4b18-e72d-fee0be4916df"
      },
      "source": [
        "evaluate( input_tensor_val\n",
        "        , target_tensor_val\n",
        "        , encoder_m1\n",
        "        , decoder_m1\n",
        "        , inp_lang\n",
        "        , targ_lang\n",
        "        , max_length_inp\n",
        "        , max_length_targ\n",
        "        , units\n",
        "        , n_layers_m1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average f1 score over validation dataset : 0.49910286989245517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di1MZeZgORY0",
        "outputId": "1cb7fa06-1b3f-4eb1-c2b5-a931811c4253"
      },
      "source": [
        "evaluate( input_tensor_val\n",
        "        , target_tensor_val\n",
        "        , encoder_m2\n",
        "        , decoder_m2\n",
        "        , inp_lang\n",
        "        , targ_lang\n",
        "        , max_length_inp\n",
        "        , max_length_targ\n",
        "        , units\n",
        "        , n_layers_m2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average f1 score over validation dataset : 0.463503092278094\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}