{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "encoder_decoder_basic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8ULnY8uJz9O",
        "outputId": "2ab3e68d-db3d-49db-ab11-86d32f54853b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YuPEfF8LgVu"
      },
      "source": [
        "from main import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from time import time\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hIatzIvLuha",
        "outputId": "1e32a7c2-9ef3-4d18-dc42-d0ed0165e411"
      },
      "source": [
        "path = \"/content/drive/My Drive/encoder_decoder_basic/spa.txt\"\n",
        "# option `None` means \"use the whole dataset\"\n",
        "num_examples = None\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path, num_examples)\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n",
        "                                                                                                target_tensor,\n",
        "                                                                                                test_size=0.1)\n",
        "print(f\"training examples: {len(input_tensor_train)}\")\n",
        "print(f\"testing examples: {len(input_tensor_val)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training examples: 115275\n",
            "testing examples: 12809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl9QmWr1MCgK"
      },
      "source": [
        "# basic parameters\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1\n",
        "\n",
        "# create a train dataset\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# create a test dataset\n",
        "steps_per_epoch_test = len(input_tensor_val) // BATCH_SIZE\n",
        "test_data = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
        "test_data = test_data.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG2Q5xIYMWWZ"
      },
      "source": [
        "# parameters shared by all the models\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "# create encoder and decoderNA\n",
        "n_layers_m1 = 1\n",
        "encoder_m1 = Encoder(vocab_inp_size, embedding_dim, n_layers_m1, units, BATCH_SIZE)\n",
        "decoder_m1 = DecoderNA(vocab_tar_size, embedding_dim, n_layers_m1, units, BATCH_SIZE)\n",
        "\n",
        "# create deeper encoder and decoderNA\n",
        "n_layers_m2 = 2\n",
        "encoder_m2 = Encoder(vocab_inp_size, embedding_dim, n_layers_m2, units, BATCH_SIZE)\n",
        "decoder_m2 = DecoderNA(vocab_tar_size, embedding_dim, n_layers_m2, units, BATCH_SIZE)\n",
        "\n",
        "# create encoder and decoder with attention\n",
        "n_layers_m3 = 1\n",
        "encoder_m3 = Encoder(vocab_inp_size, embedding_dim, n_layers_m3, units, BATCH_SIZE)\n",
        "decoder_m3 = Decoder(vocab_tar_size, embedding_dim, n_layers_m3, units, BATCH_SIZE)\n",
        "\n",
        "# create multilayer encoder and decoder with attention\n",
        "n_layers_m4 = 2\n",
        "encoder_m4 = Encoder(vocab_inp_size, embedding_dim, n_layers_m4, units, BATCH_SIZE)\n",
        "decoder_m4 = Decoder(vocab_tar_size, embedding_dim, n_layers_m4, units, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x2oF0f5OpFT"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "optimizer_m2 = tf.keras.optimizers.Adam()\n",
        "optimizer_m3 = tf.keras.optimizers.Adam()\n",
        "optimizer_m4 = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "              from_logits=True, reduction='none')\n",
        "\n",
        "checkpoint_dir = '/content/drive/My Drive/encoder_decoder_basic/training_checkpoints/'\n",
        "checkpoint_prefix_m1 = os.path.join(checkpoint_dir, 'ckpt_m1')\n",
        "checkpoint_prefix_m2 = os.path.join(checkpoint_dir, 'ckpt_m2')\n",
        "checkpoint_prefix_m3 = os.path.join(checkpoint_dir, 'ckpt_m3')\n",
        "checkpoint_prefix_m4 = os.path.join(checkpoint_dir, 'ckpt_m4')\n",
        "\n",
        "\n",
        "checkpoint_m1 = tf.train.Checkpoint( optimizer=optimizer\n",
        "                                   , encoder=encoder_m1\n",
        "                                   , decoder=decoder_m1)\n",
        "checkpoint_m2 = tf.train.Checkpoint( optimizer=optimizer_m2\n",
        "                                   , encoder=encoder_m2\n",
        "                                   , decoder=decoder_m2)\n",
        "checkpoint_m3 = tf.train.Checkpoint( optimizer=optimizer_m3\n",
        "                                   , encoder=encoder_m3\n",
        "                                   , decoder=decoder_m3)\n",
        "checkpoint_m4 = tf.train.Checkpoint( optimizer=optimizer_m4\n",
        "                                   , encoder=encoder_m4\n",
        "                                   , decoder=decoder_m4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-yzdIfZcOjaS"
      },
      "source": [
        "EPOCHS = 10\n",
        "train( dataset\n",
        "     , encoder_m1\n",
        "     , decoder_m1\n",
        "     , loss_object\n",
        "     , optimizer\n",
        "     , EPOCHS\n",
        "     , BATCH_SIZE\n",
        "     , steps_per_epoch\n",
        "     , targ_lang\n",
        "     , checkpoint_m1\n",
        "     , checkpoint_prefix_m1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhKxacQFz_Uj"
      },
      "source": [
        "EPOCHS = 10\n",
        "train( dataset\n",
        "     , encoder_m3\n",
        "     , decoder_m3\n",
        "     , loss_object\n",
        "     , optimizer_m3\n",
        "     , EPOCHS\n",
        "     , BATCH_SIZE\n",
        "     , steps_per_epoch\n",
        "     , targ_lang\n",
        "     , checkpoint_m3\n",
        "     , checkpoint_prefix_m3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GCsuaX8ObcV"
      },
      "source": [
        "EPOCHS = 10\n",
        "train( dataset\n",
        "     , encoder_m2\n",
        "     , decoder_m2\n",
        "     , loss_object\n",
        "     , optimizer_m2\n",
        "     , EPOCHS\n",
        "     , BATCH_SIZE\n",
        "     , steps_per_epoch\n",
        "     , targ_lang\n",
        "     , checkpoint_m2\n",
        "     , checkpoint_prefix_m2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KISc1P5_v5uS"
      },
      "source": [
        "EPOCHS = 10\n",
        "train( dataset\n",
        "     , encoder_m4\n",
        "     , decoder_m4\n",
        "     , loss_object\n",
        "     , optimizer_m4\n",
        "     , EPOCHS\n",
        "     , BATCH_SIZE\n",
        "     , steps_per_epoch\n",
        "     , targ_lang\n",
        "     , checkpoint_m4\n",
        "     , checkpoint_prefix_m4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCkaXFXXMnS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e1d5d78-47d6-4095-deee-078466d83a94"
      },
      "source": [
        "evaluate( test_data\n",
        "        , encoder_m4\n",
        "        , decoder_m4\n",
        "        , inp_lang\n",
        "        , targ_lang\n",
        "        , max_length_inp\n",
        "        , max_length_targ\n",
        "        , units\n",
        "        , n_layers_m4\n",
        "        , BATCH_SIZE\n",
        "        , steps_per_epoch_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average f1 score over validation dataset : 0.6094714520908817\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}